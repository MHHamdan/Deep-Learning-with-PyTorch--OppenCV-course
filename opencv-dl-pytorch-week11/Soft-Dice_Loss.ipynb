{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">Soft-Dice Loss</font>\n",
    "\n",
    "In the semantic-segmentation task, we need to apply a particular group of loss functions. They are all based on one fact: the objects of some classes tend to have several adjoined pixels in the image. Thus, we will introduce an example of one  such special loss function - `soft-Dice loss`. We will briefly describe its key concept and implement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:green\">1. Soft-Dice Loss</font>\n",
    "\n",
    "As you remember, `Dice Coefficient` or `F1-score` is the double number of true positives, divided by the total number of pixels, in both ground truth and predicted-segmentation masks. We don’t have true negatives - in our case, they are just pixels of some other class. Therefore, the ground-truth annotation can be represented as a sum of true positives and false negatives of every class. The predicted pixels are a sum of true and false positives across all classes.\n",
    "\n",
    "\n",
    "**Here is its formula:**\n",
    "\n",
    "$$ DC = \\frac{2\\cdot{\\sum_{i=0}^{N}{p_{i}y_{i}}}}{\\sum_{i=0}^{N}{p_{i}} + \\sum_{i=0}^{N}{y_{i}}},$$\n",
    "\n",
    "\n",
    "where,\n",
    "\n",
    "$p_{i}$ - is prediction for pixel $i$,\n",
    "\n",
    "$y_{i}$ - is ground truth for pixel $i$;\n",
    "\n",
    "$N$ is the total number of pixels on the image.\n",
    "\n",
    "\n",
    "Now, let’s discuss how to turn this metric into a loss function. Dice coefficient compares two discrete masks, and is therefore a discrete function. To make a loss function out of it, we must  come up with a differentiable function. So, instead of thresholded values like `0` and `1`, we make floating-point probabilities, in the range of `[0, 1]`. The function that helps us do this is a negative logarithm. You may recall that even classification cross-entropy loss uses negative logarithm for the same reasons.\n",
    "\n",
    "Taking all of the above into account, our loss function i.e. `Soft-Dice loss` can be represented as:\n",
    "\n",
    "\n",
    "$$loss_{soft-dice} = -\\log{\\frac{2\\cdot{\\sum_{i=0}^{N}{p_{i}y_{i} + \\epsilon}}}{\\sum_{i=0}^{N}{p_{i}} + \\sum_{i=0}^{N}{y_{i}} + \\epsilon}}.$$\n",
    "\n",
    "Note: We also added $\\epsilon$ - an epsilon - both to the numerator and the denominator, to aid  computation.The idea is   to avoid situations like division on zero, or taking the logarithm from zero, which is undefined. As for epsilon, you can take any value that is small enough, for example `1e-5`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:green\">2. Soft-Dice Loss Implementation</font>\n",
    "\n",
    "**Let's start implementing it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Configuration for reproducible results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SystemConfig:\n",
    "    seed: int = 42  # seed number to set the state of all random number generators\n",
    "    cudnn_benchmark_enabled: bool = False  # enable CuDNN benchmark for the sake of performance\n",
    "    cudnn_deterministic: bool = True  # make cudnn deterministic (reproducible training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_system(system_config: SystemConfig) -> None:\n",
    "    torch.manual_seed(system_config.seed)\n",
    "    np.random.seed(system_config.seed)\n",
    "    random.seed(system_config.seed)\n",
    "    torch.set_printoptions(precision=10)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(system_config.seed)\n",
    "        torch.backends.cudnn_benchmark_enabled = system_config.cudnn_benchmark_enabled\n",
    "        torch.backends.cudnn.deterministic = system_config.cudnn_deterministic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create SoftDiceLoss class inherited from `nn.Module`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SoftDiceLoss class inherited from nn.Module\n",
    "class SoftDiceLoss(nn.Module):\n",
    "    \"\"\"\n",
    "        Implementation of the Soft-Dice Loss function.\n",
    "\n",
    "        Arguments:\n",
    "            num_classes (int): number of classes.\n",
    "            eps (float): value of the floating point epsilon.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, eps=1e-5):\n",
    "        super().__init__()\n",
    "        # init class fields\n",
    "        self.num_classes = num_classes\n",
    "        self.eps = eps\n",
    "\n",
    "    # define the forward pass\n",
    "    def forward(self, preds, targets):  # pylint: disable=unused-argument\n",
    "        \"\"\"\n",
    "            Compute Soft-Dice Loss.\n",
    "\n",
    "            Arguments:\n",
    "                preds (torch.FloatTensor):\n",
    "                    tensor of predicted labels. The shape of the tensor is (B, num_classes, H, W).\n",
    "                targets (torch.LongTensor):\n",
    "                    tensor of ground-truth labels. The shape of the tensor is (B, 1, H, W).\n",
    "            Returns:\n",
    "                mean_loss (float32): mean loss by class  value.\n",
    "        \"\"\"\n",
    "        loss = 0\n",
    "        # iterate over all classes\n",
    "        for cls in range(self.num_classes):\n",
    "            # get ground truth for the current class\n",
    "            target = (targets == cls).float()\n",
    "\n",
    "            # get prediction for the current class\n",
    "            pred = preds[:, cls]\n",
    "\n",
    "            # calculate intersection\n",
    "            intersection = (pred * target).sum()\n",
    "\n",
    "            # compute dice coefficient\n",
    "            dice = (2 * intersection + self.eps) / (pred.sum() + target.sum() + self.eps)\n",
    "\n",
    "            # compute negative logarithm from the obtained dice coefficient\n",
    "            loss = loss - dice.log()\n",
    "\n",
    "        # get mean loss by class value\n",
    "        loss = loss / self.num_classes\n",
    "\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check the implementation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.8738912343978882\n"
     ]
    }
   ],
   "source": [
    "# apply system settings\n",
    "setup_system(SystemConfig)\n",
    "\n",
    "# generate input data\n",
    "ground_truth = torch.zeros(1, 224, 224)\n",
    "ground_truth[:, :50, :50] = 1\n",
    "ground_truth[:, 50:100, 50:100] = 2\n",
    "\n",
    "# generate random predictions and use softmax to get probabilities\n",
    "prediction = torch.zeros(1, 3, 224, 224).uniform_().softmax(dim=1)\n",
    "\n",
    "# create an instance of a SoftDiceLoss class\n",
    "soft_dice_loss = SoftDiceLoss(num_classes=3)\n",
    "\n",
    "# get the loss value\n",
    "loss = soft_dice_loss(prediction, ground_truth)\n",
    "\n",
    "print('Loss: {}'.format(loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
