{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">LinkNet Architecture</font>\n",
    "\n",
    "In this notebook, we will examine LinkNet architecture, do block exploration, and implement it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:green\">1. LinkNet</font>\n",
    "\n",
    "Let's briefly overview the LinkNet architecture. [LinkNet](https://arxiv.org/pdf/1707.03718.pdf) was\n",
    "introduced in 2017 by A.Chaurasia and E.Culurciello as a novel lightweight deep neural network for semantic\n",
    "segmentation, which can learn a moderate growth rate of parameters:\n",
    "\n",
    "---\n",
    "\n",
    "<img src='https://www.learnopencv.com/wp-content/uploads/2020/04/c3-w11-LinkNet_architecture.png'>\n",
    "\n",
    "---\n",
    "\n",
    "In the picture above `/2` means downsampling of the feature map by a factor of 2 which is achieved by performing strided convolution, `∗2` denotes upsampling by `2`.\n",
    "\n",
    "\n",
    "An encoder is the left half of the network, whereas the the right side of it is a decoder.\n",
    "\n",
    "\n",
    "Let's view the below picture, where the encoder scheme is presented. The initial block of the encoder consists of a convolution with `7×7` kernel size and a stride of `2` and a max-pooling layer with a `3×3` kernel and also a stride of `2`. **The later encoder parts consist of residual blocks similar to `ResNet18` architecture:**\n",
    "\n",
    "---\n",
    "\n",
    "<img src='https://www.learnopencv.com/wp-content/uploads/2020/04/c3-w11-encoder.png'>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**Let's take a look at the decoder:**\n",
    "\n",
    "---\n",
    "\n",
    "<img src='https://www.learnopencv.com/wp-content/uploads/2020/04/c3-w11-decoder.png'>\n",
    "\n",
    "---\n",
    "\n",
    "- In the decoder the full-convolution technique is used. It is noticeable that the novelty of the LinkNet is in the connection between each encoder and decoder block. \n",
    "\n",
    "\n",
    "- Usually applying multiple downsampling in encoder leads to the partial missing of spatial information and further difficulties in its recovering. \n",
    "\n",
    "\n",
    "- In the **`LinkNet`'s** encoder inputs are bypassed to the outputs of the corresponding decoders. This scheme results in reconstruction of the lost spatial information. Also, such links between encoder and decoder trigger knowledge sharing at every layer. It means, that decoder can use smaller number of parameters and eventually will obtain more qualitative image feature information restoration.\n",
    "\n",
    "\n",
    "- It will also see that to process an image of size `640×360×3` the network uses `11.5 million` parameters or `21.2 GFLOPs`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:green\">2. LinkNet Implementation</font>\n",
    "\n",
    "Now, when we have discussed underlying ideas behind the architecture, let's implement it.\n",
    "We will need the following imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# torch neural network (NN) module for building and training nets\n",
    "import torch.nn as nn\n",
    "# module with various model definitions\n",
    "import torchvision.models as models\n",
    "\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Configuration for reproducible results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SystemConfig:\n",
    "    seed: int = 42  # seed number to set the state of all random number generators\n",
    "    cudnn_benchmark_enabled: bool = False  # enable CuDNN benchmark for the sake of performance\n",
    "    cudnn_deterministic: bool = True  # make cudnn deterministic (reproducible training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_system(system_config: SystemConfig) -> None:\n",
    "    torch.manual_seed(system_config.seed)\n",
    "    np.random.seed(system_config.seed)\n",
    "    random.seed(system_config.seed)\n",
    "    torch.set_printoptions(precision=10)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(system_config.seed)\n",
    "        torch.backends.cudnn_benchmark_enabled = system_config.cudnn_benchmark_enabled\n",
    "        torch.backends.cudnn.deterministic = system_config.cudnn_deterministic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">2.1. Decoder</font>\n",
    "\n",
    "The below presented block is a decoder, which takes a feature map with defined channels number. The `channels_in` the result map should be equal to `channels_out`.\n",
    "\n",
    "We have used `ConvTranspose2d` for upsampling, find details [here](https://pytorch.org/docs/stable/nn.html#torch.nn.ConvTranspose2d)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create decoder block inherited from nn.Module\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, channels_in, channels_out):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1x1 projection module to reduce channels\n",
    "        self.proj = nn.Sequential(\n",
    "            # convolution\n",
    "            nn.Conv2d(channels_in, channels_in // 4, kernel_size=1, bias=False),\n",
    "            # batch normalization\n",
    "            nn.BatchNorm2d(channels_in // 4),\n",
    "            # relu activation\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # fully convolutional module\n",
    "        self.deconv = nn.Sequential(\n",
    "            # deconvolution\n",
    "            nn.ConvTranspose2d(\n",
    "                channels_in // 4,\n",
    "                channels_in // 4,\n",
    "                kernel_size=4,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                output_padding=0,\n",
    "                groups=channels_in // 4,\n",
    "                bias=False\n",
    "            ),\n",
    "            # batch normalization\n",
    "            nn.BatchNorm2d(channels_in // 4),\n",
    "            # relu activation\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # 1x1 unprojection module to increase channels\n",
    "        self.unproj = nn.Sequential(\n",
    "            # convolution\n",
    "            nn.Conv2d(channels_in // 4, channels_out, kernel_size=1, bias=False),\n",
    "            # batch normalization\n",
    "            nn.BatchNorm2d(channels_out),\n",
    "            # relu activation\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    # stack layers and perform a forward pass\n",
    "    def forward(self, x):\n",
    "\n",
    "        proj = self.proj(x)\n",
    "        deconv = self.deconv(proj)\n",
    "        unproj = self.unproj(deconv)\n",
    "\n",
    "        return unproj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">2.2. LinkNet</font>\n",
    "\n",
    "To define the network we also use blocks from pretrained PyTorch\n",
    "[ResNet18](https://pytorch.org/docs/stable/_modules/torchvision/models/resnet.html#resnet18) as the encoder architecture is similar to `ResNet18` encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create LinkNet model with ResNet18 encoder\n",
    "class LinkNet(nn.Module):\n",
    "    def __init__(self, num_classes, encoder=\"resnet18\"):\n",
    "        super().__init__()\n",
    "        assert hasattr(models, encoder), \"Undefined encoder type\"\n",
    "        # prepare feature extractor from `torchvision` ResNet model\n",
    "        feature_extractor = getattr(models, encoder)(pretrained=True)\n",
    "        # Init block: get configured Conv2d, BatchNorm2d layers and ReLU from torch ResNet class\n",
    "        self.init = nn.Sequential(feature_extractor.conv1, feature_extractor.bn1, feature_extractor.relu)\n",
    "        self.maxpool = feature_extractor.maxpool\n",
    "\n",
    "        # Encoder's blocks: torch ResNet18 blocks initialization\n",
    "        self.layer1 = feature_extractor.layer1\n",
    "        self.layer2 = feature_extractor.layer2\n",
    "        self.layer3 = feature_extractor.layer3\n",
    "        self.layer4 = feature_extractor.layer4\n",
    "\n",
    "        # Decoder's block: DecoderBlock module\n",
    "        self.up4 = DecoderBlock(self._num_channels(self.layer4), self._num_channels(self.layer3))\n",
    "        self.up3 = DecoderBlock(self._num_channels(self.layer3), self._num_channels(self.layer2))\n",
    "        self.up2 = DecoderBlock(self._num_channels(self.layer2), self._num_channels(self.layer1))\n",
    "        self.up1 = DecoderBlock(self._num_channels(self.layer1), self._num_channels(self.layer1))\n",
    "\n",
    "        # Classification block: define a classifier module\n",
    "        self.classifier = nn.Sequential(\n",
    "            # deconvolution layer\n",
    "            nn.ConvTranspose2d(self._num_channels(self.layer1), 32, 3, stride=2, bias=False),\n",
    "            # batch normalization with num_features = 32\n",
    "            nn.BatchNorm2d(32),\n",
    "            # activation function\n",
    "            nn.ReLU(),\n",
    "            # convolutional layer\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1, bias=False),\n",
    "            # batch normalization with num_features = 32\n",
    "            nn.BatchNorm2d(32),\n",
    "            # activation function\n",
    "            nn.ReLU(),\n",
    "            # convolutional layer\n",
    "            nn.Conv2d(32, num_classes, kernel_size=2, padding=0)\n",
    "        )\n",
    "\n",
    "    # get a compatible number of channels to stack all of the LinkNet's blocks together\n",
    "    @staticmethod\n",
    "    def _num_channels(block):\n",
    "        \"\"\"\n",
    "           Extract batch-norm num_features from the input block.\n",
    "\n",
    "            Arguments:\n",
    "                block: torch resNet18 layers.\n",
    "        \"\"\"\n",
    "        # check whether the input block is models.resnet.BasicBlock type\n",
    "        if isinstance(block[-1], models.resnet.BasicBlock):\n",
    "            return block[-1].bn2.weight.size(0)\n",
    "        # if not extract the spatial characteristic of batch-norm weights from input block\n",
    "        return block[-1].bn3.weight.size(0)\n",
    "\n",
    "    # define the forward pass\n",
    "    def forward(self, x):\n",
    "\n",
    "        # output size = (64, 160, 160)\n",
    "        init = self.init(x)\n",
    "        # output size = (64, 80, 80)\n",
    "        maxpool = self.maxpool(init)\n",
    "        # output size = (64, 80, 80)\n",
    "        layer1 = self.layer1(maxpool)\n",
    "        # output size = (128, 40, 40)\n",
    "        layer2 = self.layer2(layer1)\n",
    "        # output size = (256, 20, 20)\n",
    "        layer3 = self.layer3(layer2)\n",
    "        # output size = (512, 10, 10)\n",
    "        layer4 = self.layer4(layer3)\n",
    "        # output size = (256, 20, 20)\n",
    "        up4 = self.up4(layer4) + layer3\n",
    "        # output size = (128, 40, 40)\n",
    "        up3 = self.up3(up4) + layer2\n",
    "        # output size = (64, 80, 80)\n",
    "        up2 = self.up2(up3) + layer1\n",
    "        # output size = (64, 160, 160)\n",
    "        up1 = self.up1(up2)\n",
    "        # output size = (5, 320, 320), where 5 is the predefined number of classes\n",
    "        output = self.classifier(up1)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">2.3. Check the Implementation</font>\n",
    "\n",
    "**Let's check the implementation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Size: torch.Size([1, 5, 320, 320])\n"
     ]
    }
   ],
   "source": [
    "# apply system settings\n",
    "setup_system(SystemConfig)\n",
    "\n",
    "# input data for model check\n",
    "input_tensor = torch.zeros(1, 3, 320, 320)\n",
    "\n",
    "# LinkNet architecture\n",
    "model = LinkNet(num_classes=5, encoder=\"resnet18\")\n",
    "\n",
    "# examining the prediction size\n",
    "pred = model(input_tensor)\n",
    "print('Prediction Size: {}'.format(pred.size()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input width and height is the same as output width and height because semantic segmentation predicts the label of each pixel.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:green\">3. Model Profiler</font>\n",
    "\n",
    "**Let's find the number of parameters and the number of floating-point operations of our model.**\n",
    "\n",
    "Let's write a class and a wrapper function around it for model profiling.\n",
    "\n",
    "**Note that to count the number of parameters, we need only the model (model parameters), where to count floating-point operation, we also need input image size, because convolution output size depends on input image size.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelProfiler(nn.Module):\n",
    "    \"\"\" Profile PyTorch models.\n",
    "\n",
    "    Compute FLOPs (FLoating OPerations) and number of trainable parameters of model.\n",
    "\n",
    "    Arguments:\n",
    "        model (nn.Module): model which will be profiled.\n",
    "\n",
    "    Example:\n",
    "        model = torchvision.models.resnet50()\n",
    "        profiler = ModelProfiler(model)\n",
    "        var = torch.zeros(1, 3, 224, 224)\n",
    "        profiler(var)\n",
    "        print(\"FLOPs: {0:.5}; #Params: {1:.5}\".format(profiler.get_flops('G'), profiler.get_params('M')))\n",
    "\n",
    "    Warning:\n",
    "        Model profiler doesn't work with models, wrapped by torch.nn.DataParallel.\n",
    "    \"\"\"\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.flops = 0\n",
    "        self.units = {'K': 10.**3, 'M': 10.**6, 'G': 10.**9}\n",
    "        self.hooks = None\n",
    "        self._remove_hooks()\n",
    "\n",
    "    def get_flops(self, units='G'):\n",
    "        \"\"\" Get number of floating operations per inference.\n",
    "\n",
    "        Arguments:\n",
    "            units (string): units of the flops value ('K': Kilo (10^3), 'M': Mega (10^6), 'G': Giga (10^9)).\n",
    "\n",
    "        Returns:\n",
    "            Floating operations per inference at the choised units.\n",
    "        \"\"\"\n",
    "        assert units in self.units\n",
    "        return self.flops / self.units[units]\n",
    "\n",
    "    def get_params(self, units='G'):\n",
    "        \"\"\" Get number of trainable parameters of the model.\n",
    "\n",
    "        Arguments:\n",
    "            units (string): units of the flops value ('K': Kilo (10^3), 'M': Mega (10^6), 'G': Giga (10^9)).\n",
    "\n",
    "        Returns:\n",
    "            Number of trainable parameters of the model at the choised units.\n",
    "        \"\"\"\n",
    "        assert units in self.units\n",
    "        params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        if units is not None:\n",
    "            params = params / self.units[units]\n",
    "        return params\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        self.flops = 0\n",
    "        self._init_hooks()\n",
    "        output = self.model(*args, **kwargs)\n",
    "        self._remove_hooks()\n",
    "        return output\n",
    "\n",
    "    def _remove_hooks(self):\n",
    "        if self.hooks is not None:\n",
    "            for hook in self.hooks:\n",
    "                hook.remove()\n",
    "        self.hooks = None\n",
    "\n",
    "    def _init_hooks(self):\n",
    "        self.hooks = []\n",
    "\n",
    "        def hook_compute_flop(module, _, output):\n",
    "            self.flops += module.weight.size()[1:].numel() * output.size()[1:].numel()\n",
    "\n",
    "        def add_hooks(module):\n",
    "            if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "                self.hooks.append(module.register_forward_hook(hook_compute_flop))\n",
    "\n",
    "        self.model.apply(add_hooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_model(model, input_size, cuda):\n",
    "    \"\"\" Compute FLOPS and #Params of the CNN.\n",
    "\n",
    "    Arguments:\n",
    "        model (nn.Module): model which should be profiled.\n",
    "        input_size (tuple): size of the input variable.\n",
    "        cuda (bool): if True then variable will be upload to the GPU.\n",
    "\n",
    "    Returns:\n",
    "        dict:\n",
    "            dict[\"flops\"] (float): number of GFLOPs.\n",
    "            dict[\"params\"] (int): number of million parameters.\n",
    "    \"\"\"\n",
    "    profiler = ModelProfiler(model)\n",
    "    var = torch.zeros(input_size)\n",
    "    if cuda:\n",
    "        var = var.cuda()\n",
    "    profiler(var)\n",
    "    return {\"flops\": profiler.get_flops('G'), \"params\": profiler.get_params('M')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's calculate  `GFLOPs` and the `number of parameters` in the model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GFLOPs:\t\t\t\t9.613157376\n",
      "No. of params (in million):\t11.341829\n"
     ]
    }
   ],
   "source": [
    "# input data for model check\n",
    "input_tensor = torch.zeros(1, 3, 640, 320)\n",
    "\n",
    "flops, params = profile_model(model, input_tensor.size(), False).values()\n",
    "\n",
    "print('GFLOPs:\\t\\t\\t\\t{}\\nNo. of params (in million):\\t{}'.format(flops, params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
