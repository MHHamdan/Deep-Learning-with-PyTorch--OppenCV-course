{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">1. Detector NN-Architecture</font>\n",
    "\n",
    "Single-stage object detection pipeline looks like as follows:\n",
    "\n",
    "---\n",
    "\n",
    "<img src='https://www.learnopencv.com/wp-content/uploads/2020/03/c3-w8-pipeline.png' align=\"middle\">\n",
    "\n",
    "---\n",
    "\n",
    "We extract the features with a backbone and then use two branches for prediction: one is to regress the box coordinates of an object, and another one - classification - to predict which class the detected object belongs to. \n",
    "\n",
    "In this unit, we will go into details of detector network architecture.\n",
    "\n",
    "Here, we will use the [Feature Pyramid Network](https://arxiv.org/pdf/1612.03144.pdf) for feature extraction. On top of this, we will use class subnet and box subnet to get classification and bounding box. Let's have a look at the following architecture. \n",
    "\n",
    "---\n",
    "\n",
    "<img src='https://www.learnopencv.com/wp-content/uploads/2020/03/c3-w8-retinanet.png' align='middle'>\n",
    "\n",
    "---\n",
    "\n",
    "Feature Pyramid Net is built on top of ResNet (we will use `ResNet-18`) in a fully convolutional fashion.\n",
    "It includes two pathways: **bottom-up or forward** and **top-down**, which goes in the inverse direction. These two pathways are connected in-between with lateral connections.\n",
    "\n",
    "\n",
    "Bottom-up pathway is doing the feedforward path, extracting the features. Nothing new here.\n",
    "\n",
    "**What about the top-down pathway?**\n",
    "\n",
    "Features closer to the input image have a rich segment (bounding box) information. So it is needed to merge all of the feature maps from different levels of the pyramid into one semantically-rich feature map.\n",
    "\n",
    "**Let's have a close look at the top-down and lateral connection.**\n",
    "\n",
    "---\n",
    "\n",
    "<img src='https://www.learnopencv.com/wp-content/uploads/2020/03/c3-w8-lateral_con.png' align='middle'>\n",
    "\n",
    "---\n",
    "\n",
    "The higher-level features are upsampled to be 2x larger. For this purpose, nearest neighbor upsampling is used. The larger feature map undergoes a 1x1 convolutional layer to reduce the channel dimension. Finally, these two feature maps are added together in element-wise manner. The process continues until the finest merged feature map is created.\n",
    "\n",
    "**How is these merged features map being used for prediction?**\n",
    "\n",
    "These features map goes into two different CNN of classes and bounding boxes predictions. \n",
    "\n",
    "**So we have the following components of the object detector architecture:**\n",
    "\n",
    "1. ResNet18 (`a`)\n",
    "2. Feature Pyramid Network (`b`)\n",
    "3. Prediction Network: (i) class subnet (`c`) and (ii) box subnet (`d`).\n",
    "\n",
    "\n",
    "Let's have a look into these components with an example input. Let's assume we have a batch size of `2`, and the input image dimension is `3x256x256` (channel first).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "from IPython.display import Code\n",
    "import inspect\n",
    "\n",
    "from fpn import FPN\n",
    "from detector import Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">1.1. ResNet</font>\n",
    "\n",
    "We will be using `ResNet18`. We will use a pre-trained model because it has already enriched features for classification. Let's have a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resnet = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(resnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can see that ResNet18 has the following blocks:**\n",
    "1. `conv1`\n",
    "1. `bn1`\n",
    "1. `relu`\n",
    "1. `maxpool`\n",
    "1. `layer1`\n",
    "1. `layer2`\n",
    "1. `layer3`\n",
    "1. `layer4`\n",
    "1. avgpool\n",
    "1. fc\n",
    "\n",
    "We are using number `1-8` blocks in `FPN`. Before going into FPN let's have a look at the output dimension of these blocks.\n",
    "\n",
    "We are just focusing on the output dimension, so instead of using a real image, we will use random input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# btch_size = 2, image dimesion = 3 x 256 x 256 \n",
    "\n",
    "image_inputs = torch.rand((2, 3, 256, 256))\n",
    "\n",
    "x = resnet.conv1(image_inputs)\n",
    "x = resnet.bn1(x)\n",
    "x = resnet.relu(x)\n",
    "x = resnet.maxpool(x)\n",
    "layer1_output = resnet.layer1(x)\n",
    "layer2_output = resnet.layer2(layer1_output)\n",
    "layer3_output = resnet.layer3(layer2_output)\n",
    "layer4_output = resnet.layer4(layer3_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FPN uses `layer2_output`, `layer3_output`, and `layer4_output` to get features from different convolution layers. \n",
    "\n",
    "Let's have a look at its dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer2_output size: torch.Size([2, 128, 32, 32])\n",
      "layer3_output size: torch.Size([2, 256, 16, 16])\n",
      "layer4_output size: torch.Size([2, 512, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "print('layer2_output size: {}'.format(layer2_output.size()))\n",
    "\n",
    "print('layer3_output size: {}'.format(layer3_output.size()))\n",
    "\n",
    "print('layer4_output size: {}'.format(layer4_output.size()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">1.2. Feature Pyramid Network</font>\n",
    "\n",
    "Let us have a look at `FPN` class that implements `Feature Pyramid Networks`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.output_html .hll { background-color: #ffffcc }\n",
       ".output_html  { background: #f8f8f8; }\n",
       ".output_html .c { color: #408080; font-style: italic } /* Comment */\n",
       ".output_html .err { border: 1px solid #FF0000 } /* Error */\n",
       ".output_html .k { color: #008000; font-weight: bold } /* Keyword */\n",
       ".output_html .o { color: #666666 } /* Operator */\n",
       ".output_html .ch { color: #408080; font-style: italic } /* Comment.Hashbang */\n",
       ".output_html .cm { color: #408080; font-style: italic } /* Comment.Multiline */\n",
       ".output_html .cp { color: #BC7A00 } /* Comment.Preproc */\n",
       ".output_html .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */\n",
       ".output_html .c1 { color: #408080; font-style: italic } /* Comment.Single */\n",
       ".output_html .cs { color: #408080; font-style: italic } /* Comment.Special */\n",
       ".output_html .gd { color: #A00000 } /* Generic.Deleted */\n",
       ".output_html .ge { font-style: italic } /* Generic.Emph */\n",
       ".output_html .gr { color: #FF0000 } /* Generic.Error */\n",
       ".output_html .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
       ".output_html .gi { color: #00A000 } /* Generic.Inserted */\n",
       ".output_html .go { color: #888888 } /* Generic.Output */\n",
       ".output_html .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
       ".output_html .gs { font-weight: bold } /* Generic.Strong */\n",
       ".output_html .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
       ".output_html .gt { color: #0044DD } /* Generic.Traceback */\n",
       ".output_html .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
       ".output_html .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
       ".output_html .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
       ".output_html .kp { color: #008000 } /* Keyword.Pseudo */\n",
       ".output_html .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
       ".output_html .kt { color: #B00040 } /* Keyword.Type */\n",
       ".output_html .m { color: #666666 } /* Literal.Number */\n",
       ".output_html .s { color: #BA2121 } /* Literal.String */\n",
       ".output_html .na { color: #7D9029 } /* Name.Attribute */\n",
       ".output_html .nb { color: #008000 } /* Name.Builtin */\n",
       ".output_html .nc { color: #0000FF; font-weight: bold } /* Name.Class */\n",
       ".output_html .no { color: #880000 } /* Name.Constant */\n",
       ".output_html .nd { color: #AA22FF } /* Name.Decorator */\n",
       ".output_html .ni { color: #999999; font-weight: bold } /* Name.Entity */\n",
       ".output_html .ne { color: #D2413A; font-weight: bold } /* Name.Exception */\n",
       ".output_html .nf { color: #0000FF } /* Name.Function */\n",
       ".output_html .nl { color: #A0A000 } /* Name.Label */\n",
       ".output_html .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n",
       ".output_html .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
       ".output_html .nv { color: #19177C } /* Name.Variable */\n",
       ".output_html .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n",
       ".output_html .w { color: #bbbbbb } /* Text.Whitespace */\n",
       ".output_html .mb { color: #666666 } /* Literal.Number.Bin */\n",
       ".output_html .mf { color: #666666 } /* Literal.Number.Float */\n",
       ".output_html .mh { color: #666666 } /* Literal.Number.Hex */\n",
       ".output_html .mi { color: #666666 } /* Literal.Number.Integer */\n",
       ".output_html .mo { color: #666666 } /* Literal.Number.Oct */\n",
       ".output_html .sa { color: #BA2121 } /* Literal.String.Affix */\n",
       ".output_html .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
       ".output_html .sc { color: #BA2121 } /* Literal.String.Char */\n",
       ".output_html .dl { color: #BA2121 } /* Literal.String.Delimiter */\n",
       ".output_html .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
       ".output_html .s2 { color: #BA2121 } /* Literal.String.Double */\n",
       ".output_html .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */\n",
       ".output_html .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
       ".output_html .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */\n",
       ".output_html .sx { color: #008000 } /* Literal.String.Other */\n",
       ".output_html .sr { color: #BB6688 } /* Literal.String.Regex */\n",
       ".output_html .s1 { color: #BA2121 } /* Literal.String.Single */\n",
       ".output_html .ss { color: #19177C } /* Literal.String.Symbol */\n",
       ".output_html .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       ".output_html .fm { color: #0000FF } /* Name.Function.Magic */\n",
       ".output_html .vc { color: #19177C } /* Name.Variable.Class */\n",
       ".output_html .vg { color: #19177C } /* Name.Variable.Global */\n",
       ".output_html .vi { color: #19177C } /* Name.Variable.Instance */\n",
       ".output_html .vm { color: #19177C } /* Name.Variable.Magic */\n",
       ".output_html .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span></span><span class=\"s s-Atom\">class</span> <span class=\"nv\">FPN</span><span class=\"p\">(</span><span class=\"s s-Atom\">nn</span><span class=\"p\">.</span><span class=\"nv\">Module</span><span class=\"p\">)</span><span class=\"s s-Atom\">:</span>\n",
       "    <span class=\"s s-Atom\">def</span> <span class=\"k\">__</span><span class=\"nf\">init__</span><span class=\"p\">(</span><span class=\"s s-Atom\">self</span><span class=\"p\">,</span> <span class=\"s s-Atom\">block_expansion</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"s s-Atom\">backbone=</span><span class=\"s2\">&quot;resnet18&quot;</span><span class=\"p\">)</span><span class=\"s s-Atom\">:</span>\n",
       "        <span class=\"nf\">super</span><span class=\"p\">().</span><span class=\"k\">__</span><span class=\"nf\">init__</span><span class=\"p\">()</span>\n",
       "        <span class=\"s s-Atom\">assert</span> <span class=\"nf\">hasattr</span><span class=\"p\">(</span><span class=\"s s-Atom\">models</span><span class=\"p\">,</span> <span class=\"s s-Atom\">backbone</span><span class=\"p\">),</span> <span class=\"s2\">&quot;Undefined encoder type&quot;</span>\n",
       "        \n",
       "        <span class=\"s s-Atom\">#</span> <span class=\"s s-Atom\">load</span> <span class=\"s s-Atom\">model</span> \n",
       "        <span class=\"s s-Atom\">self</span><span class=\"p\">.</span><span class=\"s s-Atom\">feature_extractor</span> <span class=\"o\">=</span> <span class=\"nf\">getattr</span><span class=\"p\">(</span><span class=\"s s-Atom\">models</span><span class=\"p\">,</span> <span class=\"s s-Atom\">backbone</span><span class=\"p\">)(</span><span class=\"s s-Atom\">pretrained</span><span class=\"o\">=</span><span class=\"nv\">True</span><span class=\"p\">)</span>\n",
       "        \n",
       "        <span class=\"s s-Atom\">#</span> <span class=\"s s-Atom\">two</span> <span class=\"s s-Atom\">more</span> <span class=\"s s-Atom\">layers</span> <span class=\"s s-Atom\">conv6</span> <span class=\"s s-Atom\">and</span> <span class=\"s s-Atom\">conv7</span> <span class=\"s s-Atom\">on</span> <span class=\"s s-Atom\">the</span> <span class=\"s s-Atom\">top</span> <span class=\"s s-Atom\">of</span> <span class=\"nf\">layer4</span> <span class=\"p\">(</span><span class=\"s s-Atom\">if</span> <span class=\"s s-Atom\">backbone</span> <span class=\"o\">is</span> <span class=\"s s-Atom\">resnet18</span><span class=\"p\">)</span>\n",
       "        \n",
       "        <span class=\"s s-Atom\">self</span><span class=\"p\">.</span><span class=\"s s-Atom\">conv6</span> <span class=\"o\">=</span> <span class=\"s s-Atom\">nn</span><span class=\"p\">.</span><span class=\"nv\">Conv2d</span><span class=\"p\">(</span>\n",
       "            <span class=\"mi\">512</span> <span class=\"o\">*</span> <span class=\"s s-Atom\">block_expansion</span><span class=\"p\">,</span> <span class=\"mi\">64</span> <span class=\"o\">*</span> <span class=\"s s-Atom\">block_expansion</span><span class=\"p\">,</span> <span class=\"s s-Atom\">kernel_size</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"s s-Atom\">stride</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"s s-Atom\">padding</span><span class=\"o\">=</span><span class=\"mi\">1</span>\n",
       "        <span class=\"p\">)</span>\n",
       "        <span class=\"s s-Atom\">self</span><span class=\"p\">.</span><span class=\"s s-Atom\">conv7</span> <span class=\"o\">=</span> <span class=\"s s-Atom\">nn</span><span class=\"p\">.</span><span class=\"nv\">Conv2d</span><span class=\"p\">(</span><span class=\"mi\">64</span> <span class=\"o\">*</span> <span class=\"s s-Atom\">block_expansion</span><span class=\"p\">,</span> <span class=\"mi\">64</span> <span class=\"o\">*</span> <span class=\"s s-Atom\">block_expansion</span><span class=\"p\">,</span> <span class=\"s s-Atom\">kernel_size</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"s s-Atom\">stride</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"s s-Atom\">padding</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n",
       "\n",
       "        <span class=\"s s-Atom\">#</span> <span class=\"s s-Atom\">lateral</span> <span class=\"s s-Atom\">layers</span>\n",
       "        \n",
       "        <span class=\"s s-Atom\">self</span><span class=\"p\">.</span><span class=\"s s-Atom\">latlayer1</span> <span class=\"o\">=</span> <span class=\"s s-Atom\">nn</span><span class=\"p\">.</span><span class=\"nv\">Conv2d</span><span class=\"p\">(</span>\n",
       "            <span class=\"mi\">512</span> <span class=\"o\">*</span> <span class=\"s s-Atom\">block_expansion</span><span class=\"p\">,</span> <span class=\"mi\">64</span> <span class=\"o\">*</span> <span class=\"s s-Atom\">block_expansion</span><span class=\"p\">,</span> <span class=\"s s-Atom\">kernel_size</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"s s-Atom\">stride</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"s s-Atom\">padding</span><span class=\"o\">=</span><span class=\"mi\">0</span>\n",
       "        <span class=\"p\">)</span>\n",
       "        <span class=\"s s-Atom\">self</span><span class=\"p\">.</span><span class=\"s s-Atom\">latlayer2</span> <span class=\"o\">=</span> <span class=\"s s-Atom\">nn</span><span class=\"p\">.</span><span class=\"nv\">Conv2d</span><span class=\"p\">(</span>\n",
       "            <span class=\"mi\">256</span> <span class=\"o\">*</span> <span class=\"s s-Atom\">block_expansion</span><span class=\"p\">,</span> <span class=\"mi\">64</span> <span class=\"o\">*</span> <span class=\"s s-Atom\">block_expansion</span><span class=\"p\">,</span> <span class=\"s s-Atom\">kernel_size</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"s s-Atom\">stride</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"s s-Atom\">padding</span><span class=\"o\">=</span><span class=\"mi\">0</span>\n",
       "        <span class=\"p\">)</span>\n",
       "        <span class=\"s s-Atom\">self</span><span class=\"p\">.</span><span class=\"s s-Atom\">latlayer3</span> <span class=\"o\">=</span> <span class=\"s s-Atom\">nn</span><span class=\"p\">.</span><span class=\"nv\">Conv2d</span><span class=\"p\">(</span>\n",
       "            <span class=\"mi\">128</span> <span class=\"o\">*</span> <span class=\"s s-Atom\">block_expansion</span><span class=\"p\">,</span> <span class=\"mi\">64</span> <span class=\"o\">*</span> <span class=\"s s-Atom\">block_expansion</span><span class=\"p\">,</span> <span class=\"s s-Atom\">kernel_size</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"s s-Atom\">stride</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"s s-Atom\">padding</span><span class=\"o\">=</span><span class=\"mi\">0</span>\n",
       "        <span class=\"p\">)</span>\n",
       "\n",
       "        <span class=\"s s-Atom\">#</span> <span class=\"s s-Atom\">top</span><span class=\"o\">-</span><span class=\"s s-Atom\">down</span> <span class=\"s s-Atom\">layers</span>\n",
       "        <span class=\"s s-Atom\">self</span><span class=\"p\">.</span><span class=\"s s-Atom\">toplayer1</span> <span class=\"o\">=</span> <span class=\"s s-Atom\">nn</span><span class=\"p\">.</span><span class=\"nv\">Conv2d</span><span class=\"p\">(</span>\n",
       "            <span class=\"mi\">64</span> <span class=\"o\">*</span> <span class=\"s s-Atom\">block_expansion</span><span class=\"p\">,</span> <span class=\"mi\">64</span> <span class=\"o\">*</span> <span class=\"s s-Atom\">block_expansion</span><span class=\"p\">,</span> <span class=\"s s-Atom\">kernel_size</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"s s-Atom\">stride</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"s s-Atom\">padding</span><span class=\"o\">=</span><span class=\"mi\">1</span>\n",
       "        <span class=\"p\">)</span>\n",
       "        <span class=\"s s-Atom\">self</span><span class=\"p\">.</span><span class=\"s s-Atom\">toplayer2</span> <span class=\"o\">=</span> <span class=\"s s-Atom\">nn</span><span class=\"p\">.</span><span class=\"nv\">Conv2d</span><span class=\"p\">(</span>\n",
       "            <span class=\"mi\">64</span> <span class=\"o\">*</span> <span class=\"s s-Atom\">block_expansion</span><span class=\"p\">,</span> <span class=\"mi\">64</span> <span class=\"o\">*</span> <span class=\"s s-Atom\">block_expansion</span><span class=\"p\">,</span> <span class=\"s s-Atom\">kernel_size</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"s s-Atom\">stride</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"s s-Atom\">padding</span><span class=\"o\">=</span><span class=\"mi\">1</span>\n",
       "        <span class=\"p\">)</span>\n",
       "\n",
       "    <span class=\"s s-Atom\">@staticmethod</span>\n",
       "    <span class=\"s s-Atom\">def</span> <span class=\"k\">_</span><span class=\"nf\">upsample_add</span><span class=\"p\">(</span><span class=\"s s-Atom\">x</span><span class=\"p\">,</span> <span class=\"s s-Atom\">y</span><span class=\"p\">)</span><span class=\"s s-Atom\">:</span>\n",
       "        <span class=\"s s-Atom\">&#39;&#39;&#39;Upsample and add two feature maps.</span>\n",
       "\n",
       "<span class=\"s s-Atom\">        Args:</span>\n",
       "<span class=\"s s-Atom\">          x: (Variable) top feature map to be upsampled.</span>\n",
       "<span class=\"s s-Atom\">          y: (Variable) lateral feature map.</span>\n",
       "\n",
       "<span class=\"s s-Atom\">        Returns:</span>\n",
       "<span class=\"s s-Atom\">          (Variable) added feature map.</span>\n",
       "\n",
       "<span class=\"s s-Atom\">        Note in PyTorch, when input size is odd, the upsampled feature map</span>\n",
       "<span class=\"s s-Atom\">        with `F.interpolate(..., scale_factor=2, mode=&#39;nearest&#39;)`</span>\n",
       "<span class=\"s s-Atom\">        maybe not equal to the lateral feature map size.</span>\n",
       "\n",
       "<span class=\"s s-Atom\">        e.g.</span>\n",
       "<span class=\"s s-Atom\">        original input size: [N,_,15,15] -&gt;</span>\n",
       "<span class=\"s s-Atom\">        conv2d feature map size: [N,_,8,8] -&gt;</span>\n",
       "<span class=\"s s-Atom\">        upsampled feature map size: [N,_,16,16]</span>\n",
       "\n",
       "<span class=\"s s-Atom\">        So we choose bilinear upsample which supports arbitrary output sizes.</span>\n",
       "<span class=\"s s-Atom\">        &#39;&#39;&#39;</span>\n",
       "        <span class=\"k\">_</span><span class=\"p\">,</span> <span class=\"k\">_</span><span class=\"p\">,</span> <span class=\"s s-Atom\">height</span><span class=\"p\">,</span> <span class=\"s s-Atom\">width</span> <span class=\"o\">=</span> <span class=\"s s-Atom\">y</span><span class=\"p\">.</span><span class=\"nf\">size</span><span class=\"p\">()</span>\n",
       "        <span class=\"s s-Atom\">return</span> <span class=\"nv\">F</span><span class=\"p\">.</span><span class=\"nf\">interpolate</span><span class=\"p\">(</span><span class=\"s s-Atom\">x</span><span class=\"p\">,</span> <span class=\"s s-Atom\">size=</span><span class=\"p\">(</span><span class=\"s s-Atom\">height</span><span class=\"p\">,</span> <span class=\"s s-Atom\">width</span><span class=\"p\">),</span> <span class=\"s s-Atom\">mode=&#39;bilinear&#39;</span><span class=\"p\">,</span> <span class=\"s s-Atom\">align_corners</span><span class=\"o\">=</span><span class=\"nv\">True</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"s s-Atom\">y</span>\n",
       "\n",
       "    <span class=\"s s-Atom\">def</span> <span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"s s-Atom\">self</span><span class=\"p\">,</span> <span class=\"s s-Atom\">x</span><span class=\"p\">)</span><span class=\"s s-Atom\">:</span>\n",
       "        <span class=\"s s-Atom\">#</span> <span class=\"s s-Atom\">bottom</span><span class=\"o\">-</span><span class=\"s s-Atom\">up</span>\n",
       "        <span class=\"s s-Atom\">x</span> <span class=\"o\">=</span> <span class=\"s s-Atom\">self</span><span class=\"p\">.</span><span class=\"s s-Atom\">feature_extractor</span><span class=\"p\">.</span><span class=\"nf\">conv1</span><span class=\"p\">(</span><span class=\"s s-Atom\">x</span><span class=\"p\">)</span>\n",
       "        <span class=\"s s-Atom\">x</span> <span class=\"o\">=</span> <span class=\"s s-Atom\">self</span><span class=\"p\">.</span><span class=\"s s-Atom\">feature_extractor</span><span class=\"p\">.</span><span class=\"nf\">bn1</span><span class=\"p\">(</span><span class=\"s s-Atom\">x</span><span class=\"p\">)</span>\n",
       "        <span class=\"s s-Atom\">x</span> <span class=\"o\">=</span> <span class=\"s s-Atom\">self</span><span class=\"p\">.</span><span class=\"s s-Atom\">feature_extractor</span><span class=\"p\">.</span><span class=\"nf\">relu</span><span class=\"p\">(</span><span class=\"s s-Atom\">x</span><span class=\"p\">)</span>\n",
       "        <span class=\"s s-Atom\">x</span> <span class=\"o\">=</span> <span class=\"s s-Atom\">self</span><span class=\"p\">.</span><span class=\"s s-Atom\">feature_extractor</span><span class=\"p\">.</span><span class=\"nf\">maxpool</span><span class=\"p\">(</span><span class=\"s s-Atom\">x</span><span class=\"p\">)</span>\n",
       "        <span class=\"s s-Atom\">layer1_output</span> <span class=\"o\">=</span> <span class=\"s s-Atom\">self</span><span class=\"p\">.</span><span class=\"s s-Atom\">feature_extractor</span><span class=\"p\">.</span><span class=\"nf\">layer1</span><span class=\"p\">(</span><span class=\"s s-Atom\">x</span><span class=\"p\">)</span>\n",
       "        <span class=\"s s-Atom\">layer2_output</span> <span class=\"o\">=</span> <span class=\"s s-Atom\">self</span><span class=\"p\">.</span><span class=\"s s-Atom\">feature_extractor</span><span class=\"p\">.</span><span class=\"nf\">layer2</span><span class=\"p\">(</span><span class=\"s s-Atom\">layer1_output</span><span class=\"p\">)</span>\n",
       "        <span class=\"s s-Atom\">layer3_output</span> <span class=\"o\">=</span> <span class=\"s s-Atom\">self</span><span class=\"p\">.</span><span class=\"s s-Atom\">feature_extractor</span><span class=\"p\">.</span><span class=\"nf\">layer3</span><span class=\"p\">(</span><span class=\"s s-Atom\">layer2_output</span><span class=\"p\">)</span>\n",
       "        <span class=\"s s-Atom\">layer4_output</span> <span class=\"o\">=</span> <span class=\"s s-Atom\">self</span><span class=\"p\">.</span><span class=\"s s-Atom\">feature_extractor</span><span class=\"p\">.</span><span class=\"nf\">layer4</span><span class=\"p\">(</span><span class=\"s s-Atom\">layer3_output</span><span class=\"p\">)</span>\n",
       "\n",
       "        <span class=\"s s-Atom\">output</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n",
       "        \n",
       "        <span class=\"s s-Atom\">#</span> <span class=\"s s-Atom\">conv6</span> <span class=\"s s-Atom\">output</span><span class=\"p\">.</span> <span class=\"s s-Atom\">input</span> <span class=\"o\">is</span> <span class=\"s s-Atom\">output</span> <span class=\"s s-Atom\">of</span> <span class=\"s s-Atom\">layer4</span>\n",
       "        <span class=\"s s-Atom\">embedding</span> <span class=\"o\">=</span> <span class=\"s s-Atom\">self</span><span class=\"p\">.</span><span class=\"nf\">conv6</span><span class=\"p\">(</span><span class=\"s s-Atom\">layer4_output</span><span class=\"p\">)</span>\n",
       "        \n",
       "        <span class=\"s s-Atom\">#</span> <span class=\"s s-Atom\">conv7</span> <span class=\"s s-Atom\">output</span><span class=\"p\">.</span> <span class=\"s s-Atom\">input</span> <span class=\"o\">is</span> <span class=\"s s-Atom\">relu</span> <span class=\"s s-Atom\">activation</span> <span class=\"s s-Atom\">of</span> <span class=\"s s-Atom\">conv6</span> <span class=\"s s-Atom\">output</span>\n",
       "        <span class=\"s s-Atom\">output</span><span class=\"p\">.</span><span class=\"nf\">append</span><span class=\"p\">(</span><span class=\"s s-Atom\">self</span><span class=\"p\">.</span><span class=\"nf\">conv7</span><span class=\"p\">(</span><span class=\"nv\">F</span><span class=\"p\">.</span><span class=\"nf\">relu</span><span class=\"p\">(</span><span class=\"s s-Atom\">embedding</span><span class=\"p\">)))</span>\n",
       "        <span class=\"s s-Atom\">output</span><span class=\"p\">.</span><span class=\"nf\">append</span><span class=\"p\">(</span><span class=\"s s-Atom\">embedding</span><span class=\"p\">)</span>\n",
       "        \n",
       "        <span class=\"s s-Atom\">#</span> <span class=\"s s-Atom\">top</span><span class=\"o\">-</span><span class=\"s s-Atom\">down</span>\n",
       "        <span class=\"s s-Atom\">output</span><span class=\"p\">.</span><span class=\"nf\">append</span><span class=\"p\">(</span><span class=\"s s-Atom\">self</span><span class=\"p\">.</span><span class=\"nf\">latlayer1</span><span class=\"p\">(</span><span class=\"s s-Atom\">layer4_output</span><span class=\"p\">))</span>\n",
       "        <span class=\"s s-Atom\">output</span><span class=\"p\">.</span><span class=\"nf\">append</span><span class=\"p\">(</span><span class=\"s s-Atom\">self</span><span class=\"p\">.</span><span class=\"nf\">toplayer1</span><span class=\"p\">(</span><span class=\"s s-Atom\">self</span><span class=\"p\">.</span><span class=\"k\">_</span><span class=\"nf\">upsample_add</span><span class=\"p\">(</span><span class=\"s s-Atom\">output</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"s s-Atom\">self</span><span class=\"p\">.</span><span class=\"nf\">latlayer2</span><span class=\"p\">(</span><span class=\"s s-Atom\">layer3_output</span><span class=\"p\">))))</span>\n",
       "        <span class=\"s s-Atom\">output</span><span class=\"p\">.</span><span class=\"nf\">append</span><span class=\"p\">(</span><span class=\"s s-Atom\">self</span><span class=\"p\">.</span><span class=\"nf\">toplayer2</span><span class=\"p\">(</span><span class=\"s s-Atom\">self</span><span class=\"p\">.</span><span class=\"k\">_</span><span class=\"nf\">upsample_add</span><span class=\"p\">(</span><span class=\"s s-Atom\">output</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"s s-Atom\">self</span><span class=\"p\">.</span><span class=\"nf\">latlayer3</span><span class=\"p\">(</span><span class=\"s s-Atom\">layer2_output</span><span class=\"p\">))))</span>\n",
       "        \n",
       "        <span class=\"s s-Atom\">return</span> <span class=\"s s-Atom\">output</span><span class=\"p\">[</span><span class=\"s s-Atom\">::-</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n",
       "</pre></div>\n"
      ],
      "text/latex": [
       "\\begin{Verbatim}[commandchars=\\\\\\{\\}]\n",
       "\\PY{l+s+sAtom}{class} \\PY{n+nv}{FPN}\\PY{p}{(}\\PY{l+s+sAtom}{nn}\\PY{p}{.}\\PY{n+nv}{Module}\\PY{p}{)}\\PY{l+s+sAtom}{:}\n",
       "    \\PY{l+s+sAtom}{def} \\PY{k}{\\PYZus{}}\\PY{k}{\\PYZus{}}\\PY{n+nf}{init\\PYZus{}\\PYZus{}}\\PY{p}{(}\\PY{l+s+sAtom}{self}\\PY{p}{,} \\PY{l+s+sAtom}{block\\PYZus{}expansion}\\PY{o}{=}\\PY{l+m+mi}{1}\\PY{p}{,} \\PY{l+s+sAtom}{backbone}\\PY{l+s+sAtom}{=}\\PY{l+s+s2}{\\PYZdq{}resnet18\\PYZdq{}}\\PY{p}{)}\\PY{l+s+sAtom}{:}\n",
       "        \\PY{n+nf}{super}\\PY{p}{(}\\PY{p}{)}\\PY{p}{.}\\PY{k}{\\PYZus{}}\\PY{k}{\\PYZus{}}\\PY{n+nf}{init\\PYZus{}\\PYZus{}}\\PY{p}{(}\\PY{p}{)}\n",
       "        \\PY{l+s+sAtom}{assert} \\PY{n+nf}{hasattr}\\PY{p}{(}\\PY{l+s+sAtom}{models}\\PY{p}{,} \\PY{l+s+sAtom}{backbone}\\PY{p}{)}\\PY{p}{,} \\PY{l+s+s2}{\\PYZdq{}Undefined encoder type\\PYZdq{}}\n",
       "        \n",
       "        \\PY{l+s+sAtom}{\\PYZsh{}} \\PY{l+s+sAtom}{load} \\PY{l+s+sAtom}{model} \n",
       "        \\PY{l+s+sAtom}{self}\\PY{p}{.}\\PY{l+s+sAtom}{feature\\PYZus{}extractor} \\PY{o}{=} \\PY{n+nf}{getattr}\\PY{p}{(}\\PY{l+s+sAtom}{models}\\PY{p}{,} \\PY{l+s+sAtom}{backbone}\\PY{p}{)}\\PY{p}{(}\\PY{l+s+sAtom}{pretrained}\\PY{o}{=}\\PY{n+nv}{True}\\PY{p}{)}\n",
       "        \n",
       "        \\PY{l+s+sAtom}{\\PYZsh{}} \\PY{l+s+sAtom}{two} \\PY{l+s+sAtom}{more} \\PY{l+s+sAtom}{layers} \\PY{l+s+sAtom}{conv6} \\PY{l+s+sAtom}{and} \\PY{l+s+sAtom}{conv7} \\PY{l+s+sAtom}{on} \\PY{l+s+sAtom}{the} \\PY{l+s+sAtom}{top} \\PY{l+s+sAtom}{of} \\PY{n+nf}{layer4} \\PY{p}{(}\\PY{l+s+sAtom}{if} \\PY{l+s+sAtom}{backbone} \\PY{o}{is} \\PY{l+s+sAtom}{resnet18}\\PY{p}{)}\n",
       "        \n",
       "        \\PY{l+s+sAtom}{self}\\PY{p}{.}\\PY{l+s+sAtom}{conv6} \\PY{o}{=} \\PY{l+s+sAtom}{nn}\\PY{p}{.}\\PY{n+nv}{Conv2d}\\PY{p}{(}\n",
       "            \\PY{l+m+mi}{512} \\PY{o}{*} \\PY{l+s+sAtom}{block\\PYZus{}expansion}\\PY{p}{,} \\PY{l+m+mi}{64} \\PY{o}{*} \\PY{l+s+sAtom}{block\\PYZus{}expansion}\\PY{p}{,} \\PY{l+s+sAtom}{kernel\\PYZus{}size}\\PY{o}{=}\\PY{l+m+mi}{3}\\PY{p}{,} \\PY{l+s+sAtom}{stride}\\PY{o}{=}\\PY{l+m+mi}{2}\\PY{p}{,} \\PY{l+s+sAtom}{padding}\\PY{o}{=}\\PY{l+m+mi}{1}\n",
       "        \\PY{p}{)}\n",
       "        \\PY{l+s+sAtom}{self}\\PY{p}{.}\\PY{l+s+sAtom}{conv7} \\PY{o}{=} \\PY{l+s+sAtom}{nn}\\PY{p}{.}\\PY{n+nv}{Conv2d}\\PY{p}{(}\\PY{l+m+mi}{64} \\PY{o}{*} \\PY{l+s+sAtom}{block\\PYZus{}expansion}\\PY{p}{,} \\PY{l+m+mi}{64} \\PY{o}{*} \\PY{l+s+sAtom}{block\\PYZus{}expansion}\\PY{p}{,} \\PY{l+s+sAtom}{kernel\\PYZus{}size}\\PY{o}{=}\\PY{l+m+mi}{3}\\PY{p}{,} \\PY{l+s+sAtom}{stride}\\PY{o}{=}\\PY{l+m+mi}{2}\\PY{p}{,} \\PY{l+s+sAtom}{padding}\\PY{o}{=}\\PY{l+m+mi}{1}\\PY{p}{)}\n",
       "\n",
       "        \\PY{l+s+sAtom}{\\PYZsh{}} \\PY{l+s+sAtom}{lateral} \\PY{l+s+sAtom}{layers}\n",
       "        \n",
       "        \\PY{l+s+sAtom}{self}\\PY{p}{.}\\PY{l+s+sAtom}{latlayer1} \\PY{o}{=} \\PY{l+s+sAtom}{nn}\\PY{p}{.}\\PY{n+nv}{Conv2d}\\PY{p}{(}\n",
       "            \\PY{l+m+mi}{512} \\PY{o}{*} \\PY{l+s+sAtom}{block\\PYZus{}expansion}\\PY{p}{,} \\PY{l+m+mi}{64} \\PY{o}{*} \\PY{l+s+sAtom}{block\\PYZus{}expansion}\\PY{p}{,} \\PY{l+s+sAtom}{kernel\\PYZus{}size}\\PY{o}{=}\\PY{l+m+mi}{1}\\PY{p}{,} \\PY{l+s+sAtom}{stride}\\PY{o}{=}\\PY{l+m+mi}{1}\\PY{p}{,} \\PY{l+s+sAtom}{padding}\\PY{o}{=}\\PY{l+m+mi}{0}\n",
       "        \\PY{p}{)}\n",
       "        \\PY{l+s+sAtom}{self}\\PY{p}{.}\\PY{l+s+sAtom}{latlayer2} \\PY{o}{=} \\PY{l+s+sAtom}{nn}\\PY{p}{.}\\PY{n+nv}{Conv2d}\\PY{p}{(}\n",
       "            \\PY{l+m+mi}{256} \\PY{o}{*} \\PY{l+s+sAtom}{block\\PYZus{}expansion}\\PY{p}{,} \\PY{l+m+mi}{64} \\PY{o}{*} \\PY{l+s+sAtom}{block\\PYZus{}expansion}\\PY{p}{,} \\PY{l+s+sAtom}{kernel\\PYZus{}size}\\PY{o}{=}\\PY{l+m+mi}{1}\\PY{p}{,} \\PY{l+s+sAtom}{stride}\\PY{o}{=}\\PY{l+m+mi}{1}\\PY{p}{,} \\PY{l+s+sAtom}{padding}\\PY{o}{=}\\PY{l+m+mi}{0}\n",
       "        \\PY{p}{)}\n",
       "        \\PY{l+s+sAtom}{self}\\PY{p}{.}\\PY{l+s+sAtom}{latlayer3} \\PY{o}{=} \\PY{l+s+sAtom}{nn}\\PY{p}{.}\\PY{n+nv}{Conv2d}\\PY{p}{(}\n",
       "            \\PY{l+m+mi}{128} \\PY{o}{*} \\PY{l+s+sAtom}{block\\PYZus{}expansion}\\PY{p}{,} \\PY{l+m+mi}{64} \\PY{o}{*} \\PY{l+s+sAtom}{block\\PYZus{}expansion}\\PY{p}{,} \\PY{l+s+sAtom}{kernel\\PYZus{}size}\\PY{o}{=}\\PY{l+m+mi}{1}\\PY{p}{,} \\PY{l+s+sAtom}{stride}\\PY{o}{=}\\PY{l+m+mi}{1}\\PY{p}{,} \\PY{l+s+sAtom}{padding}\\PY{o}{=}\\PY{l+m+mi}{0}\n",
       "        \\PY{p}{)}\n",
       "\n",
       "        \\PY{l+s+sAtom}{\\PYZsh{}} \\PY{l+s+sAtom}{top}\\PY{o}{\\PYZhy{}}\\PY{l+s+sAtom}{down} \\PY{l+s+sAtom}{layers}\n",
       "        \\PY{l+s+sAtom}{self}\\PY{p}{.}\\PY{l+s+sAtom}{toplayer1} \\PY{o}{=} \\PY{l+s+sAtom}{nn}\\PY{p}{.}\\PY{n+nv}{Conv2d}\\PY{p}{(}\n",
       "            \\PY{l+m+mi}{64} \\PY{o}{*} \\PY{l+s+sAtom}{block\\PYZus{}expansion}\\PY{p}{,} \\PY{l+m+mi}{64} \\PY{o}{*} \\PY{l+s+sAtom}{block\\PYZus{}expansion}\\PY{p}{,} \\PY{l+s+sAtom}{kernel\\PYZus{}size}\\PY{o}{=}\\PY{l+m+mi}{3}\\PY{p}{,} \\PY{l+s+sAtom}{stride}\\PY{o}{=}\\PY{l+m+mi}{1}\\PY{p}{,} \\PY{l+s+sAtom}{padding}\\PY{o}{=}\\PY{l+m+mi}{1}\n",
       "        \\PY{p}{)}\n",
       "        \\PY{l+s+sAtom}{self}\\PY{p}{.}\\PY{l+s+sAtom}{toplayer2} \\PY{o}{=} \\PY{l+s+sAtom}{nn}\\PY{p}{.}\\PY{n+nv}{Conv2d}\\PY{p}{(}\n",
       "            \\PY{l+m+mi}{64} \\PY{o}{*} \\PY{l+s+sAtom}{block\\PYZus{}expansion}\\PY{p}{,} \\PY{l+m+mi}{64} \\PY{o}{*} \\PY{l+s+sAtom}{block\\PYZus{}expansion}\\PY{p}{,} \\PY{l+s+sAtom}{kernel\\PYZus{}size}\\PY{o}{=}\\PY{l+m+mi}{3}\\PY{p}{,} \\PY{l+s+sAtom}{stride}\\PY{o}{=}\\PY{l+m+mi}{1}\\PY{p}{,} \\PY{l+s+sAtom}{padding}\\PY{o}{=}\\PY{l+m+mi}{1}\n",
       "        \\PY{p}{)}\n",
       "\n",
       "    \\PY{l+s+sAtom}{@}\\PY{l+s+sAtom}{staticmethod}\n",
       "    \\PY{l+s+sAtom}{def} \\PY{k}{\\PYZus{}}\\PY{n+nf}{upsample\\PYZus{}add}\\PY{p}{(}\\PY{l+s+sAtom}{x}\\PY{p}{,} \\PY{l+s+sAtom}{y}\\PY{p}{)}\\PY{l+s+sAtom}{:}\n",
       "        \\PY{l+s+sAtom}{\\PYZsq{}\\PYZsq{}\\PYZsq{}Upsample and add two feature maps.}\n",
       "\n",
       "\\PY{l+s+sAtom}{        Args:}\n",
       "\\PY{l+s+sAtom}{          x: (Variable) top feature map to be upsampled.}\n",
       "\\PY{l+s+sAtom}{          y: (Variable) lateral feature map.}\n",
       "\n",
       "\\PY{l+s+sAtom}{        Returns:}\n",
       "\\PY{l+s+sAtom}{          (Variable) added feature map.}\n",
       "\n",
       "\\PY{l+s+sAtom}{        Note in PyTorch, when input size is odd, the upsampled feature map}\n",
       "\\PY{l+s+sAtom}{        with `F.interpolate(..., scale\\PYZus{}factor=2, mode=\\PYZsq{}}\\PY{l+s+sAtom}{nearest}\\PY{l+s+sAtom}{\\PYZsq{})`}\n",
       "\\PY{l+s+sAtom}{        maybe not equal to the lateral feature map size.}\n",
       "\n",
       "\\PY{l+s+sAtom}{        e.g.}\n",
       "\\PY{l+s+sAtom}{        original input size: [N,\\PYZus{},15,15] \\PYZhy{}\\PYZgt{}}\n",
       "\\PY{l+s+sAtom}{        conv2d feature map size: [N,\\PYZus{},8,8] \\PYZhy{}\\PYZgt{}}\n",
       "\\PY{l+s+sAtom}{        upsampled feature map size: [N,\\PYZus{},16,16]}\n",
       "\n",
       "\\PY{l+s+sAtom}{        So we choose bilinear upsample which supports arbitrary output sizes.}\n",
       "\\PY{l+s+sAtom}{        \\PYZsq{}\\PYZsq{}\\PYZsq{}}\n",
       "        \\PY{k}{\\PYZus{}}\\PY{p}{,} \\PY{k}{\\PYZus{}}\\PY{p}{,} \\PY{l+s+sAtom}{height}\\PY{p}{,} \\PY{l+s+sAtom}{width} \\PY{o}{=} \\PY{l+s+sAtom}{y}\\PY{p}{.}\\PY{n+nf}{size}\\PY{p}{(}\\PY{p}{)}\n",
       "        \\PY{l+s+sAtom}{return} \\PY{n+nv}{F}\\PY{p}{.}\\PY{n+nf}{interpolate}\\PY{p}{(}\\PY{l+s+sAtom}{x}\\PY{p}{,} \\PY{l+s+sAtom}{size}\\PY{l+s+sAtom}{=}\\PY{p}{(}\\PY{l+s+sAtom}{height}\\PY{p}{,} \\PY{l+s+sAtom}{width}\\PY{p}{)}\\PY{p}{,} \\PY{l+s+sAtom}{mode}\\PY{l+s+sAtom}{=}\\PY{l+s+sAtom}{\\PYZsq{}bilinear\\PYZsq{}}\\PY{p}{,} \\PY{l+s+sAtom}{align\\PYZus{}corners}\\PY{o}{=}\\PY{n+nv}{True}\\PY{p}{)} \\PY{o}{+} \\PY{l+s+sAtom}{y}\n",
       "\n",
       "    \\PY{l+s+sAtom}{def} \\PY{n+nf}{forward}\\PY{p}{(}\\PY{l+s+sAtom}{self}\\PY{p}{,} \\PY{l+s+sAtom}{x}\\PY{p}{)}\\PY{l+s+sAtom}{:}\n",
       "        \\PY{l+s+sAtom}{\\PYZsh{}} \\PY{l+s+sAtom}{bottom}\\PY{o}{\\PYZhy{}}\\PY{l+s+sAtom}{up}\n",
       "        \\PY{l+s+sAtom}{x} \\PY{o}{=} \\PY{l+s+sAtom}{self}\\PY{p}{.}\\PY{l+s+sAtom}{feature\\PYZus{}extractor}\\PY{p}{.}\\PY{n+nf}{conv1}\\PY{p}{(}\\PY{l+s+sAtom}{x}\\PY{p}{)}\n",
       "        \\PY{l+s+sAtom}{x} \\PY{o}{=} \\PY{l+s+sAtom}{self}\\PY{p}{.}\\PY{l+s+sAtom}{feature\\PYZus{}extractor}\\PY{p}{.}\\PY{n+nf}{bn1}\\PY{p}{(}\\PY{l+s+sAtom}{x}\\PY{p}{)}\n",
       "        \\PY{l+s+sAtom}{x} \\PY{o}{=} \\PY{l+s+sAtom}{self}\\PY{p}{.}\\PY{l+s+sAtom}{feature\\PYZus{}extractor}\\PY{p}{.}\\PY{n+nf}{relu}\\PY{p}{(}\\PY{l+s+sAtom}{x}\\PY{p}{)}\n",
       "        \\PY{l+s+sAtom}{x} \\PY{o}{=} \\PY{l+s+sAtom}{self}\\PY{p}{.}\\PY{l+s+sAtom}{feature\\PYZus{}extractor}\\PY{p}{.}\\PY{n+nf}{maxpool}\\PY{p}{(}\\PY{l+s+sAtom}{x}\\PY{p}{)}\n",
       "        \\PY{l+s+sAtom}{layer1\\PYZus{}output} \\PY{o}{=} \\PY{l+s+sAtom}{self}\\PY{p}{.}\\PY{l+s+sAtom}{feature\\PYZus{}extractor}\\PY{p}{.}\\PY{n+nf}{layer1}\\PY{p}{(}\\PY{l+s+sAtom}{x}\\PY{p}{)}\n",
       "        \\PY{l+s+sAtom}{layer2\\PYZus{}output} \\PY{o}{=} \\PY{l+s+sAtom}{self}\\PY{p}{.}\\PY{l+s+sAtom}{feature\\PYZus{}extractor}\\PY{p}{.}\\PY{n+nf}{layer2}\\PY{p}{(}\\PY{l+s+sAtom}{layer1\\PYZus{}output}\\PY{p}{)}\n",
       "        \\PY{l+s+sAtom}{layer3\\PYZus{}output} \\PY{o}{=} \\PY{l+s+sAtom}{self}\\PY{p}{.}\\PY{l+s+sAtom}{feature\\PYZus{}extractor}\\PY{p}{.}\\PY{n+nf}{layer3}\\PY{p}{(}\\PY{l+s+sAtom}{layer2\\PYZus{}output}\\PY{p}{)}\n",
       "        \\PY{l+s+sAtom}{layer4\\PYZus{}output} \\PY{o}{=} \\PY{l+s+sAtom}{self}\\PY{p}{.}\\PY{l+s+sAtom}{feature\\PYZus{}extractor}\\PY{p}{.}\\PY{n+nf}{layer4}\\PY{p}{(}\\PY{l+s+sAtom}{layer3\\PYZus{}output}\\PY{p}{)}\n",
       "\n",
       "        \\PY{l+s+sAtom}{output} \\PY{o}{=} \\PY{p}{[}\\PY{p}{]}\n",
       "        \n",
       "        \\PY{l+s+sAtom}{\\PYZsh{}} \\PY{l+s+sAtom}{conv6} \\PY{l+s+sAtom}{output}\\PY{p}{.} \\PY{l+s+sAtom}{input} \\PY{o}{is} \\PY{l+s+sAtom}{output} \\PY{l+s+sAtom}{of} \\PY{l+s+sAtom}{layer4}\n",
       "        \\PY{l+s+sAtom}{embedding} \\PY{o}{=} \\PY{l+s+sAtom}{self}\\PY{p}{.}\\PY{n+nf}{conv6}\\PY{p}{(}\\PY{l+s+sAtom}{layer4\\PYZus{}output}\\PY{p}{)}\n",
       "        \n",
       "        \\PY{l+s+sAtom}{\\PYZsh{}} \\PY{l+s+sAtom}{conv7} \\PY{l+s+sAtom}{output}\\PY{p}{.} \\PY{l+s+sAtom}{input} \\PY{o}{is} \\PY{l+s+sAtom}{relu} \\PY{l+s+sAtom}{activation} \\PY{l+s+sAtom}{of} \\PY{l+s+sAtom}{conv6} \\PY{l+s+sAtom}{output}\n",
       "        \\PY{l+s+sAtom}{output}\\PY{p}{.}\\PY{n+nf}{append}\\PY{p}{(}\\PY{l+s+sAtom}{self}\\PY{p}{.}\\PY{n+nf}{conv7}\\PY{p}{(}\\PY{n+nv}{F}\\PY{p}{.}\\PY{n+nf}{relu}\\PY{p}{(}\\PY{l+s+sAtom}{embedding}\\PY{p}{)}\\PY{p}{)}\\PY{p}{)}\n",
       "        \\PY{l+s+sAtom}{output}\\PY{p}{.}\\PY{n+nf}{append}\\PY{p}{(}\\PY{l+s+sAtom}{embedding}\\PY{p}{)}\n",
       "        \n",
       "        \\PY{l+s+sAtom}{\\PYZsh{}} \\PY{l+s+sAtom}{top}\\PY{o}{\\PYZhy{}}\\PY{l+s+sAtom}{down}\n",
       "        \\PY{l+s+sAtom}{output}\\PY{p}{.}\\PY{n+nf}{append}\\PY{p}{(}\\PY{l+s+sAtom}{self}\\PY{p}{.}\\PY{n+nf}{latlayer1}\\PY{p}{(}\\PY{l+s+sAtom}{layer4\\PYZus{}output}\\PY{p}{)}\\PY{p}{)}\n",
       "        \\PY{l+s+sAtom}{output}\\PY{p}{.}\\PY{n+nf}{append}\\PY{p}{(}\\PY{l+s+sAtom}{self}\\PY{p}{.}\\PY{n+nf}{toplayer1}\\PY{p}{(}\\PY{l+s+sAtom}{self}\\PY{p}{.}\\PY{k}{\\PYZus{}}\\PY{n+nf}{upsample\\PYZus{}add}\\PY{p}{(}\\PY{l+s+sAtom}{output}\\PY{p}{[}\\PY{o}{\\PYZhy{}}\\PY{l+m+mi}{1}\\PY{p}{]}\\PY{p}{,} \\PY{l+s+sAtom}{self}\\PY{p}{.}\\PY{n+nf}{latlayer2}\\PY{p}{(}\\PY{l+s+sAtom}{layer3\\PYZus{}output}\\PY{p}{)}\\PY{p}{)}\\PY{p}{)}\\PY{p}{)}\n",
       "        \\PY{l+s+sAtom}{output}\\PY{p}{.}\\PY{n+nf}{append}\\PY{p}{(}\\PY{l+s+sAtom}{self}\\PY{p}{.}\\PY{n+nf}{toplayer2}\\PY{p}{(}\\PY{l+s+sAtom}{self}\\PY{p}{.}\\PY{k}{\\PYZus{}}\\PY{n+nf}{upsample\\PYZus{}add}\\PY{p}{(}\\PY{l+s+sAtom}{output}\\PY{p}{[}\\PY{o}{\\PYZhy{}}\\PY{l+m+mi}{1}\\PY{p}{]}\\PY{p}{,} \\PY{l+s+sAtom}{self}\\PY{p}{.}\\PY{n+nf}{latlayer3}\\PY{p}{(}\\PY{l+s+sAtom}{layer2\\PYZus{}output}\\PY{p}{)}\\PY{p}{)}\\PY{p}{)}\\PY{p}{)}\n",
       "        \n",
       "        \\PY{l+s+sAtom}{return} \\PY{l+s+sAtom}{output}\\PY{p}{[}\\PY{l+s+sAtom}{::\\PYZhy{}}\\PY{l+m+mi}{1}\\PY{p}{]}\n",
       "\\end{Verbatim}\n"
      ],
      "text/plain": [
       "class FPN(nn.Module):\n",
       "    def __init__(self, block_expansion=1, backbone=\"resnet18\"):\n",
       "        super().__init__()\n",
       "        assert hasattr(models, backbone), \"Undefined encoder type\"\n",
       "        \n",
       "        # load model \n",
       "        self.feature_extractor = getattr(models, backbone)(pretrained=True)\n",
       "        \n",
       "        # two more layers conv6 and conv7 on the top of layer4 (if backbone is resnet18)\n",
       "        \n",
       "        self.conv6 = nn.Conv2d(\n",
       "            512 * block_expansion, 64 * block_expansion, kernel_size=3, stride=2, padding=1\n",
       "        )\n",
       "        self.conv7 = nn.Conv2d(64 * block_expansion, 64 * block_expansion, kernel_size=3, stride=2, padding=1)\n",
       "\n",
       "        # lateral layers\n",
       "        \n",
       "        self.latlayer1 = nn.Conv2d(\n",
       "            512 * block_expansion, 64 * block_expansion, kernel_size=1, stride=1, padding=0\n",
       "        )\n",
       "        self.latlayer2 = nn.Conv2d(\n",
       "            256 * block_expansion, 64 * block_expansion, kernel_size=1, stride=1, padding=0\n",
       "        )\n",
       "        self.latlayer3 = nn.Conv2d(\n",
       "            128 * block_expansion, 64 * block_expansion, kernel_size=1, stride=1, padding=0\n",
       "        )\n",
       "\n",
       "        # top-down layers\n",
       "        self.toplayer1 = nn.Conv2d(\n",
       "            64 * block_expansion, 64 * block_expansion, kernel_size=3, stride=1, padding=1\n",
       "        )\n",
       "        self.toplayer2 = nn.Conv2d(\n",
       "            64 * block_expansion, 64 * block_expansion, kernel_size=3, stride=1, padding=1\n",
       "        )\n",
       "\n",
       "    @staticmethod\n",
       "    def _upsample_add(x, y):\n",
       "        '''Upsample and add two feature maps.\n",
       "\n",
       "        Args:\n",
       "          x: (Variable) top feature map to be upsampled.\n",
       "          y: (Variable) lateral feature map.\n",
       "\n",
       "        Returns:\n",
       "          (Variable) added feature map.\n",
       "\n",
       "        Note in PyTorch, when input size is odd, the upsampled feature map\n",
       "        with `F.interpolate(..., scale_factor=2, mode='nearest')`\n",
       "        maybe not equal to the lateral feature map size.\n",
       "\n",
       "        e.g.\n",
       "        original input size: [N,_,15,15] ->\n",
       "        conv2d feature map size: [N,_,8,8] ->\n",
       "        upsampled feature map size: [N,_,16,16]\n",
       "\n",
       "        So we choose bilinear upsample which supports arbitrary output sizes.\n",
       "        '''\n",
       "        _, _, height, width = y.size()\n",
       "        return F.interpolate(x, size=(height, width), mode='bilinear', align_corners=True) + y\n",
       "\n",
       "    def forward(self, x):\n",
       "        # bottom-up\n",
       "        x = self.feature_extractor.conv1(x)\n",
       "        x = self.feature_extractor.bn1(x)\n",
       "        x = self.feature_extractor.relu(x)\n",
       "        x = self.feature_extractor.maxpool(x)\n",
       "        layer1_output = self.feature_extractor.layer1(x)\n",
       "        layer2_output = self.feature_extractor.layer2(layer1_output)\n",
       "        layer3_output = self.feature_extractor.layer3(layer2_output)\n",
       "        layer4_output = self.feature_extractor.layer4(layer3_output)\n",
       "\n",
       "        output = []\n",
       "        \n",
       "        # conv6 output. input is output of layer4\n",
       "        embedding = self.conv6(layer4_output)\n",
       "        \n",
       "        # conv7 output. input is relu activation of conv6 output\n",
       "        output.append(self.conv7(F.relu(embedding)))\n",
       "        output.append(embedding)\n",
       "        \n",
       "        # top-down\n",
       "        output.append(self.latlayer1(layer4_output))\n",
       "        output.append(self.toplayer1(self._upsample_add(output[-1], self.latlayer2(layer3_output))))\n",
       "        output.append(self.toplayer2(self._upsample_add(output[-1], self.latlayer3(layer2_output))))\n",
       "        \n",
       "        return output[::-1]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Code(data=inspect.getsource(FPN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that `FPN` has added two more convolution layers, `conv6` and `conv7` on the top of `layer4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 64, 32, 32])\n",
      "torch.Size([2, 64, 16, 16])\n",
      "torch.Size([2, 64, 8, 8])\n",
      "torch.Size([2, 64, 4, 4])\n",
      "torch.Size([2, 64, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "fpn = FPN()\n",
    "\n",
    "output = fpn(image_inputs)\n",
    "\n",
    "for layer in output:\n",
    "    print(layer.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that all layers have the same number of channels (`64`).\n",
    "\n",
    "We can also see that width and height is half of the previous layer width and height. \n",
    "\n",
    "Let's take another example of an input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 64, 38, 38])\n",
      "torch.Size([2, 64, 19, 19])\n",
      "torch.Size([2, 64, 10, 10])\n",
      "torch.Size([2, 64, 5, 5])\n",
      "torch.Size([2, 64, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "image_inputs = torch.rand((2, 3, 300, 300))\n",
    "\n",
    "output = fpn(image_inputs)\n",
    "\n",
    "for layer in output:\n",
    "    print(layer.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here number of channes is the same as above (`64`).\n",
    "\n",
    "But, width and height is not half of the previous layer width and height in all cases. \n",
    "\n",
    "We can use following expression to find next layer width and height:\n",
    "\n",
    "$$\n",
    "next\\_layer\\_width = \\big \\lceil {\\frac{current\\_layer\\_width}{2}} \\big\\rceil\n",
    "$$\n",
    "\n",
    "$$\n",
    "next\\_layer\\_height = \\big \\lceil {\\frac{current\\_layer\\_height}{2}} \\big\\rceil\n",
    "$$\n",
    "\n",
    "Verify this expression with different examples. You can also verify by observing `kernel_size`, `stride`, and `padding` of `conv2d` in `FPN` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">1.3. Prediction Network</font>\n",
    "\n",
    "Let us have a look at `Detector` class that implements our detector network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.output_html .hll { background-color: #ffffcc }\n",
       ".output_html  { background: #f8f8f8; }\n",
       ".output_html .c { color: #408080; font-style: italic } /* Comment */\n",
       ".output_html .err { border: 1px solid #FF0000 } /* Error */\n",
       ".output_html .k { color: #008000; font-weight: bold } /* Keyword */\n",
       ".output_html .o { color: #666666 } /* Operator */\n",
       ".output_html .ch { color: #408080; font-style: italic } /* Comment.Hashbang */\n",
       ".output_html .cm { color: #408080; font-style: italic } /* Comment.Multiline */\n",
       ".output_html .cp { color: #BC7A00 } /* Comment.Preproc */\n",
       ".output_html .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */\n",
       ".output_html .c1 { color: #408080; font-style: italic } /* Comment.Single */\n",
       ".output_html .cs { color: #408080; font-style: italic } /* Comment.Special */\n",
       ".output_html .gd { color: #A00000 } /* Generic.Deleted */\n",
       ".output_html .ge { font-style: italic } /* Generic.Emph */\n",
       ".output_html .gr { color: #FF0000 } /* Generic.Error */\n",
       ".output_html .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
       ".output_html .gi { color: #00A000 } /* Generic.Inserted */\n",
       ".output_html .go { color: #888888 } /* Generic.Output */\n",
       ".output_html .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
       ".output_html .gs { font-weight: bold } /* Generic.Strong */\n",
       ".output_html .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
       ".output_html .gt { color: #0044DD } /* Generic.Traceback */\n",
       ".output_html .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
       ".output_html .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
       ".output_html .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
       ".output_html .kp { color: #008000 } /* Keyword.Pseudo */\n",
       ".output_html .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
       ".output_html .kt { color: #B00040 } /* Keyword.Type */\n",
       ".output_html .m { color: #666666 } /* Literal.Number */\n",
       ".output_html .s { color: #BA2121 } /* Literal.String */\n",
       ".output_html .na { color: #7D9029 } /* Name.Attribute */\n",
       ".output_html .nb { color: #008000 } /* Name.Builtin */\n",
       ".output_html .nc { color: #0000FF; font-weight: bold } /* Name.Class */\n",
       ".output_html .no { color: #880000 } /* Name.Constant */\n",
       ".output_html .nd { color: #AA22FF } /* Name.Decorator */\n",
       ".output_html .ni { color: #999999; font-weight: bold } /* Name.Entity */\n",
       ".output_html .ne { color: #D2413A; font-weight: bold } /* Name.Exception */\n",
       ".output_html .nf { color: #0000FF } /* Name.Function */\n",
       ".output_html .nl { color: #A0A000 } /* Name.Label */\n",
       ".output_html .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n",
       ".output_html .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
       ".output_html .nv { color: #19177C } /* Name.Variable */\n",
       ".output_html .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n",
       ".output_html .w { color: #bbbbbb } /* Text.Whitespace */\n",
       ".output_html .mb { color: #666666 } /* Literal.Number.Bin */\n",
       ".output_html .mf { color: #666666 } /* Literal.Number.Float */\n",
       ".output_html .mh { color: #666666 } /* Literal.Number.Hex */\n",
       ".output_html .mi { color: #666666 } /* Literal.Number.Integer */\n",
       ".output_html .mo { color: #666666 } /* Literal.Number.Oct */\n",
       ".output_html .sa { color: #BA2121 } /* Literal.String.Affix */\n",
       ".output_html .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
       ".output_html .sc { color: #BA2121 } /* Literal.String.Char */\n",
       ".output_html .dl { color: #BA2121 } /* Literal.String.Delimiter */\n",
       ".output_html .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
       ".output_html .s2 { color: #BA2121 } /* Literal.String.Double */\n",
       ".output_html .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */\n",
       ".output_html .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
       ".output_html .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */\n",
       ".output_html .sx { color: #008000 } /* Literal.String.Other */\n",
       ".output_html .sr { color: #BB6688 } /* Literal.String.Regex */\n",
       ".output_html .s1 { color: #BA2121 } /* Literal.String.Single */\n",
       ".output_html .ss { color: #19177C } /* Literal.String.Symbol */\n",
       ".output_html .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       ".output_html .fm { color: #0000FF } /* Name.Function.Magic */\n",
       ".output_html .vc { color: #19177C } /* Name.Variable.Class */\n",
       ".output_html .vg { color: #19177C } /* Name.Variable.Global */\n",
       ".output_html .vi { color: #19177C } /* Name.Variable.Instance */\n",
       ".output_html .vm { color: #19177C } /* Name.Variable.Magic */\n",
       ".output_html .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span></span><span class=\"nv\">class</span> <span class=\"nv\">Detector</span><span class=\"ss\">(</span><span class=\"nv\">nn</span>.<span class=\"nv\">Module</span><span class=\"ss\">)</span>:\n",
       "    <span class=\"nv\">num_anchors</span> <span class=\"o\">=</span> <span class=\"mi\">9</span>\n",
       "\n",
       "    <span class=\"nv\">def</span> <span class=\"nv\">__init__</span><span class=\"ss\">(</span><span class=\"nv\">self</span>, <span class=\"nv\">num_classes</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"ss\">)</span>:\n",
       "        <span class=\"nv\">super</span><span class=\"ss\">(</span><span class=\"nv\">Detector</span>, <span class=\"nv\">self</span><span class=\"ss\">)</span>.<span class=\"nv\">__init__</span><span class=\"ss\">()</span>\n",
       "        <span class=\"nv\">self</span>.<span class=\"nv\">fpn</span> <span class=\"o\">=</span> <span class=\"nv\">FPN</span><span class=\"ss\">()</span>\n",
       "        <span class=\"nv\">self</span>.<span class=\"nv\">num_classes</span> <span class=\"o\">=</span> <span class=\"nv\">num_classes</span>\n",
       "        <span class=\"nv\">self</span>.<span class=\"nv\">loc_head</span> <span class=\"o\">=</span> <span class=\"nv\">self</span>.<span class=\"nv\">_make_head</span><span class=\"ss\">(</span><span class=\"nv\">self</span>.<span class=\"nv\">num_anchors</span> <span class=\"o\">*</span> <span class=\"mi\">4</span><span class=\"ss\">)</span>\n",
       "        <span class=\"nv\">self</span>.<span class=\"nv\">cls_head</span> <span class=\"o\">=</span> <span class=\"nv\">self</span>.<span class=\"nv\">_make_head</span><span class=\"ss\">(</span><span class=\"nv\">self</span>.<span class=\"nv\">num_anchors</span> <span class=\"o\">*</span> <span class=\"nv\">self</span>.<span class=\"nv\">num_classes</span><span class=\"ss\">)</span>\n",
       "\n",
       "    <span class=\"nv\">def</span> <span class=\"nv\">forward</span><span class=\"ss\">(</span><span class=\"nv\">self</span>, <span class=\"nv\">x</span><span class=\"ss\">)</span>:\n",
       "        <span class=\"nv\">fms</span> <span class=\"o\">=</span> <span class=\"nv\">self</span>.<span class=\"nv\">fpn</span><span class=\"ss\">(</span><span class=\"nv\">x</span><span class=\"ss\">)</span>\n",
       "        <span class=\"nv\">loc_preds</span> <span class=\"o\">=</span> []\n",
       "        <span class=\"nv\">cls_preds</span> <span class=\"o\">=</span> []\n",
       "        <span class=\"k\">for</span> <span class=\"nv\">feature_map</span> <span class=\"nv\">in</span> <span class=\"nv\">fms</span>:\n",
       "            <span class=\"nv\">loc_pred</span> <span class=\"o\">=</span> <span class=\"nv\">self</span>.<span class=\"nv\">loc_head</span><span class=\"ss\">(</span><span class=\"nv\">feature_map</span><span class=\"ss\">)</span>\n",
       "            <span class=\"nv\">cls_pred</span> <span class=\"o\">=</span> <span class=\"nv\">self</span>.<span class=\"nv\">cls_head</span><span class=\"ss\">(</span><span class=\"nv\">feature_map</span><span class=\"ss\">)</span>\n",
       "            <span class=\"nv\">loc_pred</span> <span class=\"o\">=</span> <span class=\"nv\">loc_pred</span>.<span class=\"nv\">permute</span><span class=\"ss\">(</span><span class=\"mi\">0</span>, <span class=\"mi\">2</span>, <span class=\"mi\">3</span>, <span class=\"mi\">1</span><span class=\"ss\">)</span>.<span class=\"nv\">contiguous</span><span class=\"ss\">()</span>.<span class=\"nv\">view</span><span class=\"ss\">(</span>\n",
       "                <span class=\"nv\">x</span>.<span class=\"nv\">size</span><span class=\"ss\">(</span><span class=\"mi\">0</span><span class=\"ss\">)</span>, <span class=\"o\">-</span><span class=\"mi\">1</span>, <span class=\"mi\">4</span>\n",
       "            <span class=\"ss\">)</span>  # [<span class=\"nv\">N</span>, <span class=\"mi\">9</span><span class=\"o\">*</span><span class=\"mi\">4</span>,<span class=\"nv\">H</span>,<span class=\"nv\">W</span>] <span class=\"o\">-&gt;</span> [<span class=\"nv\">N</span>,<span class=\"nv\">H</span>,<span class=\"nv\">W</span>, <span class=\"mi\">9</span><span class=\"o\">*</span><span class=\"mi\">4</span>] <span class=\"o\">-&gt;</span> [<span class=\"nv\">N</span>,<span class=\"nv\">H</span><span class=\"o\">*</span><span class=\"nv\">W</span><span class=\"o\">*</span><span class=\"mi\">9</span>, <span class=\"mi\">4</span>]\n",
       "            <span class=\"nv\">cls_pred</span> <span class=\"o\">=</span> <span class=\"nv\">cls_pred</span>.<span class=\"nv\">permute</span><span class=\"ss\">(</span><span class=\"mi\">0</span>, <span class=\"mi\">2</span>, <span class=\"mi\">3</span>, <span class=\"mi\">1</span><span class=\"ss\">)</span>.<span class=\"nv\">contiguous</span><span class=\"ss\">()</span>.<span class=\"nv\">view</span><span class=\"ss\">(</span>\n",
       "                <span class=\"nv\">x</span>.<span class=\"nv\">size</span><span class=\"ss\">(</span><span class=\"mi\">0</span><span class=\"ss\">)</span>, <span class=\"o\">-</span><span class=\"mi\">1</span>, <span class=\"nv\">self</span>.<span class=\"nv\">num_classes</span>\n",
       "            <span class=\"ss\">)</span>  # [<span class=\"nv\">N</span>,<span class=\"mi\">9</span><span class=\"o\">*</span><span class=\"mi\">20</span>,<span class=\"nv\">H</span>,<span class=\"nv\">W</span>] <span class=\"o\">-&gt;</span> [<span class=\"nv\">N</span>,<span class=\"nv\">H</span>,<span class=\"nv\">W</span>,<span class=\"mi\">9</span><span class=\"o\">*</span><span class=\"mi\">20</span>] <span class=\"o\">-&gt;</span> [<span class=\"nv\">N</span>,<span class=\"nv\">H</span><span class=\"o\">*</span><span class=\"nv\">W</span><span class=\"o\">*</span><span class=\"mi\">9</span>,<span class=\"mi\">20</span>]\n",
       "            <span class=\"nv\">loc_preds</span>.<span class=\"nv\">append</span><span class=\"ss\">(</span><span class=\"nv\">loc_pred</span><span class=\"ss\">)</span>\n",
       "            <span class=\"nv\">cls_preds</span>.<span class=\"nv\">append</span><span class=\"ss\">(</span><span class=\"nv\">cls_pred</span><span class=\"ss\">)</span>\n",
       "            \n",
       "        \n",
       "        <span class=\"k\">return</span> <span class=\"nv\">torch</span>.<span class=\"nv\">cat</span><span class=\"ss\">(</span><span class=\"nv\">loc_preds</span>, <span class=\"mi\">1</span><span class=\"ss\">)</span>, <span class=\"nv\">torch</span>.<span class=\"nv\">cat</span><span class=\"ss\">(</span><span class=\"nv\">cls_preds</span>, <span class=\"mi\">1</span><span class=\"ss\">)</span>\n",
       "\n",
       "    @<span class=\"nv\">staticmethod</span>\n",
       "    <span class=\"nv\">def</span> <span class=\"nv\">_make_head</span><span class=\"ss\">(</span><span class=\"nv\">out_planes</span><span class=\"ss\">)</span>:\n",
       "        <span class=\"nv\">layers</span> <span class=\"o\">=</span> []\n",
       "        <span class=\"k\">for</span> <span class=\"nv\">_</span> <span class=\"nv\">in</span> <span class=\"nv\">range</span><span class=\"ss\">(</span><span class=\"mi\">4</span><span class=\"ss\">)</span>:\n",
       "            <span class=\"nv\">layers</span>.<span class=\"nv\">append</span><span class=\"ss\">(</span><span class=\"nv\">nn</span>.<span class=\"nv\">Conv2d</span><span class=\"ss\">(</span><span class=\"mi\">64</span>, <span class=\"mi\">64</span>, <span class=\"nv\">kernel_size</span><span class=\"o\">=</span><span class=\"mi\">3</span>, <span class=\"nv\">stride</span><span class=\"o\">=</span><span class=\"mi\">1</span>, <span class=\"nv\">padding</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"ss\">))</span>\n",
       "            <span class=\"nv\">layers</span>.<span class=\"nv\">append</span><span class=\"ss\">(</span><span class=\"nv\">nn</span>.<span class=\"nv\">ReLU</span><span class=\"ss\">(</span><span class=\"nv\">True</span><span class=\"ss\">))</span>\n",
       "        <span class=\"nv\">layers</span>.<span class=\"nv\">append</span><span class=\"ss\">(</span><span class=\"nv\">nn</span>.<span class=\"nv\">Conv2d</span><span class=\"ss\">(</span><span class=\"mi\">64</span>, <span class=\"nv\">out_planes</span>, <span class=\"nv\">kernel_size</span><span class=\"o\">=</span><span class=\"mi\">3</span>, <span class=\"nv\">stride</span><span class=\"o\">=</span><span class=\"mi\">1</span>, <span class=\"nv\">padding</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"ss\">))</span>\n",
       "        <span class=\"k\">return</span> <span class=\"nv\">nn</span>.<span class=\"nv\">Sequential</span><span class=\"ss\">(</span><span class=\"o\">*</span><span class=\"nv\">layers</span><span class=\"ss\">)</span>\n",
       "</pre></div>\n"
      ],
      "text/latex": [
       "\\begin{Verbatim}[commandchars=\\\\\\{\\}]\n",
       "\\PY{n+nv}{class} \\PY{n+nv}{Detector}\\PY{l+s+ss}{(}\\PY{n+nv}{nn}.\\PY{n+nv}{Module}\\PY{l+s+ss}{)}:\n",
       "    \\PY{n+nv}{num\\PYZus{}anchors} \\PY{o}{=} \\PY{l+m+mi}{9}\n",
       "\n",
       "    \\PY{n+nv}{def} \\PY{n+nv}{\\PYZus{}\\PYZus{}init\\PYZus{}\\PYZus{}}\\PY{l+s+ss}{(}\\PY{n+nv}{self}, \\PY{n+nv}{num\\PYZus{}classes}\\PY{o}{=}\\PY{l+m+mi}{2}\\PY{l+s+ss}{)}:\n",
       "        \\PY{n+nv}{super}\\PY{l+s+ss}{(}\\PY{n+nv}{Detector}, \\PY{n+nv}{self}\\PY{l+s+ss}{)}.\\PY{n+nv}{\\PYZus{}\\PYZus{}init\\PYZus{}\\PYZus{}}\\PY{l+s+ss}{(}\\PY{l+s+ss}{)}\n",
       "        \\PY{n+nv}{self}.\\PY{n+nv}{fpn} \\PY{o}{=} \\PY{n+nv}{FPN}\\PY{l+s+ss}{(}\\PY{l+s+ss}{)}\n",
       "        \\PY{n+nv}{self}.\\PY{n+nv}{num\\PYZus{}classes} \\PY{o}{=} \\PY{n+nv}{num\\PYZus{}classes}\n",
       "        \\PY{n+nv}{self}.\\PY{n+nv}{loc\\PYZus{}head} \\PY{o}{=} \\PY{n+nv}{self}.\\PY{n+nv}{\\PYZus{}make\\PYZus{}head}\\PY{l+s+ss}{(}\\PY{n+nv}{self}.\\PY{n+nv}{num\\PYZus{}anchors} \\PY{o}{*} \\PY{l+m+mi}{4}\\PY{l+s+ss}{)}\n",
       "        \\PY{n+nv}{self}.\\PY{n+nv}{cls\\PYZus{}head} \\PY{o}{=} \\PY{n+nv}{self}.\\PY{n+nv}{\\PYZus{}make\\PYZus{}head}\\PY{l+s+ss}{(}\\PY{n+nv}{self}.\\PY{n+nv}{num\\PYZus{}anchors} \\PY{o}{*} \\PY{n+nv}{self}.\\PY{n+nv}{num\\PYZus{}classes}\\PY{l+s+ss}{)}\n",
       "\n",
       "    \\PY{n+nv}{def} \\PY{n+nv}{forward}\\PY{l+s+ss}{(}\\PY{n+nv}{self}, \\PY{n+nv}{x}\\PY{l+s+ss}{)}:\n",
       "        \\PY{n+nv}{fms} \\PY{o}{=} \\PY{n+nv}{self}.\\PY{n+nv}{fpn}\\PY{l+s+ss}{(}\\PY{n+nv}{x}\\PY{l+s+ss}{)}\n",
       "        \\PY{n+nv}{loc\\PYZus{}preds} \\PY{o}{=} []\n",
       "        \\PY{n+nv}{cls\\PYZus{}preds} \\PY{o}{=} []\n",
       "        \\PY{k}{for} \\PY{n+nv}{feature\\PYZus{}map} \\PY{n+nv}{in} \\PY{n+nv}{fms}:\n",
       "            \\PY{n+nv}{loc\\PYZus{}pred} \\PY{o}{=} \\PY{n+nv}{self}.\\PY{n+nv}{loc\\PYZus{}head}\\PY{l+s+ss}{(}\\PY{n+nv}{feature\\PYZus{}map}\\PY{l+s+ss}{)}\n",
       "            \\PY{n+nv}{cls\\PYZus{}pred} \\PY{o}{=} \\PY{n+nv}{self}.\\PY{n+nv}{cls\\PYZus{}head}\\PY{l+s+ss}{(}\\PY{n+nv}{feature\\PYZus{}map}\\PY{l+s+ss}{)}\n",
       "            \\PY{n+nv}{loc\\PYZus{}pred} \\PY{o}{=} \\PY{n+nv}{loc\\PYZus{}pred}.\\PY{n+nv}{permute}\\PY{l+s+ss}{(}\\PY{l+m+mi}{0}, \\PY{l+m+mi}{2}, \\PY{l+m+mi}{3}, \\PY{l+m+mi}{1}\\PY{l+s+ss}{)}.\\PY{n+nv}{contiguous}\\PY{l+s+ss}{(}\\PY{l+s+ss}{)}.\\PY{n+nv}{view}\\PY{l+s+ss}{(}\n",
       "                \\PY{n+nv}{x}.\\PY{n+nv}{size}\\PY{l+s+ss}{(}\\PY{l+m+mi}{0}\\PY{l+s+ss}{)}, \\PY{o}{\\PYZhy{}}\\PY{l+m+mi}{1}, \\PY{l+m+mi}{4}\n",
       "            \\PY{l+s+ss}{)}  \\PYZsh{} [\\PY{n+nv}{N}, \\PY{l+m+mi}{9}\\PY{o}{*}\\PY{l+m+mi}{4},\\PY{n+nv}{H},\\PY{n+nv}{W}] \\PY{o}{\\PYZhy{}\\PYZgt{}} [\\PY{n+nv}{N},\\PY{n+nv}{H},\\PY{n+nv}{W}, \\PY{l+m+mi}{9}\\PY{o}{*}\\PY{l+m+mi}{4}] \\PY{o}{\\PYZhy{}\\PYZgt{}} [\\PY{n+nv}{N},\\PY{n+nv}{H}\\PY{o}{*}\\PY{n+nv}{W}\\PY{o}{*}\\PY{l+m+mi}{9}, \\PY{l+m+mi}{4}]\n",
       "            \\PY{n+nv}{cls\\PYZus{}pred} \\PY{o}{=} \\PY{n+nv}{cls\\PYZus{}pred}.\\PY{n+nv}{permute}\\PY{l+s+ss}{(}\\PY{l+m+mi}{0}, \\PY{l+m+mi}{2}, \\PY{l+m+mi}{3}, \\PY{l+m+mi}{1}\\PY{l+s+ss}{)}.\\PY{n+nv}{contiguous}\\PY{l+s+ss}{(}\\PY{l+s+ss}{)}.\\PY{n+nv}{view}\\PY{l+s+ss}{(}\n",
       "                \\PY{n+nv}{x}.\\PY{n+nv}{size}\\PY{l+s+ss}{(}\\PY{l+m+mi}{0}\\PY{l+s+ss}{)}, \\PY{o}{\\PYZhy{}}\\PY{l+m+mi}{1}, \\PY{n+nv}{self}.\\PY{n+nv}{num\\PYZus{}classes}\n",
       "            \\PY{l+s+ss}{)}  \\PYZsh{} [\\PY{n+nv}{N},\\PY{l+m+mi}{9}\\PY{o}{*}\\PY{l+m+mi}{20},\\PY{n+nv}{H},\\PY{n+nv}{W}] \\PY{o}{\\PYZhy{}\\PYZgt{}} [\\PY{n+nv}{N},\\PY{n+nv}{H},\\PY{n+nv}{W},\\PY{l+m+mi}{9}\\PY{o}{*}\\PY{l+m+mi}{20}] \\PY{o}{\\PYZhy{}\\PYZgt{}} [\\PY{n+nv}{N},\\PY{n+nv}{H}\\PY{o}{*}\\PY{n+nv}{W}\\PY{o}{*}\\PY{l+m+mi}{9},\\PY{l+m+mi}{20}]\n",
       "            \\PY{n+nv}{loc\\PYZus{}preds}.\\PY{n+nv}{append}\\PY{l+s+ss}{(}\\PY{n+nv}{loc\\PYZus{}pred}\\PY{l+s+ss}{)}\n",
       "            \\PY{n+nv}{cls\\PYZus{}preds}.\\PY{n+nv}{append}\\PY{l+s+ss}{(}\\PY{n+nv}{cls\\PYZus{}pred}\\PY{l+s+ss}{)}\n",
       "            \n",
       "        \n",
       "        \\PY{k}{return} \\PY{n+nv}{torch}.\\PY{n+nv}{cat}\\PY{l+s+ss}{(}\\PY{n+nv}{loc\\PYZus{}preds}, \\PY{l+m+mi}{1}\\PY{l+s+ss}{)}, \\PY{n+nv}{torch}.\\PY{n+nv}{cat}\\PY{l+s+ss}{(}\\PY{n+nv}{cls\\PYZus{}preds}, \\PY{l+m+mi}{1}\\PY{l+s+ss}{)}\n",
       "\n",
       "    @\\PY{n+nv}{staticmethod}\n",
       "    \\PY{n+nv}{def} \\PY{n+nv}{\\PYZus{}make\\PYZus{}head}\\PY{l+s+ss}{(}\\PY{n+nv}{out\\PYZus{}planes}\\PY{l+s+ss}{)}:\n",
       "        \\PY{n+nv}{layers} \\PY{o}{=} []\n",
       "        \\PY{k}{for} \\PY{n+nv}{\\PYZus{}} \\PY{n+nv}{in} \\PY{n+nv}{range}\\PY{l+s+ss}{(}\\PY{l+m+mi}{4}\\PY{l+s+ss}{)}:\n",
       "            \\PY{n+nv}{layers}.\\PY{n+nv}{append}\\PY{l+s+ss}{(}\\PY{n+nv}{nn}.\\PY{n+nv}{Conv2d}\\PY{l+s+ss}{(}\\PY{l+m+mi}{64}, \\PY{l+m+mi}{64}, \\PY{n+nv}{kernel\\PYZus{}size}\\PY{o}{=}\\PY{l+m+mi}{3}, \\PY{n+nv}{stride}\\PY{o}{=}\\PY{l+m+mi}{1}, \\PY{n+nv}{padding}\\PY{o}{=}\\PY{l+m+mi}{1}\\PY{l+s+ss}{)}\\PY{l+s+ss}{)}\n",
       "            \\PY{n+nv}{layers}.\\PY{n+nv}{append}\\PY{l+s+ss}{(}\\PY{n+nv}{nn}.\\PY{n+nv}{ReLU}\\PY{l+s+ss}{(}\\PY{n+nv}{True}\\PY{l+s+ss}{)}\\PY{l+s+ss}{)}\n",
       "        \\PY{n+nv}{layers}.\\PY{n+nv}{append}\\PY{l+s+ss}{(}\\PY{n+nv}{nn}.\\PY{n+nv}{Conv2d}\\PY{l+s+ss}{(}\\PY{l+m+mi}{64}, \\PY{n+nv}{out\\PYZus{}planes}, \\PY{n+nv}{kernel\\PYZus{}size}\\PY{o}{=}\\PY{l+m+mi}{3}, \\PY{n+nv}{stride}\\PY{o}{=}\\PY{l+m+mi}{1}, \\PY{n+nv}{padding}\\PY{o}{=}\\PY{l+m+mi}{1}\\PY{l+s+ss}{)}\\PY{l+s+ss}{)}\n",
       "        \\PY{k}{return} \\PY{n+nv}{nn}.\\PY{n+nv}{Sequential}\\PY{l+s+ss}{(}\\PY{o}{*}\\PY{n+nv}{layers}\\PY{l+s+ss}{)}\n",
       "\\end{Verbatim}\n"
      ],
      "text/plain": [
       "class Detector(nn.Module):\n",
       "    num_anchors = 9\n",
       "\n",
       "    def __init__(self, num_classes=2):\n",
       "        super(Detector, self).__init__()\n",
       "        self.fpn = FPN()\n",
       "        self.num_classes = num_classes\n",
       "        self.loc_head = self._make_head(self.num_anchors * 4)\n",
       "        self.cls_head = self._make_head(self.num_anchors * self.num_classes)\n",
       "\n",
       "    def forward(self, x):\n",
       "        fms = self.fpn(x)\n",
       "        loc_preds = []\n",
       "        cls_preds = []\n",
       "        for feature_map in fms:\n",
       "            loc_pred = self.loc_head(feature_map)\n",
       "            cls_pred = self.cls_head(feature_map)\n",
       "            loc_pred = loc_pred.permute(0, 2, 3, 1).contiguous().view(\n",
       "                x.size(0), -1, 4\n",
       "            )  # [N, 9*4,H,W] -> [N,H,W, 9*4] -> [N,H*W*9, 4]\n",
       "            cls_pred = cls_pred.permute(0, 2, 3, 1).contiguous().view(\n",
       "                x.size(0), -1, self.num_classes\n",
       "            )  # [N,9*20,H,W] -> [N,H,W,9*20] -> [N,H*W*9,20]\n",
       "            loc_preds.append(loc_pred)\n",
       "            cls_preds.append(cls_pred)\n",
       "            \n",
       "        \n",
       "        return torch.cat(loc_preds, 1), torch.cat(cls_preds, 1)\n",
       "\n",
       "    @staticmethod\n",
       "    def _make_head(out_planes):\n",
       "        layers = []\n",
       "        for _ in range(4):\n",
       "            layers.append(nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1))\n",
       "            layers.append(nn.ReLU(True))\n",
       "        layers.append(nn.Conv2d(64, out_planes, kernel_size=3, stride=1, padding=1))\n",
       "        return nn.Sequential(*layers)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Code(data=inspect.getsource(Detector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the detector has two heads, one for class prediction and another for location prediction. Let's have look at its output size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location_pred size: torch.Size([2, 12276, 4])\n",
      "class_pred size: torch.Size([2, 12276, 2])\n"
     ]
    }
   ],
   "source": [
    "image_inputs = torch.rand((2, 3, 256, 256))\n",
    "\n",
    "detector = Detector()\n",
    "\n",
    "location_pred, class_pred = detector(image_inputs)\n",
    "\n",
    "print('location_pred size: {}'.format(location_pred.size()))\n",
    "\n",
    "print('class_pred size: {}'.format(class_pred.size()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Where does the number `12276` come from?**\n",
    "\n",
    "output of `FPN`:\n",
    "\n",
    "```\n",
    "torch.Size([2, 64, 32, 32])\n",
    "torch.Size([2, 64, 16, 16])\n",
    "torch.Size([2, 64, 8, 8])\n",
    "torch.Size([2, 64, 4, 4])\n",
    "torch.Size([2, 64, 2, 2])\n",
    "```\n",
    "\n",
    "Location predictor (`loc_pred`) in the detector using multiple convolutions to transform the output to the following: \n",
    "\n",
    "```\n",
    "torch.Size([2, 9*4, 32, 32])  # (batch_size, num_anchor*4 , H, W)\n",
    "torch.Size([2, 9*4, 16, 16])\n",
    "torch.Size([2, 9*4, 8, 8])\n",
    "torch.Size([2, 9*4, 4, 4])\n",
    "torch.Size([2, 9*4, 2, 2])\n",
    "```\n",
    "\n",
    "`(batch_size, number_of_anchor*4 , H, W)` re-arranged as follows:\n",
    "\n",
    "```\n",
    "(batch_size, num_anchor*4 , H, W)-->(batch_size, H, W, num_anchor*4)-->(batch_size, H*W*num_anchor, 4)\n",
    "```\n",
    "\n",
    "`num_anchor = 9`\n",
    "\n",
    "So, `32*32*9 + 16*16*9 + 8*8*9 + 4*4*9 + 2*2*9 = 12276`.\n",
    "\n",
    "From the above re-arrangement, it is clear that each feature map of `FPN` (starting from first feature of `(32, 32)` and end to last feature of `(2, 2)`) has `9x4` sized mapping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location_pred size: torch.Size([2, 17451, 4])\n",
      "class_pred size: torch.Size([2, 17451, 2])\n"
     ]
    }
   ],
   "source": [
    "image_inputs = torch.rand((2, 3, 300, 300))\n",
    "\n",
    "location_pred, class_pred = detector(image_inputs)\n",
    "\n",
    "print('location_pred size: {}'.format(location_pred.size()))\n",
    "\n",
    "print('class_pred size: {}'.format(class_pred.size()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are we interested in the output dimension of the detector network? Because to find loss (for training) we need similar dimensional targets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
