{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m7e0xDqd_Wk1"
   },
   "source": [
    "# <font style=\"color:blue\">Faster RCNN</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aJRzxGxSJw-Y"
   },
   "source": [
    "In the following notebook, we will go through how an image travels from the input to the Faster-RCNN and returns as the output.\n",
    "Consequently, we shall explore all the blocks involved in the architecture of Faster-RCNN.\n",
    "\n",
    "1. The Backbone network\n",
    "2. Region-Proposal network\n",
    "3. ROIPooling layer\n",
    "4. ROI Heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_V6xbKV0FI9f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb43d146d50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's import the necessary libs\n",
    "import torch\n",
    "import torchvision\n",
    "import collections\n",
    "from torchvision.models.detection import faster_rcnn\n",
    "import torchvision.models as models\n",
    "torch.manual_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7A8NqSwJFI9l"
   },
   "outputs": [],
   "source": [
    "# Let's keep the input size of the image as (800, 800, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zp4KveHmFI9p"
   },
   "source": [
    "## <font style=\"color:blue\">1.  The backbone</font>\n",
    "\n",
    "<img src=\"https://www.learnopencv.com/wp-content/uploads/2020/09/c3-w8-FasterRcnn_backbone.jpg\" width=700 heigth=700 >\n",
    "\n",
    "The backbone is simply a Convolutional Neural Network with only convolutional-layers (no fully-connected layers).\n",
    "\n",
    "Till now we have never seen any CNN with only convolutional layers, so how do we get one now?  \n",
    "Well, we have seen so many CNNs such as Resnet or VGG or Lenet. Take a minute to think what happens if we cut-down their fully connected layers?  \n",
    "We will get only the convolutional layers, right? \n",
    "Therefore, now we have a backbone.\n",
    "\n",
    "Let's see how this happens in code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "colab_type": "code",
    "id": "cRWkpxuLFI9p",
    "outputId": "477b0b07-ae5a-4815-fe6b-ae40acb186bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Let's use a simple model called the `AlexNet` instead of VGG or Resnet or FPN.\n",
    "# torchvision lib already has built this model, it even has the pretrained-weights too.\n",
    "\n",
    "alexnet = models.alexnet(pretrained=False)\n",
    "print(alexnet)\n",
    "\n",
    "# As we see below, the model is broken down into `features`, `avgpool` and `classifier`.\n",
    "# We shall notice that the `features` is full of Conv-layers and we are interested in only this module.\n",
    "# So let's go ahead and just keep this `features` module and store it inside `backbone`.\n",
    "\n",
    "backbone = alexnet.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bvoqQNfjFI9u",
    "outputId": "4a0be53d-2e56-49d1-e9c1-ceedf39b9a8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape from backbone is  torch.Size([1, 256, 24, 24])\n"
     ]
    }
   ],
   "source": [
    "# Now if we print the backbone, it will just consist of the `features`.\n",
    "\n",
    "# print(backbone) \n",
    "\n",
    "# We also need to note the output-channels that the `features` will return.  \n",
    "# We can get the final channels from the 10th line from the Alexnet features - \n",
    "# `(10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))` which is `256`.\n",
    "\n",
    "\n",
    "# Let's take a random input of size (800,800) and pass it through the `feature` module.\n",
    "image = torch.randn(1, 3, 800, 800)\n",
    "backbone_op = backbone(image)\n",
    "print(\"Output shape from backbone is \", backbone_op.shape)\n",
    "\n",
    "\n",
    "# We notice that the feature-map size has (24, 24) cells, which essentially means we can place \n",
    "# as many anchors with as many aspect-ratios as possible across each of these cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LW_pAnnYFI9x"
   },
   "source": [
    "#### <font style=\"color:green\">At this point, we are done with the backbone, and now we can move to the Region Proposal Network.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ADUz9PtrFI91"
   },
   "source": [
    "## <font style=\"color:blue\">2. Region Proposal Layer</font>\n",
    "\n",
    "\n",
    "<img src=\"https://www.learnopencv.com/wp-content/uploads/2020/09/c3-w8-FasterRcnn_rpn_layer.jpg\" width=700 height=700>\n",
    "\n",
    "\n",
    "Like the backbone, the RPN is simply a convolutional model.\n",
    "But it has some specialities.\n",
    "1. It takes an input, but spits out two outputs. One of the output tells us something about the objectness score and the other tells us about the box locations.\n",
    "2. The size of the outputs will be same as its input.\n",
    "\n",
    "\n",
    "A few things to keep in mind about the RPN Layer:\n",
    "\n",
    "1. RPN is a Neural network which tells whether an anchor consists of an object or not (a binary classifier). \n",
    "   Note that it does not tell which object it is. \n",
    "2. It also refines the anchor boxes associated with objects.\n",
    "3. Once we have positive anchors, we pass it to the ROIPooling layer which will be explained in the next block.\n",
    "\n",
    "Note, Faster RCNN has too many anchor-boxes, and it is a very hectic task to predict each and every anchor since most of them are background anchors. Hence we use the RPN module to remove these background anchors and just get the positive anchors.  \n",
    "This is what we refer to as the first stage in object detection.\n",
    "\n",
    "Let's see how we can create this RPN Layer via code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "JI1T4SE7FI95",
    "outputId": "8af937c5-23ad-4c06-8f18-f723b3d20ada",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RPN layer \n",
      "\n",
      " RPNHead(\n",
      "  (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (cls_logits): Conv2d(256, 9, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (bbox_pred): Conv2d(256, 36, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# The RPNHead is the convolutional model we talked about, in the previous block.\n",
    "# Again, torchvision already has built the `RPN` for us. You see, it is a very simple \n",
    "# convolutional block parameterized with `in_channels` and `num_anchors`.\n",
    "\n",
    "# Notice that `in_channels` must be the same as final channels from the `backbone` i.e 256.\n",
    "# What about the `num_anchors`?\n",
    "# Well, it depends on how many anchors we want to place over the feature map. \n",
    "# For example, if we want to place anchors of sizes-\n",
    "# [ (128, 64), (128, 128), (128, 256), \n",
    "#  (256, 128), (256, 256), (256,512), \n",
    "# (512, 256), (512, 512), (512, 1024)]\n",
    "# we will substitute 9 as the parameter for `num_anchors`.\n",
    "\n",
    "from torchvision.models.detection import rpn\n",
    "rpn_layer = rpn.RPNHead(in_channels = 256, num_anchors = 9)\n",
    "print(\"The RPN layer \\n\\n\",rpn_layer)\n",
    "\n",
    "\n",
    "# After we print the `RPNHead()` we notice a `cls_logits` which is a Conv-Layer with 9 output channels.\n",
    "# Well, each of these output channels correspond to each anchor box.\n",
    "# And what about the `36` channels in `bbox_pred`? \n",
    "# Well, we can split these 36 channels into 9 groups, such that first four channels \n",
    "# depict box locations of anchor (128, 64).\n",
    "# The next set of 4 channels correspond to box location for anchor with size-(128, 128) and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1-HPdv9iFI99",
    "outputId": "652d0556-f93f-4db6-ad12-77c405b99d4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape from RPN-layer is  torch.Size([1, 9, 24, 24]) torch.Size([1, 36, 24, 24])\n"
     ]
    }
   ],
   "source": [
    "# Let's forward the output of backbone to the `RPNHead()` and look at the output size.\n",
    "\n",
    "object_score, bbox_locs = rpn_layer(backbone_op.unsqueeze(0))\n",
    "print(\"Output shape from RPN-layer is \", torch.stack(object_score).squeeze(0).shape, \n",
    "                                          torch.stack(bbox_locs).squeeze(0).shape)\n",
    "\n",
    "# As we see below, we have a feature map with 9 channels but with the same size as the backbone output.\n",
    "# Similarly, we have another feature map with 36 channels( 4 channels each for 9 anchors) of same size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a2X4ViusCpcU"
   },
   "source": [
    "#### <font style=\"color:green\">At this point, we need to understand something called Receptive Field.</font>\n",
    "\n",
    "Simply put, a receptive field is the region in the input image as seen by a feature map.  \n",
    "\n",
    "For example, if our input is of size `(1, 3, 800,800)` and if we pass this image inside ResNet and grab the output \n",
    "feature maps at a few layers, say `(1, 64, 200, 200)`, `(1, 32, 80, 80)`, `(1, 128, 50, 50)`, then the receptive field\n",
    "for these feature maps will be `(4x4)`, `(10x10)` and `(16x16)` respectively.  \n",
    "\n",
    "Essentially, if we consider the fmap `(1, 128, 50, 50)`, then each cell among the `(50, 50)` will correspond to \n",
    "an area in the original image.  \n",
    "For example, the fmap cell at location `(49, 50)`, will correspond to the bottom right portion of the image.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EpOO9oUACxu8"
   },
   "source": [
    "#### <font style=\"color:green\">Now that you know what a receptive field is, lets jump back to previous explanation.</font>\n",
    "\n",
    "Previously, we saw that we have feature maps of size `(1, 9, 24, 24)` and `(1, 36, 24, 24)` from the RPN output. \n",
    "\n",
    "Can you guess what will be the receptive field of this feature map? It will be `(33.33 x 33.33)`.  \n",
    "\n",
    "It is as though each cell in the `(24, 24)`-cells is looking at a separate `(33.33 x 33.33)` portion of \n",
    "the image and hence will be associated with a separate anchor(box).  \n",
    "\n",
    "Since each cell(in the feature map) corresponds to a reference anchor box(in the image), \n",
    "the RPN needs to predict if that cell (or an anchor) consists of an object or not (a binary classifier).  \n",
    "Therefore, the RPN is really looking at `(9 * 24 * 24)` regions in the featuremap and trying to figure out\n",
    "which of these regions might contain an object.\n",
    "\n",
    "Well, the job of binary prediction is greatly acheived by a `Sigmoid()` or a `Softmax()` layer. \n",
    "Hence we use a Cross Entropy Loss.\n",
    "\n",
    "Nevertheless, 90% of the objects will not coincide with the actual reference anchors, right?  \n",
    "Hence, we need to predict the offsets to the anchors ie `(dx1, dy1, dx2, dy2)`.  \n",
    "This kind of prediction on offsets is a Regression problem, hence we use a L1 or L2-loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NtkZqeplFI-G"
   },
   "outputs": [],
   "source": [
    "# At this point, we have just created the RPN but we haven't set (or associated) the anchors with the channels yet.\n",
    "# So we need to create these anchors.\n",
    "\n",
    "# Fortunately, `torchvision` also has a function which generates the anchor boxes with respective aspect ratios.\n",
    "anchor_generator = rpn.AnchorGenerator(sizes=((128, 256, 512),), aspect_ratios=(0.5, 1, 2))\n",
    "\n",
    "# The above line generates the following anchors.\n",
    "# [ (128, 64), (128, 128), (128, 256), \n",
    "#  (256, 128), (256, 256), (256,512), \n",
    "# (512, 256), (512, 512), (512, 1024)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "RGrK1G0HFI-K",
    "outputId": "2cfcc1ef-1301-4643-de36-17cd462080d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Region-Proposal-Network is \n",
      "\n",
      "\n",
      " RegionProposalNetwork(\n",
      "  (anchor_generator): AnchorGenerator()\n",
      "  (head): RPNHead(\n",
      "    (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (cls_logits): Conv2d(256, 9, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (bbox_pred): Conv2d(256, 36, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Now that we have created the `rpn_layer` and the anchors i.e `anchor_generator`, \n",
    "# lets fuse these two into a single block.\n",
    "\n",
    "# Again, torchvision provides us a class `RegionProposalLayer()` whose inputs \n",
    "# will be the `rpn-layer` and the `anchors` and a few parameters.\n",
    "\n",
    "# There are a few technical details about the parameters such as the `nms_threshold`, `foreground_iou_threshold`, \n",
    "# `background_iou_threshold`, etc which will not be covered here.\n",
    "# These parameters' source can be found in the Faster-RCNN research paper itself.\n",
    "\n",
    "\n",
    "rpn_pre_nms = dict(training=2000, testing=1000)\n",
    "rpn_post_nms = dict(training=2000, testing=1000)\n",
    "region_proposer = rpn.RegionProposalNetwork(anchor_generator, rpn_layer, 0.5, 0.5, 512, 0.5, \n",
    "                                            rpn_pre_nms, rpn_post_nms, 0.7 )\n",
    "region_proposer.training = False\n",
    "\n",
    "# Let's print this `region_proposer`\n",
    "print(\"The Region-Proposal-Network is \\n\\n\\n\", region_proposer)\n",
    "\n",
    "# As you see, the whole `Region Proposal Network` consists of the `anchor_generator` i.e the anchors, and the \n",
    "# `RPNHead` which is the rpn-layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QcWrbc9j_Wlu"
   },
   "source": [
    "#### <font style=\"color:green\">Let's see how many objects (or locations) the RPN can detect out of the `(9*24*24) = 5184` possible locations.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Rao0o-lWFI-N",
    "outputId": "e8f04670-374a-40f4-c42b-3626734c327f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of boxes with objects detected by RPN  torch.Size([312, 4])\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models.detection import image_list\n",
    "\n",
    "inputs = image_list.ImageList(image,  [(800,800)])\n",
    "\n",
    "# The `region_proposer` takes the input as a list of images and the backbone output.\n",
    "rpn_bbox, _ = region_proposer(inputs, dict(feats = backbone_op))\n",
    "\n",
    "# Since the `inputs` is a list of images, the output i.e `bbox` will also be a list of boxes. \n",
    "# Hence we need to index the list to access the boxes.\n",
    "\n",
    "print(\"Number of boxes with objects detected by RPN \" , rpn_bbox[0].shape)\n",
    "\n",
    "num_boxes = rpn_bbox[0].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AmbNSSre_Wlz"
   },
   "source": [
    "#### <font style=\"color:green\">Now that we have identified the regions with potential objects using the RPN, can we go ahead and predict which object it was? </font>\n",
    "The answer is no.\n",
    "\n",
    "#### <font style=\"color:green\">At this point, one most important thing to notice is that the size of each `rpn_bbox` will be different, hence we cannot apply a Fully Connected Layer or a Conv-Layer or a Pooling layer to all of them directly.</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qv_ZROec_Wl0"
   },
   "source": [
    "These boxes (or regions of feature maps) need to be scaled-down or scaled-up to a particular size first, and only then we can further apply our Fully connected layers.  \n",
    "\n",
    "This process of scaling-down or scaling-up an arbitrary sized feature map to a fixed size is done by a special layer called `ROIPooling layer`.   \n",
    "In Pytorch, we call this layer the `AdaptiveMaxPooling2d()` layer.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RtOajgGQ_Wl1"
   },
   "source": [
    "##  <font style=\"color:blue\">3. ROI Pooling Layer</font>\n",
    "\n",
    "<img src=\"https://www.learnopencv.com/wp-content/uploads/2020/09/c3-w8-FasterRcnn_roi_pool.jpg\" width=700 height=700>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xVp-gOFRFI-d",
    "outputId": "2fc510f0-116d-4ef9-fe5b-fa007f3baa8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from ROI-Pooling with different-sized inputs  torch.Size([1, 64, 7, 10]) torch.Size([3, 7, 10]) torch.Size([1, 33, 7, 10])\n"
     ]
    }
   ],
   "source": [
    "# Let's try and use the `nn.AdaptiveMaxPool2d()` layer once to verify what it really does.\n",
    "\n",
    "# Its argument will be the final output size that we need. Let's use the final size of (7,10).\n",
    "fixed_size_pooler = torch.nn.AdaptiveMaxPool2d((7,10)) \n",
    "inp1 = torch.randn(1, 64, 77, 63)\n",
    "inp2 = torch.randn(3, 44, 67)\n",
    "inp3 = torch.randn(1, 33, 10, 10)\n",
    "out1 = fixed_size_pooler(inp1)\n",
    "out2 = fixed_size_pooler(inp2)\n",
    "out3 = fixed_size_pooler(inp3)\n",
    "\n",
    "print(\"Output from ROI-Pooling with different-sized inputs \", out1.shape, out2.shape, out3.shape )\n",
    "\n",
    "# Notice how the feature sizes are all (7, 10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ussvoUGsjple",
    "outputId": "5dca9e9d-e6ce-4d3a-a1e6-d542730c7d1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from ROI-Pooling layer  torch.Size([312, 256, 7, 10])\n"
     ]
    }
   ],
   "source": [
    "# Fortunately, again `torchvision` has a function which does the task of ROI Pooling for us.\n",
    "# It goes by the name `MultiScaleROIAlign()`.\n",
    "# Note that this function does the same operation as the `nn.AdaptiveMaxPooling()` but it has some extra features.\n",
    "# Hence we tend to use this class instead of the `nn.AdaptiveMaxPooling2d()`\n",
    "\n",
    "# It takes the argument `feature_map-names` as the input, the final `output_size` which we want to end up with\n",
    "# and a `sampling_ratio` which tells how to sample the points in a feature map.\n",
    "\n",
    "from torchvision.ops import MultiScaleRoIAlign\n",
    "roi_pooler = MultiScaleRoIAlign(['0'], output_size=(7, 10), sampling_ratio=2)\n",
    "\n",
    "\n",
    "# As we know, the ROIPooling pools the variable sized feature map regions into fixed sized regions, \n",
    "# we need the backbone features (got from the backbone) and the adjusted-boxes (got from the RPN-layer).\n",
    "# Also, it needs the original image size to calculate the scale.\n",
    "\n",
    "# We already have the output from backbone i.e the `backbone_op`.\n",
    "# We also have the RPN output i.e `rpn_bbox` but these outputs are completely random. \n",
    "# Hence we will create the rpn boxes again. \n",
    "\n",
    "# One quick question! Can you guess what will be the size of the output from `roi_pooler`?\n",
    "# Don't worry if you don't know the answer.\n",
    "\n",
    "\n",
    "fmaps = collections.OrderedDict()\n",
    "fmaps[\"0\"]=backbone_op\n",
    "\n",
    "# lets assume that we have the 312 boxes from the RPN \n",
    "# (the same number of boxes we got previously i.e `rpn_bbox[0].shape`)\n",
    "rpn_bbox_rand = torch.rand(num_boxes, 4) * 400; # lets create some random boxes as output from RPN. \n",
    "                                                # (top, left, width, height)\n",
    "rpn_bbox_rand[:, 2:] += rpn_bbox_rand[:, :2] # the format should be (top, left, bottom, right) \n",
    "                                             # hence we add the (top, left) to (width, height) coordinates\n",
    "\n",
    "roi_pooling_op = roi_pooler(fmaps, [rpn_bbox_rand], [(800, 800)])\n",
    "\n",
    "print(\"Output from ROI-Pooling layer \", roi_pooling_op.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T_PIr4Li_WmC"
   },
   "source": [
    "### <font style=\"color:green\">Story so far</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MNZEOcMsm4XZ"
   },
   "source": [
    "You see, we had the backbone feature map of size `(1, 256, 24, 24)` by passing the input to an `AlexNet`\n",
    "\n",
    "And using this backbone, the RPN predicted `312 variable-sized regions` in the backbone feature map (or 312 variable sized boxes) which potentially consisted of objects `(at object or no-object level only)`.  \n",
    "\n",
    "But in order to further process these variable sized feature maps, they need to be scaled to a fixed size, hence we used the ROI-Pooling to bring back all the `312 boxes` to a fixed size of `(7, 10)`.  \n",
    "\n",
    "Therefore we shall have 312 such feature maps each of size `(256, 7, 10)` resulting in `(312, 256, 7, 10)`. \n",
    "\n",
    "Since we now want to connect the output from ROI-Pooling with Fully Connected layers, so we should `flatten-out` the `roi-pooling-output` first and then feed this output to the `Linear-layers`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iMbqYiGl_WmF"
   },
   "source": [
    "#### <font style=\"color:green\">Now we can think of moving to predicting which object each of this 312 regions contain and also further refine their box locations with the help of Linear layers.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x-2o3x4X_WmG"
   },
   "source": [
    "## <font style=\"color:blue\">4. ROI Heads</font>\n",
    "<img src=\"https://www.learnopencv.com/wp-content/uploads/2020/09/c3-w8-FasterRcnn_roi_head.png\" width=700 height=700>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "vk-umMWBApAf",
    "outputId": "a387353f-8e1c-4a80-fb67-c8379d273c4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPHead is \n",
      " TwoMLPHead(\n",
      "  (fc6): Linear(in_features=17920, out_features=1024, bias=True)\n",
      "  (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "Output from the mlp-head is  torch.Size([312, 1024])\n"
     ]
    }
   ],
   "source": [
    "# 1. The MLP.\n",
    "\n",
    "# Again, `torchvision` provides us an in-built linear layer called `TwoMLPHead()`. It has two arguments -\n",
    "# the 'in_channels' which is the flattened version of `256*7*10` and\n",
    "#`representation_size` which is kept as 1024.\n",
    "# Let's go ahead and create it.\n",
    "\n",
    "from torchvision.models.detection import faster_rcnn\n",
    "mlp_head = faster_rcnn.TwoMLPHead(in_channels = 256*7*10, representation_size=1024)\n",
    "print(\"MLPHead is \\n\", mlp_head)\n",
    "\n",
    "# As you see below, it is a very simple network with just two linear layers.\n",
    "\n",
    "# Note that we did not flatten the roi_pooling_op. The reason is that the `TwoMLPHead` does the flattening for us.\n",
    "mlp_op = mlp_head(roi_pooling_op)\n",
    "\n",
    "print(\"Output from the mlp-head is \", mlp_op.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xog1WGJ__WmL"
   },
   "source": [
    "You must be wondering by now have we reached the output layer?  \n",
    "Well we are almost there! Just one more layer.  \n",
    "We call this last layer, the FastRCNNPredictor. Again, it is a simple network with one speciality.  \n",
    "It spits out two outputs, one for the classification and the other for the bounding-box regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "BDPoFkBjL13a",
    "outputId": "1e686b7e-b6fb-4042-baa0-ac42004349bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final layer of Faster-RCNN is  FastRCNNPredictor(\n",
      "  (cls_score): Linear(in_features=1024, out_features=17, bias=True)\n",
      "  (bbox_pred): Linear(in_features=1024, out_features=68, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 2. The FastRCNNPredictor.\n",
    "\n",
    "# Well, torchvision already has it for us, called `FastRCNNPredictor()`\n",
    "# It takes argument as in_channels which will be 1024 and num_classes which we shall keep `17`.\n",
    "\n",
    "final_layer = faster_rcnn.FastRCNNPredictor(in_channels=1024, num_classes=17)\n",
    "# Let's see how the final layer looks like.\n",
    "\n",
    "print(\"Final layer of Faster-RCNN is \", final_layer)\n",
    "\n",
    "# As you see, there is a `cls_score` whose output nodes are equal to num_classes\n",
    "# and `bbox_pred` whose output nodes are equal to 4 times the num_classes.\n",
    "# Basically, the `68` nodes are broken into 17 such groups such that each set contains 4 nodes\n",
    "# depicting the offset for x1, y1, x2, y2 location corresponding to that box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "DazxRAc0eGMI",
    "outputId": "4c22993b-398f-4864-d379-bae80aefd6cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from final-layer of Faster-RCNN is  torch.Size([312, 17]) torch.Size([312, 68])\n"
     ]
    }
   ],
   "source": [
    "# Now let's pass in the output of `mlp_head` to the `final_layer`.\n",
    "\n",
    "final_scores, final_bboxes = final_layer(mlp_op)\n",
    "print(\"Output from final-layer of Faster-RCNN is \", final_scores.shape, final_bboxes.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oPm1sxQmDTZG"
   },
   "source": [
    "We finally got the classes for each of the boxes the RPN detected.  \n",
    "However, most of these boxes actually overlay a lot.   \n",
    "Hence we need to use a technique called the `Non-Max-Supression` which removes off most of these boxes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P4V8lpNr_WmV"
   },
   "source": [
    "#####  <font style=\"color:green\">Can we combine all the topics covered and all the functions called in a very few lines of code? Yes!</font>\n",
    "\n",
    "##### <font style=\"color:green\">Let's see how can we do it.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83,
     "referenced_widgets": [
      "9ba5182287c34c3dbf0f0105190f1323",
      "83fdc63b3e864884a7a9642daa4d3cbc",
      "dfe5b9eca9334bf292de04ffa56865ac",
      "b523d042250f4c8dab9fbced71249c68",
      "3f6c449c716b4f87ab70e7ff2eaa0652",
      "f7e5168b92a34ef8be9d77257bf1b959",
      "2f39c3e9fae04f8bbbde7f9d19a0348c",
      "c5db2e5f75f94ca6ae5ad14b885a88de"
     ]
    },
    "colab_type": "code",
    "id": "ROpmBN_N_WmW",
    "outputId": "6b5f1712-4ced-474e-d968-c3d46638ebf0"
   },
   "outputs": [],
   "source": [
    "from torchvision.ops import MultiScaleRoIAlign\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "\n",
    "\n",
    "# Get the backbone of any pretrained network, we'll use AlexNet\n",
    "alexnet = models.alexnet(pretrained=True)\n",
    "new_backbone = alexnet.features\n",
    "new_backbone.out_channels = 256\n",
    "\n",
    "# Configure the anchors. We shall have 12 different anchors.\n",
    "new_anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256),), \n",
    "                                      aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "\n",
    "# Configure the output size of ROI-Pooling layer. \n",
    "# We shall end up with (num_boxes, num_features, 4, 4) after the ROIPooling layer\n",
    "new_roi_pooler = MultiScaleRoIAlign(featmap_names=['0'], output_size=4, sampling_ratio=1)\n",
    "\n",
    "\n",
    "# let's use dummy variables for mean, std, min_size and max_size\n",
    "min_size = 300\n",
    "max_size = 500\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Instantiate the Faster-rcnn model with the variables declared above.\n",
    "frcnn_model = FasterRCNN(backbone=new_backbone,\n",
    "                      num_classes=17, \n",
    "                      min_size=min_size, \n",
    "                      max_size=max_size, \n",
    "                      image_mean=mean, \n",
    "                      image_std=std, \n",
    "                      rpn_anchor_generator=new_anchor_generator, \n",
    "                      box_roi_pool=new_roi_pooler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "colab_type": "code",
    "id": "uzQP7qKq_Wma",
    "outputId": "1ababb83-ff5a-491e-f45e-1090d41c27b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FasterRCNN(\n",
      "  (transform): GeneralizedRCNNTransform(\n",
      "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "      Resize(min_size=(300,), max_size=500, mode='bilinear')\n",
      "  )\n",
      "  (backbone): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (rpn): RegionProposalNetwork(\n",
      "    (anchor_generator): AnchorGenerator()\n",
      "    (head): RPNHead(\n",
      "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (cls_logits): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bbox_pred): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): RoIHeads(\n",
      "    (box_roi_pool): MultiScaleRoIAlign()\n",
      "    (box_head): TwoMLPHead(\n",
      "      (fc6): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (box_predictor): FastRCNNPredictor(\n",
      "      (cls_score): Linear(in_features=1024, out_features=17, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=68, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# As you see below, the backbone, rpn and roi_heads are joined to form one big network.\n",
    "print(frcnn_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "id_y1rME_Wme"
   },
   "source": [
    "This notebook has mainly focused on the intricacies of how the image traverses through the network.  \n",
    "We have shown the different modules present in the Faster-RCNN model.  \n",
    "However, there are ideas such as how an anchor is treated as positive or negative, during the RPN training,\n",
    "how the NMS is performed, etc which are left untouched.  \n",
    "We request the students to visit the research paper for further details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font style=\"color:green\">References:-</font>\n",
    "\n",
    "1. The image used throughout this notebook was taken from [here]( https://i.insider.com/5d9cd5947fa74b01402f5082?width=1100&format=jpeg&auto=webp\n",
    ")\n",
    "2. https://github.com/pytorch/vision/tree/master/torchvision/models/detection"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "faster_rcnn_comprehension_final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
