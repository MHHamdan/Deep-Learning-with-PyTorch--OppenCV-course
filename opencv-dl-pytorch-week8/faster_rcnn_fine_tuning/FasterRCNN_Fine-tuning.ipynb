{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H44pKtxkE9xP"
   },
   "source": [
    "# <font style=\"color:blue\">Faster RCNN Fine-tuning</font>\n",
    "\n",
    "We have already seen the below Faster RCNN flow-diagram. We have also used the PyTorch pre-trained object detection model (`torchvision.models.detection.fasterrcnn_resnet50_fpn`) to infer on data samples.\n",
    "\n",
    "We know that this model is trained on the [coco dataset](https://tech.amikelive.com/node-718/what-object-categories-labels-are-in-coco-dataset/) that has `91` classes. What if our dataset doesn't have the class which we are interested in? Even though our interest class is available and the number of classes much lower than than the coco dataset, it is better to fine-tune the model for better mAP (mean average precession).\n",
    "\n",
    "\n",
    "If the model inference speed is slowfor our requirements, we might be interested in changing the backbone of the Faster RCNN model.\n",
    "\n",
    "\n",
    "We will see resnet-50_fpn implementation building blocks. We will use the understanding to fine-tune the model.\n",
    "\n",
    "\n",
    "#  <font style=\"color:blue\">1. Faster RCNN with Resnet-50 FPN Backbone</font>\n",
    "\n",
    "---\n",
    "\n",
    "![](https://www.researchgate.net/profile/Giang_Son_Tran/publication/324549019/figure/fig1/AS:649929152266241@1531966593689/Faster-R-CNN-Architecture-9.png)\n",
    "\n",
    "---\n",
    "\n",
    "**Let's start with mapping above building blocks with PyTorch `torchvision.models.detection.fasterrcnn_resnet50_fpn` implementation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eq9QpACQE9xR"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LF9B2WW_E9xW"
   },
   "source": [
    "**Find details of FasterRCNN with Resnet-50 FPN backbone [here](https://pytorch.org/docs/stable/torchvision/models.html#torchvision.models.detection.fasterrcnn_resnet50_fpn)**.\n",
    "\n",
    "Let's load pre-trained Faster RCNN with ResNet-50 FPN backbone detection model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XdbgjKbWH_M8"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\" \n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XhOaPzQdE9xX"
   },
   "outputs": [],
   "source": [
    "# load fasterrcnn_resnet50_fpn\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fi3HRlQoE9xb"
   },
   "source": [
    "## <font style=\"color:green\">1.1. Inputs Samples</font>\n",
    "\n",
    "**Let's load two images and their target.**\n",
    "\n",
    "**[Download image1](https://www.dropbox.com/s/jet087pwhln5b2j/FudanPed00066.png?dl=1)**\n",
    "\n",
    "**[Download image2](https://www.dropbox.com/s/uv8676diqwrstvl/PennPed00011.png?dl=1)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "TO0Q-EzFE9xc",
    "outputId": "75bbbcf1-a02c-4741-ca77-c3e65bd38fc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 1 size: torch.Size([3, 359, 360])\n",
      "Image 2 size: torch.Size([3, 376, 508])\n"
     ]
    }
   ],
   "source": [
    "image1 = T.ToTensor()(Image.open('FudanPed00066.png'))\n",
    "\n",
    "bboxes1 = torch.tensor([[248.0, 50.0, 329.0, 351.0]])\n",
    "labels1 = torch.tensor([1])\n",
    "\n",
    "image2 = T.ToTensor()(Image.open('PennPed00011.png'))\n",
    "\n",
    "bboxes2 = torch.tensor([[92.0, 62.0, 236.0, 344.0], [242.0, 52.0, 301.0, 355.0]])\n",
    "labels2 = torch.tensor([1, 1])\n",
    "\n",
    "print('Image 1 size: {}'.format(image1.size()))\n",
    "\n",
    "print('Image 2 size: {}'.format(image2.size()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Bs_MX6wE9xh"
   },
   "source": [
    "**We can see that both images (`image1` and `image2`) have different sizes.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7eynvrGdE9xi"
   },
   "source": [
    "## <font style=\"color:green\">1.2. Model Inference</font>\n",
    "\n",
    "- We need a list (not tensor) of images for the model inference.\n",
    "\n",
    "\n",
    "- Images size may be different. This means we need not resize to a constant size. Faster RCNN PyTorch Implementation has its own image pre-process block. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "colab_type": "code",
    "id": "au8SsW99E9xj",
    "outputId": "4339a029-c59b-4ac9-9177-e456d121db50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'boxes': tensor([[243.1263,  47.7870, 327.8619, 349.8769]], device='cuda:0',\n",
      "       grad_fn=<StackBackward>), 'labels': tensor([1], device='cuda:0'), 'scores': tensor([0.9997], device='cuda:0', grad_fn=<IndexBackward>)}, {'boxes': tensor([[ 89.9230,  59.4910, 225.3071, 342.8298],\n",
      "        [244.2283,  49.8334, 304.4795, 362.8902],\n",
      "        [245.9230, 127.6201, 276.5670, 197.8546],\n",
      "        [252.0381,  15.8489, 369.9043, 367.3777],\n",
      "        [245.7938,  99.7875, 294.3491, 198.7888],\n",
      "        [243.9077, 121.8000, 276.3352, 198.4012],\n",
      "        [247.6824,  51.5020, 301.1053, 203.1744],\n",
      "        [245.5552,  95.3306, 295.3181, 199.0098],\n",
      "        [274.8139,  96.3892, 301.2039, 188.7242],\n",
      "        [123.6462,  56.9277, 191.9256, 338.0054],\n",
      "        [240.7440,  44.0858, 333.5300, 235.6630],\n",
      "        [267.6390, 100.0401, 299.7915, 187.5079]], device='cuda:0',\n",
      "       grad_fn=<StackBackward>), 'labels': tensor([ 1,  1, 27,  1, 27, 31,  1, 31, 27,  1,  1, 31], device='cuda:0'), 'scores': tensor([0.9996, 0.9931, 0.7120, 0.6038, 0.3407, 0.3300, 0.3079, 0.2703, 0.2572,\n",
      "        0.0977, 0.0517, 0.0514], device='cuda:0', grad_fn=<IndexBackward>)}]\n"
     ]
    }
   ],
   "source": [
    "input_image1 = image1.clone()\n",
    "\n",
    "input_image2 = image2.clone()\n",
    "\n",
    "# input its image list\n",
    "inputs = [input_image1.to(device), input_image2.to(device)]\n",
    "\n",
    "model.eval()\n",
    "output = model(inputs)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yJEE4S-LE9xn"
   },
   "source": [
    "## <font style=\"color:green\">1.3. Model Training</font>\n",
    "\n",
    "- In training mode, `targets` are mandatory. Targets are required because they calculate loss. This loss can be used to find gradients by using `backward()`.\n",
    "\n",
    "\n",
    "- Targets should be a list, and each target should have the following format:\n",
    "\n",
    "```\n",
    "    {\n",
    "        'boxes': bounding boxes tensor,\n",
    "        'labels': label tensor\n",
    "    \n",
    "    } \n",
    "```\n",
    "\n",
    "\n",
    "- Object detection in Faster RCNN is done in two stages. \n",
    "\n",
    "\n",
    "- First, it classifies all regions of the image in just two classes- background or object. \n",
    "\n",
    "\n",
    "- In the second stage, it predicts classes of the object and improves its bounding box predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "3XuPMuK6E9xo",
    "outputId": "46a7f12d-88c9-4855-eec0-3f7d63c75eca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss_box_reg': tensor(0.0074, device='cuda:0', grad_fn=<DivBackward0>),\n",
       " 'loss_classifier': tensor(0.0219, device='cuda:0', grad_fn=<NllLossBackward>),\n",
       " 'loss_objectness': tensor(0.0010, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>),\n",
       " 'loss_rpn_box_reg': tensor(0.0106, device='cuda:0', grad_fn=<DivBackward0>)}"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_image1 = image1.clone()\n",
    "\n",
    "target1 = {\n",
    "    'boxes': bboxes1.clone().to(device),\n",
    "    'labels' : labels1.clone().to(device)\n",
    "    \n",
    "} \n",
    "\n",
    "input_image2 = image2.clone()\n",
    "\n",
    "\n",
    "target2 = {\n",
    "    'boxes': bboxes2.clone().to(device),\n",
    "    'labels' : labels2.clone().to(device)\n",
    "    \n",
    "} \n",
    "\n",
    "inputs = [input_image1.to(device), input_image2.to(device)]\n",
    "targets = [target1, target2]\n",
    "\n",
    "# change to train mode\n",
    "model.train()\n",
    "model(inputs, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HbTnro3DE9xs"
   },
   "source": [
    "- `loss_objectness` and `loss_rpn_box_reg` are losses of the first stage.\n",
    "\n",
    "\n",
    "- `loss_classifier` and `loss_box_reg` are losses of the second stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P6gwBq-XE9xt"
   },
   "source": [
    "## <font style=\"color:green\">1.4. Model Building Blocks</font>\n",
    "\n",
    "**We will see building blocks of the model and modifying the cloned model for fine-tuning.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "M4u27nkYE9xu",
    "outputId": "ca4fc747-a91e-4a60-fa36-f7983f298b9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FasterRCNN(\n",
      "  (transform): GeneralizedRCNNTransform()\n",
      "  (backbone): BackboneWithFPN(\n",
      "    (body): IntermediateLayerGetter(\n",
      "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (bn1): FrozenBatchNorm2d()\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "      (layer1): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d()\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d()\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d()\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): FrozenBatchNorm2d()\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d()\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d()\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d()\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d()\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d()\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d()\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer2): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d()\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d()\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d()\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): FrozenBatchNorm2d()\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d()\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d()\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d()\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d()\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d()\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d()\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d()\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d()\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d()\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer3): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d()\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d()\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d()\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): FrozenBatchNorm2d()\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d()\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d()\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d()\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d()\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d()\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d()\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d()\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d()\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d()\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (4): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d()\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d()\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d()\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (5): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d()\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d()\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d()\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer4): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d()\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d()\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d()\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): FrozenBatchNorm2d()\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d()\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d()\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d()\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d()\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d()\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d()\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fpn): FeaturePyramidNetwork(\n",
      "      (inner_blocks): ModuleList(\n",
      "        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (layer_blocks): ModuleList(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (extra_blocks): LastLevelMaxPool()\n",
      "    )\n",
      "  )\n",
      "  (rpn): RegionProposalNetwork(\n",
      "    (anchor_generator): AnchorGenerator()\n",
      "    (head): RPNHead(\n",
      "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): RoIHeads(\n",
      "    (box_roi_pool): MultiScaleRoIAlign()\n",
      "    (box_head): TwoMLPHead(\n",
      "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (box_predictor): FastRCNNPredictor(\n",
      "      (cls_score): Linear(in_features=1024, out_features=91, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GNjceklXE9xz"
   },
   "source": [
    "**We can see the model has the following building blocks of `FasterRCNN`:**\n",
    "\n",
    "- **`transform`:** This block pre-processes the input image.\n",
    "\n",
    "\n",
    "- **`backbone`:** This is equivalent to **conv layers** in the above image.\n",
    "\n",
    "\n",
    "- **`rpn`:** This is equivalent to **Region Proposal Network** in the above image.\n",
    "\n",
    "\n",
    "- **`roi_heads`:** This is equivalent to **RoI Pooling**.\n",
    "\n",
    "\n",
    "- **`box_predictor`:** This is equivalent to **classifier** in the above image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P547n_I8E9x0"
   },
   "source": [
    "### <font style=\"color:green\">transform</font>\n",
    "\n",
    "- This block pre-processes the input like normalizing, resizing, etc.\n",
    "\n",
    "Let's have a look at the pre-processed tensor size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qXnN2R1nE9x1",
    "outputId": "a13271a2-79ef-4fde-f2b0-3f95c3e1a6cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor size: torch.Size([2, 3, 800, 1088])\n"
     ]
    }
   ],
   "source": [
    "input_image1 = image1.clone()\n",
    "\n",
    "input_image2 = image2.clone()\n",
    "\n",
    "inputs = [input_image1.to(device), input_image2.to(device)]\n",
    "\n",
    "trans_image_list, trans_target_list = model.transform(inputs)\n",
    "\n",
    "print('Tensor size: {}'.format(trans_image_list.tensors.size()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oLsuRYeNE9x5"
   },
   "source": [
    "Let's have a look at transforms parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "dBxYqu0lE9x6",
    "outputId": "f4864e51-4c69-4530-8fc0-0c5b729d2b2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transform ( GeneralizedRCNNTransform) parameters:\n",
      "min_size: (800,)\n",
      "max_size: 1333\n",
      "image_mean: [0.485, 0.456, 0.406]\n",
      "image_std: [0.229, 0.224, 0.225]\n"
     ]
    }
   ],
   "source": [
    "print('transform ( GeneralizedRCNNTransform) parameters:')\n",
    "print('min_size: {}'.format(model.transform.min_size))\n",
    "print('max_size: {}'.format(model.transform.max_size))\n",
    "print('image_mean: {}'.format(model.transform.image_mean))\n",
    "print('image_std: {}'.format(model.transform.image_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yxnUOYMHE9x9"
   },
   "source": [
    "### <font style=\"color:magenta\">Transform params in Faster RCNN Fine-tune model</font>\n",
    "\n",
    "If we have smaller images for training, then we might be interested in changing the transform parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6S5eL7SFE9x-"
   },
   "outputs": [],
   "source": [
    "ft_min_size = 300\n",
    "ft_max_size = 500\n",
    "\n",
    "ft_mean = [0.485, 0.456, 0.406]\n",
    "ft_std = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WN7YRkZ-E9yB"
   },
   "source": [
    "### <font style=\"color:green\">backbone (conv layers)</font>\n",
    "\n",
    "- It has used `resnet50` (trained on image net dataset) with `FPN` as the backbone for feature extraction. Don't worry if you don't know what is `FPN`. In short, it extracts features from different layers of `resnet50`. In object detection, features from different layers perform better than the immediate last layer. More details of the FPN find [here](https://arxiv.org/pdf/1612.03144.pdf).\n",
    "\n",
    "\n",
    "- Generally, we use the pre-trained model (trained on extensive data set e.g., image-net) as a backbone in the object detection network.\n",
    "\n",
    "\n",
    "\n",
    "- We can change the backbone with another backbone (`resnet-18`, `vgg-16` etc. ) for fine-tuning.\n",
    "\n",
    "**Let's see the number of output channels of the backbone.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3xpjfOZiE9yC"
   },
   "outputs": [],
   "source": [
    "backbone_out = model.backbone(trans_image_list.tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "EX1KGr72E9yF",
    "outputId": "1a7a5b3e-72af-4bfc-b1ba-b6c438c97dab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: torch.Size([2, 256, 200, 272])\n",
      "1: torch.Size([2, 256, 100, 136])\n",
      "2: torch.Size([2, 256, 50, 68])\n",
      "3: torch.Size([2, 256, 25, 34])\n",
      "pool: torch.Size([2, 256, 13, 17])\n"
     ]
    }
   ],
   "source": [
    "for key, value in backbone_out.items():\n",
    "    print('{}: {}'.format(key, value.size()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NEx-ANrAE9yJ"
   },
   "source": [
    "**The output of the backbone is `OrderedDict[Tensor]` of five tuples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PT9AWXGfE9yK",
    "outputId": "47ec38d9-9684-4f42-c18c-cccb28262281"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of output channel of the backbone: 256\n"
     ]
    }
   ],
   "source": [
    "print('Number of output channel of the backbone: {}'.format(model.backbone.out_channels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PjY7uvClE9yN"
   },
   "source": [
    "### <font style=\"color:magenta\">Backbone of Faster RCNN Fine-tune model</font>\n",
    "\n",
    "Let's choose pre-trained AlexNet fr0m torchvision models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542,
     "referenced_widgets": [
      "f392d0f2a23348e982f730ba12da4cfc",
      "f1883bac6a3f4680b0c1e3c62d3587e4",
      "1bf9068908194a49a8afdd334216c316",
      "3673b6da00f8477f8090e6f1f3c901e3",
      "55d6172edaec44b2ac84a4f8ab1f8feb",
      "5905b69a9b234ef4973c06bf12f4775b",
      "19b9c4d703a0452e80cb78c04a161225",
      "d1a673f3678a4baba36765c3350babb0"
     ]
    },
    "colab_type": "code",
    "id": "NGdxm2asE9yO",
    "outputId": "33ec8585-016a-4ecc-e80a-a698ee15d184"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth\" to /root/.cache/torch/checkpoints/alexnet-owt-4df8aa71.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f392d0f2a23348e982f730ba12da4cfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=244418560), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "alexnet = models.alexnet(pretrained=True)\n",
    "print(alexnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dJf2Xx4mE9yR"
   },
   "source": [
    "- For the Faster RCNN backbone, we are just interested in convolution features. \n",
    "\n",
    "\n",
    "- Faster RCNN also needs the number of out-channel of the backbone. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2lizdYvQE9yS"
   },
   "outputs": [],
   "source": [
    "ft_backbone = alexnet.features\n",
    "\n",
    "# number of out-channel in alexnet features is 256\n",
    "ft_backbone.out_channels = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6QpDz8QwE9yV"
   },
   "source": [
    "### <font style=\"color:green\">rpn (Region Proposal Network)</font>\n",
    "\n",
    "- It takes features from the backbone and predicts the objectness (the region and whether it is object or background) and coordinates of the region. \n",
    "\n",
    "\n",
    "**What is the meaning of the region here?**\n",
    "\n",
    "Generally, in object detection, we use anchor (a rectangular block) to denote the region. \n",
    "\n",
    "---\n",
    "\n",
    "<img src='https://www.learnopencv.com/wp-content/uploads/2020/03/c3-w8-anchors.png' align='middle'>\n",
    "\n",
    "---\n",
    "\n",
    "- In the above image, it has two feature maps `b` (`8 x 8 feature map (grid)`) and `c` (`4 x 4 feature map (grid)`). One element of the feature map represents segments of pixels in the original image `a`.\n",
    "\n",
    "\n",
    "\n",
    "- Each feature map has a set of anchors.\n",
    "\n",
    "\n",
    "- We can change the number of anchors for each feature map as fine-tuning process. \n",
    "\n",
    "\n",
    "Let's first see the `rpn` (Region Proposal Network) in `resnet-50-fpn` model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "tQDHRHIOE9yV",
    "outputId": "bdb11a77-5580-45a9-9247-de239b65cae5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RegionProposalNetwork(\n",
       "  (anchor_generator): AnchorGenerator()\n",
       "  (head): RPNHead(\n",
       "    (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.rpn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yw2HDi6ME9yZ"
   },
   "source": [
    "We can see that it has two parts- (1) `anchor_generator` and (2) `head`.\n",
    "\n",
    "**`anchor_generator`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "9hU3OESxE9ya",
    "outputId": "005b0288-8837-4080-b29f-19ed2ee89318"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor sizes: ((32,), (64,), (128,), (256,), (512,))\n",
      "Aspect ratios: ((0.5, 1.0, 2.0), (0.5, 1.0, 2.0), (0.5, 1.0, 2.0), (0.5, 1.0, 2.0), (0.5, 1.0, 2.0))\n"
     ]
    }
   ],
   "source": [
    "print('Anchor sizes: {}'.format(model.rpn.anchor_generator.sizes))\n",
    "print('Aspect ratios: {}'.format(model.rpn.anchor_generator.aspect_ratios))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OwWKBDe_E9yd"
   },
   "source": [
    "- Sizes (32, 64, ..) corresponds to numbers of pixels in original images.\n",
    "\n",
    "\n",
    "- We can see `sizes` is a tuple of five tuples.\n",
    "\n",
    "\n",
    "- Each tuple corresponds to a single label CNN features of `resnet50-fpn` (note that the output of the backbone is `OrderedDict[Tensor]` of five tuples) backbone.\n",
    "\n",
    "\n",
    "- We also see that `aspect ratio` is also a tuple of `five` tuples. The first tuple corresponds to the first anchor tuple, second to second, and so on. \n",
    "\n",
    "\n",
    "- As each label has one `anchor size` and each anchor has three `aspect ratios`, the number of anchors per feature map will be `three` (`1*3`). \n",
    "\n",
    "\n",
    "- Backbone has five label output, and each label is associated with a different `anchor size`, so the total number of anchors will be `fifteen` (`5*3`).\n",
    "\n",
    "\n",
    "- Changing the anchor size and ratios may be important for fine-tuning. For example, let's assume we have to detect just pedestrians. Having aspect ratios as (`(0.5, 1.0, 2.0)`) may not be very much useful as compared to (`(2.0, 2.5, 3.0)`).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZdlHzHHdE9ye"
   },
   "source": [
    "### <font style=\"color:magenta\">Anchor of Faster RCNN Fine-tune model</font>\n",
    "\n",
    "- Since the AlexNet has a single label output; the anchor size should be a single tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "035W_T52E9yf"
   },
   "outputs": [],
   "source": [
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "ft_anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256),), \n",
    "                                      aspect_ratios=((0.5, 1.0, 2.0),))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IAoPBgDZE9yi"
   },
   "source": [
    "**`head`**\n",
    "\n",
    "**`cls_logits`:** It is just classifying whether the corresponding feature map is an object or a background. It uses logistics regression (means if value > 0.5 then object else background). That is why the number of output channels is `3` (one channel for one aspect ratio).\n",
    "\n",
    "\n",
    "**`bbox_pred`:** To represent a bounding box, we need four numbers. So output channels are 12, four for each aspect ratio.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "niK-UhEtE9yj"
   },
   "source": [
    "### <font style=\"color:green\">roi_heads (RoI Pooling)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "S4TV5rMWE9yj",
    "outputId": "da44a7ad-7af8-4f63-da64-e4e3cacc3329"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RoIHeads(\n",
       "  (box_roi_pool): MultiScaleRoIAlign()\n",
       "  (box_head): TwoMLPHead(\n",
       "    (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "    (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  )\n",
       "  (box_predictor): FastRCNNPredictor(\n",
       "    (cls_score): Linear(in_features=1024, out_features=91, bias=True)\n",
       "    (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.roi_heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uf_beTFCE9yo"
   },
   "source": [
    "**`box_roi_pool`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zhjXeTZFE9yp"
   },
   "source": [
    "- Take bounding boxes predicted by `RegionProposalNetwork head` and `convolution features` from the `backbone`. \n",
    "\n",
    "\n",
    "- For bounding boxes for which `objectness score` is greater than the `threshold`, it crop features from convolution layers and resized (e.g. `14 x 14`), then sub-sample feature from resized bounding box (e.g. if `sampling_ratio` is `2` then `14 x 14` will resize to `7 x 7`).\n",
    "\n",
    "\n",
    "- These sub-sampled features converted to `1-d` vector and are stacked like a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "bwnQk-vmE9yp",
    "outputId": "5f35a62e-d84c-4a88-d50a-9f02bbe62276"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box RoI Pool Parameters:\n",
      "featmap_names: ['0', '1', '2', '3']\n",
      "output_size: (7, 7)\n",
      "sampling_ratio: 2\n"
     ]
    }
   ],
   "source": [
    "print('Box RoI Pool Parameters:')\n",
    "print('featmap_names: {}'.format(model.roi_heads.box_roi_pool.featmap_names))\n",
    "print('output_size: {}'.format(model.roi_heads.box_roi_pool.output_size))\n",
    "print('sampling_ratio: {}'.format(model.roi_heads.box_roi_pool.sampling_ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Og3wNagDE9ys"
   },
   "source": [
    "### <font style=\"color:magenta\">RoI Pooler of Faster RCNN Fine-tune model</font>\n",
    "\n",
    "Recall backbone output was five tuples ordered dictionary. Lets print it below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eSRj95amE9yt",
    "outputId": "3f619979-caf7-452e-c588-332521c17ffd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['0', '1', '2', '3', 'pool'])"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# backbone output keys \n",
    "backbone_out.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XAIjM9V9E9yw"
   },
   "source": [
    "`featmap_names = ['0', '1', '2', '3']`, it means for region of interest pooling current implementation have not used `'pool'` layer output.\n",
    "\n",
    "Do we have ordered dictionary output of the ft_backbone (alexnet.features)?\n",
    "\n",
    "Let's check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "r0beRfnvE9yx",
    "outputId": "b8f5181d-43c2-4910-f3ce-aa8b1ce7affe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ft_backbone(torch.rand((2, 3, 300, 300))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xi0_rJG5E9y1"
   },
   "source": [
    "Oh! It is just a **tensor**. If it is just a tensor, then we can use `featmap_names=['0']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iJezZr_PE9y1"
   },
   "outputs": [],
   "source": [
    "from torchvision.ops import MultiScaleRoIAlign\n",
    "\n",
    "ft_roi_pooler = MultiScaleRoIAlign(featmap_names=['0'], output_size=4, sampling_ratio=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rnlnc0KyE9y4"
   },
   "source": [
    "**`box_head`**\n",
    "\n",
    "- It has two fully connected layers, which takes input from the output of `box_roi_pool`.\n",
    "\n",
    "\n",
    "**`box_predictor`**\n",
    "\n",
    "- `box_head` output goes to two fully connected layers- one for class prediction and other for bounding prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DpYUhX1QE9y5"
   },
   "source": [
    "#  <font style=\"color:blue\">2. Faster RCNN with AlexNet Backbone</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SMlHAZ2yE9y6"
   },
   "outputs": [],
   "source": [
    "from torchvision.models.detection import FasterRCNN\n",
    "\n",
    "# let number of classes 4 (including background)\n",
    "\n",
    "ft_model = FasterRCNN(backbone=ft_backbone,\n",
    "                      num_classes=2, \n",
    "                      min_size=ft_min_size, \n",
    "                      max_size=ft_max_size, \n",
    "                      image_mean=ft_mean, \n",
    "                      image_std=ft_std, \n",
    "                      rpn_anchor_generator=ft_anchor_generator, \n",
    "                      box_roi_pool=ft_roi_pooler)\n",
    "ft_model = ft_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JZuh9ZD0E9y-"
   },
   "source": [
    "## <font style=\"color:green\">2.1. Model Inference</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "w5kv0V2xE9y-",
    "outputId": "7625dc83-3869-4808-850a-ab441653609b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'boxes': tensor([[1.7834e+02, 3.2221e+02, 2.3245e+02, 3.5068e+02],\n",
      "        [1.9000e+02, 2.1708e+02, 3.0626e+02, 3.5900e+02],\n",
      "        [1.9436e+02, 2.5965e+02, 2.1921e+02, 3.0624e+02],\n",
      "        [1.8164e+02, 2.8102e+02, 2.3684e+02, 3.0569e+02],\n",
      "        [2.5561e+02, 3.2282e+02, 3.1316e+02, 3.5218e+02],\n",
      "        [2.1547e+02, 2.7961e+02, 2.7209e+02, 3.0538e+02],\n",
      "        [1.4949e+02, 3.0448e+02, 2.5016e+02, 3.5792e+02],\n",
      "        [1.7703e+02, 2.1721e+02, 2.3312e+02, 3.1368e+02],\n",
      "        [1.4695e+02, 2.3953e+02, 2.5904e+02, 3.5900e+02],\n",
      "        [2.1273e+02, 2.1134e+02, 2.7442e+02, 3.1199e+02],\n",
      "        [2.3421e+02, 2.5772e+02, 2.5892e+02, 3.1400e+02],\n",
      "        [2.1721e+02, 2.6717e+02, 2.9212e+02, 3.4661e+02],\n",
      "        [1.9375e+02, 3.0927e+02, 2.1732e+02, 3.5862e+02],\n",
      "        [1.4121e+02, 2.8185e+02, 1.9467e+02, 3.0654e+02],\n",
      "        [1.0478e+02, 1.6494e+02, 2.1878e+02, 3.5863e+02],\n",
      "        [1.4568e+02, 1.1086e+02, 2.7043e+02, 3.5575e+02],\n",
      "        [2.7622e+01, 2.3406e+02, 2.9426e+02, 3.4048e+02],\n",
      "        [3.2299e+01, 1.1822e+02, 3.6000e+02, 3.5900e+02],\n",
      "        [2.3273e+02, 3.0692e+02, 2.5632e+02, 3.5900e+02],\n",
      "        [2.5784e+02, 3.0394e+02, 3.1853e+02, 3.5900e+02],\n",
      "        [1.8056e+02, 1.6932e+02, 2.5319e+02, 2.4852e+02],\n",
      "        [2.5359e+02, 2.7957e+02, 3.1084e+02, 3.0787e+02],\n",
      "        [2.1341e+02, 2.3645e+02, 2.7688e+02, 2.6139e+02],\n",
      "        [1.7933e+02, 1.1768e+02, 2.5780e+02, 1.9864e+02],\n",
      "        [2.1885e+02, 3.2295e+02, 2.7302e+02, 3.5149e+02],\n",
      "        [1.5341e+02, 2.6354e+02, 1.7719e+02, 3.1050e+02],\n",
      "        [2.2081e+02, 1.4442e+02, 2.7149e+02, 1.8624e+02],\n",
      "        [1.1102e+02, 2.6470e+02, 2.2537e+02, 3.1978e+02],\n",
      "        [2.2447e+02, 2.3320e+02, 2.7138e+02, 2.7444e+02],\n",
      "        [1.3221e+02, 2.9947e+02, 2.0762e+02, 3.5896e+02],\n",
      "        [1.4512e+02, 2.5072e+02, 1.9792e+02, 3.5254e+02],\n",
      "        [0.0000e+00, 2.0452e+01, 5.4200e+01, 2.3930e+02],\n",
      "        [2.1364e+02, 2.9667e+02, 2.8418e+02, 3.5900e+02],\n",
      "        [1.9205e+02, 1.8077e+02, 2.2090e+02, 2.3235e+02],\n",
      "        [2.7340e+02, 3.0376e+02, 2.9776e+02, 3.5900e+02],\n",
      "        [2.0079e-03, 1.8257e+02, 1.2171e+01, 2.3317e+02],\n",
      "        [1.4282e+02, 8.2751e+01, 2.8886e+02, 2.4378e+02],\n",
      "        [1.5257e+02, 3.0891e+02, 1.7672e+02, 3.5587e+02],\n",
      "        [1.3470e+02, 3.2226e+02, 1.8927e+02, 3.4956e+02],\n",
      "        [1.0347e+02, 1.9902e+02, 3.5326e+02, 3.0619e+02],\n",
      "        [1.8143e+02, 1.9660e+02, 2.3186e+02, 2.2394e+02],\n",
      "        [2.3206e+02, 1.4384e+02, 2.5921e+02, 1.9260e+02],\n",
      "        [9.7584e-03, 7.0451e+01, 2.5077e+01, 9.7029e+01],\n",
      "        [1.5152e+02, 2.2067e+02, 1.7723e+02, 2.7015e+02],\n",
      "        [1.7895e+02, 2.3703e+02, 2.3517e+02, 2.6319e+02],\n",
      "        [1.3050e+02, 2.4925e+02, 3.6000e+02, 3.4497e+02],\n",
      "        [5.5730e-02, 4.4828e+01, 4.6400e+01, 1.2250e+02],\n",
      "        [3.0691e-02, 1.8442e+02, 4.2626e+01, 2.3989e+02],\n",
      "        [1.0760e-02, 5.6668e+01, 1.1789e+01, 1.1087e+02],\n",
      "        [0.0000e+00, 1.9638e+02, 2.2128e+01, 2.2359e+02],\n",
      "        [0.0000e+00, 2.0978e+02, 2.9481e+01, 3.0950e+02],\n",
      "        [8.1758e-02, 1.0702e+02, 2.1241e+01, 1.4612e+02],\n",
      "        [1.0281e+02, 2.8787e+02, 3.6000e+02, 3.5900e+02],\n",
      "        [4.0558e-01, 5.3632e+01, 9.1291e+01, 1.1052e+02],\n",
      "        [2.5566e+02, 2.6457e+02, 3.3199e+02, 3.3884e+02],\n",
      "        [1.3828e+02, 2.1913e+02, 1.8950e+02, 3.1257e+02],\n",
      "        [0.0000e+00, 3.5081e+01, 2.7967e+01, 1.3966e+02],\n",
      "        [1.4323e+01, 6.9573e+01, 6.5704e+01, 9.6834e+01],\n",
      "        [0.0000e+00, 7.3897e+01, 2.8925e+01, 1.7972e+02],\n",
      "        [1.3429e+02, 1.7068e+02, 2.1321e+02, 2.4726e+02],\n",
      "        [1.9218e+02, 1.4096e+02, 2.1772e+02, 1.9274e+02],\n",
      "        [1.6419e+02, 1.0211e+02, 2.6610e+02, 1.5486e+02],\n",
      "        [0.0000e+00, 1.8392e+02, 1.0476e+02, 3.5900e+02],\n",
      "        [1.7385e+02, 1.2695e+02, 3.2274e+02, 2.8376e+02],\n",
      "        [2.7863e+01, 5.6039e+01, 5.3128e+01, 1.0843e+02],\n",
      "        [4.1842e-01, 3.2782e+01, 1.1303e+02, 1.3716e+02],\n",
      "        [1.8890e+02, 1.7757e+02, 3.3325e+02, 3.2533e+02],\n",
      "        [1.5101e+02, 1.8097e+02, 1.7793e+02, 2.3181e+02],\n",
      "        [2.5676e-02, 2.2378e+02, 5.5739e+01, 3.5900e+02],\n",
      "        [2.2341e-01, 1.8140e+02, 9.1174e+01, 2.3938e+02],\n",
      "        [1.4635e+02, 4.5821e+01, 3.6000e+02, 2.7834e+02],\n",
      "        [2.2145e+02, 1.2699e+02, 2.9449e+02, 2.0773e+02],\n",
      "        [1.2816e+02, 1.4529e+02, 3.6000e+02, 2.5326e+02],\n",
      "        [7.1279e-01, 5.9926e+01, 8.7284e+01, 2.9192e+02],\n",
      "        [3.7603e+00, 8.2269e+01, 1.5475e+02, 2.3435e+02],\n",
      "        [1.4004e+02, 1.9566e+02, 1.9213e+02, 2.2491e+02],\n",
      "        [3.3238e-02, 2.6385e+02, 1.3757e+01, 3.1560e+02],\n",
      "        [2.7393e+02, 2.6346e+02, 2.9952e+02, 3.2075e+02],\n",
      "        [2.1550e+02, 1.9609e+02, 2.7559e+02, 2.2150e+02],\n",
      "        [1.8900e-01, 2.6711e+02, 4.8807e+01, 3.2358e+02],\n",
      "        [7.0549e+01, 2.6867e+02, 1.8260e+02, 3.2176e+02],\n",
      "        [0.0000e+00, 2.4063e+02, 2.5434e+01, 2.6673e+02],\n",
      "        [2.1197e+01, 1.0710e+02, 6.0306e+01, 1.4489e+02],\n",
      "        [1.1243e-01, 9.7489e+01, 8.9712e+01, 1.5319e+02],\n",
      "        [2.1334e+02, 1.1806e+02, 2.6975e+02, 1.4356e+02],\n",
      "        [1.2931e+00, 1.5747e+02, 2.1795e+02, 2.6182e+02],\n",
      "        [0.0000e+00, 2.1863e+02, 8.8942e+01, 2.7537e+02],\n",
      "        [2.1853e+02, 1.6366e+02, 2.7574e+02, 2.6707e+02],\n",
      "        [2.2099e+02, 2.2745e+02, 3.6000e+02, 3.5900e+02],\n",
      "        [1.7901e+02, 1.1358e+02, 2.2864e+02, 1.4097e+02],\n",
      "        [2.5788e+02, 2.0349e+02, 3.1695e+02, 3.1464e+02],\n",
      "        [8.9804e-02, 2.4736e+02, 3.0441e+01, 3.5384e+02],\n",
      "        [1.5631e-01, 1.1435e+02, 2.8040e+01, 2.1796e+02],\n",
      "        [1.9139e+02, 2.2018e+02, 2.1797e+02, 2.7108e+02],\n",
      "        [0.0000e+00, 2.2491e+02, 4.1858e+01, 2.7986e+02],\n",
      "        [5.4691e+01, 7.0423e+01, 1.0661e+02, 9.6707e+01],\n",
      "        [1.8020e+02, 1.5681e+02, 2.2963e+02, 1.8463e+02],\n",
      "        [2.9110e+02, 3.2118e+02, 3.5105e+02, 3.5009e+02],\n",
      "        [2.8992e+01, 5.5740e+01, 1.3382e+02, 1.1054e+02],\n",
      "        [1.0871e+02, 1.8297e+02, 1.3448e+02, 2.3408e+02]], device='cuda:0',\n",
      "       grad_fn=<StackBackward>), 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1], device='cuda:0'), 'scores': tensor([0.5532, 0.5506, 0.5467, 0.5459, 0.5423, 0.5406, 0.5398, 0.5396, 0.5390,\n",
      "        0.5390, 0.5380, 0.5371, 0.5349, 0.5347, 0.5346, 0.5337, 0.5308, 0.5300,\n",
      "        0.5296, 0.5270, 0.5267, 0.5266, 0.5264, 0.5262, 0.5258, 0.5257, 0.5252,\n",
      "        0.5230, 0.5220, 0.5218, 0.5204, 0.5198, 0.5191, 0.5191, 0.5189, 0.5185,\n",
      "        0.5184, 0.5181, 0.5181, 0.5181, 0.5175, 0.5173, 0.5172, 0.5170, 0.5167,\n",
      "        0.5157, 0.5156, 0.5156, 0.5144, 0.5140, 0.5133, 0.5127, 0.5127, 0.5115,\n",
      "        0.5114, 0.5111, 0.5108, 0.5100, 0.5092, 0.5087, 0.5086, 0.5081, 0.5081,\n",
      "        0.5081, 0.5073, 0.5068, 0.5067, 0.5063, 0.5063, 0.5059, 0.5053, 0.5049,\n",
      "        0.5049, 0.5048, 0.5047, 0.5044, 0.5034, 0.5033, 0.5028, 0.5024, 0.5019,\n",
      "        0.5017, 0.5016, 0.5011, 0.5010, 0.5008, 0.5005, 0.5001, 0.5000, 0.4999,\n",
      "        0.4996, 0.4988, 0.4986, 0.4983, 0.4981, 0.4980, 0.4979, 0.4978, 0.4976,\n",
      "        0.4970], device='cuda:0', grad_fn=<IndexBackward>)}, {'boxes': tensor([[1.6026e+02, 1.9714e+02, 1.8169e+02, 2.5035e+02],\n",
      "        [1.4748e+02, 2.0584e+02, 1.9667e+02, 2.3501e+02],\n",
      "        [7.9227e+00, 1.0170e+02, 1.6966e+02, 2.6876e+02],\n",
      "        [7.2751e+01, 9.8078e+01, 1.0052e+02, 1.5398e+02],\n",
      "        [1.8259e+02, 2.0686e+02, 2.3593e+02, 2.3328e+02],\n",
      "        [1.0084e+02, 7.5749e+01, 1.6206e+02, 1.9511e+02],\n",
      "        [6.5804e+01, 7.6630e+01, 1.1504e+02, 1.0382e+02],\n",
      "        [0.0000e+00, 7.6387e+01, 2.0961e+02, 3.7600e+02],\n",
      "        [8.0386e+01, 1.9376e+02, 1.8272e+02, 2.5587e+02],\n",
      "        [1.1380e+02, 7.7255e+01, 1.6262e+02, 1.0290e+02],\n",
      "        [3.0939e+01, 3.0781e+01, 1.4556e+02, 3.2750e+02],\n",
      "        [1.1569e+02, 5.9212e+01, 1.4158e+02, 1.1497e+02],\n",
      "        [6.5910e+01, 1.1929e+02, 1.1848e+02, 1.4543e+02],\n",
      "        [1.1362e+02, 1.9155e+02, 1.3758e+02, 2.4690e+02],\n",
      "        [1.6983e+02, 9.1698e+01, 2.8480e+02, 3.5333e+02],\n",
      "        [7.0530e+01, 5.7905e+01, 1.8656e+02, 1.1625e+02],\n",
      "        [1.1166e+02, 2.0899e+02, 1.6412e+02, 2.3636e+02],\n",
      "        [7.3752e+01, 5.7731e+01, 9.9272e+01, 1.1275e+02],\n",
      "        [8.3154e+01, 7.0529e+00, 1.8680e+02, 2.6859e+02],\n",
      "        [1.0405e+02, 1.2231e+02, 1.6151e+02, 2.4411e+02],\n",
      "        [3.5524e+01, 1.5498e+02, 1.4915e+02, 2.0788e+02],\n",
      "        [1.0274e+02, 1.7152e+02, 1.5817e+02, 2.8302e+02],\n",
      "        [3.2204e+01, 1.0369e+02, 1.4744e+02, 1.6060e+02],\n",
      "        [7.4911e+01, 1.2593e+02, 1.7521e+02, 3.6351e+02],\n",
      "        [1.6815e+02, 8.6171e+01, 3.4523e+02, 2.6839e+02],\n",
      "        [1.1188e+02, 1.6257e+02, 1.6555e+02, 1.8908e+02],\n",
      "        [6.6840e+01, 1.0602e+02, 1.8273e+02, 1.6339e+02],\n",
      "        [1.1643e+02, 1.0235e+02, 1.4142e+02, 1.5425e+02],\n",
      "        [6.8920e+01, 1.2559e+02, 1.0839e+02, 1.6239e+02],\n",
      "        [1.2852e+02, 7.1262e+01, 2.2876e+02, 3.0210e+02],\n",
      "        [1.1618e+02, 1.4381e+02, 1.3948e+02, 1.9643e+02],\n",
      "        [2.0565e+02, 2.8282e+02, 2.3073e+02, 3.3409e+02],\n",
      "        [1.8768e+02, 2.4444e+02, 2.3857e+02, 2.8457e+02],\n",
      "        [9.1003e+01, 7.7814e+01, 2.7595e+02, 2.6805e+02],\n",
      "        [2.2555e+02, 3.3745e+02, 2.8858e+02, 3.6430e+02],\n",
      "        [2.0218e+02, 2.3678e+02, 2.2835e+02, 2.8951e+02],\n",
      "        [1.4690e+00, 4.2577e+01, 1.0367e+02, 3.0956e+02],\n",
      "        [5.8588e+01, 1.3887e+02, 1.1913e+02, 2.5177e+02],\n",
      "        [1.8317e+02, 2.7083e+02, 2.6192e+02, 3.5847e+02],\n",
      "        [1.5382e+01, 1.5139e+02, 9.0553e+01, 2.2926e+02],\n",
      "        [1.8767e+02, 2.3311e+02, 2.7318e+02, 3.1706e+02],\n",
      "        [1.0646e+02, 1.1906e+02, 1.6286e+02, 1.4431e+02],\n",
      "        [2.3071e+02, 2.9023e+02, 2.7995e+02, 3.3016e+02],\n",
      "        [1.2464e+02, 2.3152e+02, 2.3036e+02, 2.9508e+02],\n",
      "        [7.2965e+01, 1.3842e+02, 9.8156e+01, 1.9308e+02],\n",
      "        [3.6113e+01, 1.2111e+02, 2.4048e+02, 2.3304e+02],\n",
      "        [6.0123e+01, 3.9699e+01, 1.1994e+02, 1.5436e+02],\n",
      "        [1.0241e+02, 3.5191e+01, 1.6494e+02, 1.4398e+02],\n",
      "        [8.8969e+01, 1.1058e+01, 3.5029e+02, 3.7574e+02],\n",
      "        [1.9783e+02, 1.7804e+02, 3.2888e+02, 2.4028e+02],\n",
      "        [2.5084e+01, 1.6358e+02, 6.8932e+01, 2.0028e+02],\n",
      "        [7.8167e-01, 1.7127e+02, 1.5270e+02, 2.7860e+02],\n",
      "        [1.8970e+02, 2.8777e+02, 2.3598e+02, 3.2957e+02],\n",
      "        [2.2697e+02, 2.0600e+02, 2.9215e+02, 2.3140e+02],\n",
      "        [4.4534e-01, 1.4950e+02, 4.3998e+01, 2.2730e+02],\n",
      "        [1.6384e+02, 2.3248e+02, 3.0036e+02, 2.8868e+02],\n",
      "        [1.0710e+02, 1.0910e+02, 2.2007e+02, 1.6665e+02],\n",
      "        [6.6812e+01, 1.6236e+02, 1.1978e+02, 1.9018e+02],\n",
      "        [1.6905e+00, 1.4715e+02, 1.0801e+02, 2.0324e+02],\n",
      "        [1.2184e-01, 1.5159e+02, 1.2956e+01, 2.0296e+02],\n",
      "        [1.7558e+02, 1.9293e+02, 2.5988e+02, 2.7083e+02],\n",
      "        [2.6582e-01, 1.6234e+02, 2.6551e+01, 1.9076e+02],\n",
      "        [9.3534e+01, 2.5805e+02, 3.1614e+02, 3.6929e+02],\n",
      "        [1.8664e+02, 3.3731e+02, 2.4393e+02, 3.6533e+02],\n",
      "        [2.2844e+02, 2.4644e+02, 2.8336e+02, 2.8731e+02],\n",
      "        [2.3139e+02, 2.7434e+02, 3.1153e+02, 3.5022e+02],\n",
      "        [6.8601e+01, 1.6646e+02, 1.0775e+02, 2.0129e+02],\n",
      "        [1.0567e+00, 5.8220e+01, 5.9272e+01, 2.8565e+02],\n",
      "        [1.8190e+02, 1.0199e+02, 2.5836e+02, 1.8889e+02],\n",
      "        [2.9797e+01, 1.4482e+02, 5.4750e+01, 1.9611e+02],\n",
      "        [2.2341e+02, 1.8735e+02, 3.0458e+02, 2.6422e+02],\n",
      "        [2.2590e+01, 8.1891e+01, 2.5719e+02, 1.9318e+02],\n",
      "        [2.2587e+02, 1.6488e+02, 2.9079e+02, 1.9182e+02],\n",
      "        [2.9838e+01, 1.8980e+02, 5.4254e+01, 2.3852e+02],\n",
      "        [2.2141e+02, 1.4123e+02, 3.0212e+02, 2.2323e+02],\n",
      "        [1.5085e+02, 2.3048e+02, 2.0299e+02, 3.3768e+02],\n",
      "        [1.8515e+00, 1.9220e+02, 1.1192e+02, 2.5028e+02],\n",
      "        [2.3719e+02, 1.8472e+02, 3.6183e+02, 2.5079e+02],\n",
      "        [7.4002e+01, 2.3508e+02, 1.0060e+02, 2.8617e+02],\n",
      "        [1.7335e+02, 1.4652e+02, 2.5141e+02, 2.2798e+02],\n",
      "        [2.0000e+02, 2.2370e+02, 3.4503e+02, 2.8444e+02],\n",
      "        [1.8822e+02, 2.3385e+02, 3.0369e+02, 3.7529e+02],\n",
      "        [4.6698e+01, 1.8841e+02, 3.6344e+02, 3.7600e+02],\n",
      "        [2.4624e+02, 2.7769e+02, 2.7155e+02, 3.2974e+02],\n",
      "        [7.1007e-01, 5.9090e+01, 1.0803e+02, 1.1662e+02],\n",
      "        [1.4949e+02, 2.5001e+02, 1.9576e+02, 2.7908e+02],\n",
      "        [2.4074e+02, 2.3349e+02, 2.7041e+02, 2.8612e+02],\n",
      "        [7.3083e+01, 1.8592e+02, 9.6373e+01, 2.3864e+02],\n",
      "        [1.4281e+02, 1.6475e+02, 1.9791e+02, 1.9355e+02],\n",
      "        [6.2018e+01, 2.5387e+02, 1.1208e+02, 2.8216e+02],\n",
      "        [1.8316e+02, 1.6685e+02, 2.4080e+02, 1.9401e+02],\n",
      "        [1.4613e+02, 2.7650e+02, 2.2495e+02, 3.6179e+02],\n",
      "        [1.8187e+02, 1.2200e+02, 2.4433e+02, 1.4871e+02],\n",
      "        [1.4135e+02, 1.6136e+02, 3.5845e+02, 2.7600e+02],\n",
      "        [3.2890e+02, 1.9762e+02, 3.5221e+02, 2.5112e+02],\n",
      "        [8.2123e+01, 2.2756e+02, 1.8909e+02, 2.9150e+02],\n",
      "        [1.2896e+01, 6.5819e+01, 5.0800e+02, 3.0350e+02],\n",
      "        [3.2891e+02, 1.5287e+02, 3.5317e+02, 2.0353e+02],\n",
      "        [2.0344e+02, 3.2083e+02, 3.1477e+02, 3.7600e+02],\n",
      "        [3.1881e-01, 2.0475e+02, 1.8941e+01, 2.4582e+02]], device='cuda:0',\n",
      "       grad_fn=<StackBackward>), 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1], device='cuda:0'), 'scores': tensor([0.5966, 0.5950, 0.5879, 0.5874, 0.5870, 0.5838, 0.5816, 0.5790, 0.5782,\n",
      "        0.5780, 0.5774, 0.5773, 0.5748, 0.5725, 0.5717, 0.5713, 0.5704, 0.5701,\n",
      "        0.5695, 0.5691, 0.5688, 0.5681, 0.5677, 0.5675, 0.5674, 0.5669, 0.5666,\n",
      "        0.5658, 0.5649, 0.5621, 0.5617, 0.5614, 0.5607, 0.5605, 0.5600, 0.5600,\n",
      "        0.5596, 0.5585, 0.5582, 0.5573, 0.5570, 0.5565, 0.5561, 0.5540, 0.5538,\n",
      "        0.5536, 0.5533, 0.5516, 0.5513, 0.5481, 0.5481, 0.5476, 0.5473, 0.5468,\n",
      "        0.5458, 0.5455, 0.5451, 0.5448, 0.5438, 0.5435, 0.5434, 0.5431, 0.5419,\n",
      "        0.5417, 0.5410, 0.5404, 0.5390, 0.5379, 0.5375, 0.5372, 0.5371, 0.5368,\n",
      "        0.5363, 0.5359, 0.5357, 0.5344, 0.5342, 0.5318, 0.5318, 0.5314, 0.5295,\n",
      "        0.5295, 0.5294, 0.5292, 0.5292, 0.5285, 0.5280, 0.5274, 0.5262, 0.5258,\n",
      "        0.5256, 0.5249, 0.5246, 0.5245, 0.5244, 0.5244, 0.5241, 0.5236, 0.5235,\n",
      "        0.5235], device='cuda:0', grad_fn=<IndexBackward>)}]\n"
     ]
    }
   ],
   "source": [
    "input_image1 = image1.clone()\n",
    "\n",
    "input_image2 = image2.clone()\n",
    "\n",
    "# input is image list\n",
    "inputs = [input_image1.to(device), input_image2.to(device)]\n",
    "\n",
    "ft_model.eval()\n",
    "output = ft_model(inputs)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TU1Va_EsE9zB"
   },
   "source": [
    "## <font style=\"color:green\">2.2. Model Training</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "EAXi7c95E9zC",
    "outputId": "7f93ddda-4bf4-4ca6-8a65-5aa54d18d934"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss_box_reg': tensor(0.0387, device='cuda:0', grad_fn=<DivBackward0>),\n",
       " 'loss_classifier': tensor(0.6748, device='cuda:0', grad_fn=<NllLossBackward>),\n",
       " 'loss_objectness': tensor(0.6991, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>),\n",
       " 'loss_rpn_box_reg': tensor(0.0081, device='cuda:0', grad_fn=<DivBackward0>)}"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_image1 = image1.clone()\n",
    "\n",
    "target1 = {\n",
    "    'boxes': bboxes1.clone().to(device),\n",
    "    'labels' : labels1.clone().to(device)\n",
    "    \n",
    "} \n",
    "\n",
    "input_image2 = image2.clone()\n",
    "\n",
    "\n",
    "target2 = {\n",
    "    'boxes': bboxes2.clone().to(device),\n",
    "    'labels' : labels2.clone().to(device)\n",
    "    \n",
    "} \n",
    "\n",
    "inputs = [input_image1.to(device), input_image2.to(device)]\n",
    "targets = [target1, target2]\n",
    "\n",
    "# change to train mode\n",
    "ft_model.train()\n",
    "ft_model(inputs, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4_qGiCNGE9zF"
   },
   "source": [
    "**In the next section, we will see the training and validation of Faster RCNN model.**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Overview-FasterRCNN-TorchVision-Implementation.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "19b9c4d703a0452e80cb78c04a161225": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1bf9068908194a49a8afdd334216c316": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5905b69a9b234ef4973c06bf12f4775b",
      "max": 244418560,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_55d6172edaec44b2ac84a4f8ab1f8feb",
      "value": 244418560
     }
    },
    "3673b6da00f8477f8090e6f1f3c901e3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d1a673f3678a4baba36765c3350babb0",
      "placeholder": "​",
      "style": "IPY_MODEL_19b9c4d703a0452e80cb78c04a161225",
      "value": " 233M/233M [00:03&lt;00:00, 74.7MB/s]"
     }
    },
    "55d6172edaec44b2ac84a4f8ab1f8feb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "5905b69a9b234ef4973c06bf12f4775b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d1a673f3678a4baba36765c3350babb0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f1883bac6a3f4680b0c1e3c62d3587e4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f392d0f2a23348e982f730ba12da4cfc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1bf9068908194a49a8afdd334216c316",
       "IPY_MODEL_3673b6da00f8477f8090e6f1f3c901e3"
      ],
      "layout": "IPY_MODEL_f1883bac6a3f4680b0c1e3c62d3587e4"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
