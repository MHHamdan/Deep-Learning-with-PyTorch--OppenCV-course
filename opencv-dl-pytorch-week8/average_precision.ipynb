{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">Mean Average Precision (mAP)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "In this unit we will discuss and implement a variant of a very common object\n",
    "detection metric - Average Precision (mAP).\n",
    "\n",
    "The variant of Average Precision (AP) metric we're going to implement was\n",
    "introduced in the [PASCAL\n",
    "VOC](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/) challenge. During the\n",
    "years of the challenge, it was changed several times, so let's discuss the\n",
    "common pipeline and its specific parts.\n",
    "\n",
    "We will implement the AP calculation with some limitations. They are not\n",
    "applicable to the real cases, but your final code may be easily transofrmed\n",
    "to a general case.\n",
    "1. All the ground truth objects and detection objects are from the same image\n",
    "   (we consider the case of one image only).\n",
    "2. All the ground truth objects and detection objects belong to the same\n",
    "   class (we consider the case of 1 class only).\n",
    "\n",
    "Let's briefly recall the essential concepts:\n",
    "* **True Positives (TP)** - correctly detected objects.\n",
    "* **False Positives (FP)** - incorrectly detected objects.\n",
    "* **False Negative (FN)** - GT boxes that do not have a\n",
    "  corresponding detection with high enough IoU.\n",
    "* **Precision** is the ratio of true positives in the obtained results. In\n",
    "  other words, it is the percentage of correct predictions among all\n",
    "  predictions:\n",
    "  $ \\text{precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}} =\n",
    "  \\frac{\\text{TP}}{D},$ where $D$ is a total number of detections.\n",
    "* **Recall** - the amount of true positives we found among all the GT boxes in the data.\n",
    "  $\\text{recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}} =\n",
    "  \\frac{\\text{TP}}{G},$ where $G$ is a total number of ground truth objects.\n",
    "* **Intersection over Union** is a way to measure the level of overlap\n",
    "  between bounding boxes, for example, between ground truth and predicted\n",
    "  detection boxes: $\\text{IoU} = \\frac{\\text{intersection_area}}{\\text{union_area}}$\n",
    "\n",
    "Computation pipeline in the object detection context is the following:\n",
    "1. for each confidence threshold among selected confidence levels:\n",
    "     1. select detections with confidences higher than the confidence threshold\n",
    "     2. find the correspondence between the detections and ground truths:\n",
    "         1. sort the detections according to their scores in descending order\n",
    "         2. compute the Intersection over Union (IoU) of detection boxes with GT\n",
    "         bounding boxes\n",
    "         3. for each detection, find the ground truth with the highest IoU higher than\n",
    "         IoU threshold\n",
    "         4. for each ground truth, find all the detections with IoUs higher than IoU\n",
    "         threshold.\n",
    "     3. define and count the true positives, false positives, false negatives.\n",
    "         1. all the detections associated with ground truths that have only one match\n",
    "         are considered true positives\n",
    "         2. in case if the ground truth object has more than one associated detection:\n",
    "             - the associated detection with the highest score is considered to be the true\n",
    "             positive\n",
    "             - other detections are considered to be false positives\n",
    "         3. all detections that have no associated ground truth objects (due to too\n",
    "         low IoUs) are considered false positives\n",
    "     4. calculate the recall and the precision with the given numbers of TP, FP and ground\n",
    "     truth objects\n",
    "2. after the step 1, we have precision at some points - interpolate the\n",
    "precision at all other points\n",
    "3. calculate the Average Precision (AP) as the area under the interpolated curve\n",
    "\n",
    "When we computed pairs of recall and precision at confidence thresholds, we\n",
    "can plot a precision-recall curve: x-axis is recall, and y-axis - precision.\n",
    "\n",
    "In practice, researchers typically use an interpolated version of the\n",
    "recall-precision curve. In this case, we do not estimate the recall-precision\n",
    "pairs for every possible confidence threshold. Instead, we select some\n",
    "points, measure the recall and precision in them, and interpolate all the\n",
    "values in between. To compute the interpolated precision, we use the maximum\n",
    "value for the specified recall level:\n",
    "$p_{\\text{interp}}(r) = \\underset{r':r'\\geq r}\\max{p(r')}$\n",
    "\n",
    "When all the ingrediends we described are defined, we can evaluate the mean\n",
    "average precision value. According to [The PASCAL Visual Object Classes (VOC)\n",
    "Challenge](http://homepages.inf.ed.ac.uk/ckiw/postscript/ijcv_voc09.pdf), the\n",
    "mean precision at a set of eleven equal recalls defines the AP:\n",
    "\n",
    "$$\n",
    "\\text{AP} = \\frac{1}{11}_{r\\in{\\{0, 0.1,...,1\\}}}\\sum{p_{\\text{interp}}(r)}\n",
    "$$\n",
    "\n",
    "Thus, AP summarizes the interpolated precisions at `11` recall areas from  0 to 1: `0.0, 0.1, ..., 1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's Implement Average Precision (as an approximation of the\n",
    "precision-recall curve by 11 points) based on the [The PASCAL Visual Object\n",
    "Classes (VOC)\n",
    "Challenge](http://homepages.inf.ed.ac.uk/ckiw/postscript/ijcv_voc09.pdf)\n",
    "article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:Blue\">1. Utils Methods</font>\n",
    "\n",
    "### <font style=\"color:green\">Intersection Over Union (IoU)</font>\n",
    "\n",
    "Find IoU for a predicted bounding box with all ground truth boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iou(predicted_dbox, ground_truth_dboxes):\n",
    "    \"\"\"\n",
    "        Calculates IoU (Jaccard index) of two detection boxes:\n",
    "            predicted_dbox ∩ ground_truth_dbox / (area(predicted_dbox) +\n",
    "            area(ground_truth_dbox) - predicted_dbox ∩ ground_truth_dbox)\n",
    "\n",
    "        Parameters:\n",
    "            Coordinates of detection boxes are supposed to be in the following form: [x1, y1, x2, y2]\n",
    "            predicted_dbox: [tensor] predicted detection boxes\n",
    "            ground_truth_dboxes: [tensor] ground truth detection boxes\n",
    "\n",
    "        Return value:\n",
    "            overlap area\n",
    "    \"\"\"\n",
    "    \n",
    "    ixmin = torch.max(ground_truth_dboxes[:, 0], predicted_dbox[0])\n",
    "    iymin = torch.max(ground_truth_dboxes[:, 1], predicted_dbox[1])\n",
    "    ixmax = torch.min(ground_truth_dboxes[:, 2], predicted_dbox[2])\n",
    "    iymax = torch.min(ground_truth_dboxes[:, 3], predicted_dbox[3])\n",
    "    # except:\n",
    "    # print('GT:', ground_truth_dboxes)\n",
    "    # print('PR:', predicted_dbox)\n",
    "    # raise\n",
    "\n",
    "    width = torch.max(ixmax - ixmin + 1., torch.tensor(0.))\n",
    "    height = torch.max(iymax - iymin + 1., torch.tensor(0.))\n",
    "    intersection_area = width * height\n",
    "\n",
    "    union = ((predicted_dbox[2] - predicted_dbox[0] + 1.) * (predicted_dbox[3] - predicted_dbox[1] + 1.) +\n",
    "             (ground_truth_dboxes[:, 2] - ground_truth_dboxes[:, 0] + 1.) *\n",
    "             (ground_truth_dboxes[:, 3] - ground_truth_dboxes[:, 1] + 1.) - intersection_area)\n",
    "\n",
    "    return intersection_area / union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:green\">Check IoU Threshold</font>\n",
    "\n",
    "Check if the predicted box IoU with ground truth is greater than the threshold and also get ground truth bbox index that has maximum overlap with predicted bbox. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_iou_threshold(predicted_dboxes, ground_truth_dboxes, intersection_threshold):\n",
    "    \"\"\"\n",
    "        Get the predictions with an appropriate IoU area for further true positives computations\n",
    "\n",
    "        Parameters:\n",
    "        predicted_dboxes: predicted by the detector detection boxes\n",
    "        ground_truth_dboxes: ground truth\n",
    "        intersection_threshold: IoU threshold\n",
    "\n",
    "        Return value:\n",
    "            tensor with the following values:\n",
    "                    True - if the IoU passed defined threshold\n",
    "                    False - if the IoU did not pass defined threshold\n",
    "            index of the maximum IoU value\n",
    "    \"\"\"\n",
    "    intersection_over_union = get_iou(predicted_dboxes, ground_truth_dboxes)\n",
    "    return torch.max(intersection_over_union) >= intersection_threshold, torch.argmax(intersection_over_union)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:green\">Match Predicted Bboxes with GT Bboxes</font>\n",
    "\n",
    "Check if predicted bounding boxes match with ground truth bounding boxes for given IoU threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match(sorted_ind, predicted_dboxes, ground_truth_dboxes, intersection_threshold):\n",
    "    \n",
    "    true_positives = torch.zeros((len(sorted_ind)), dtype=torch.float64)\n",
    "    false_positives = torch.zeros((len(sorted_ind)), dtype=torch.float64)\n",
    "    is_obj_already_detected = [False for _ in range(ground_truth_dboxes.shape[0])]\n",
    "\n",
    "    for box_num in range(len(sorted_ind)):\n",
    "        predicted_dbox = predicted_dboxes[box_num, :]\n",
    "\n",
    "        is_pass_threshold, max_iou_index = check_iou_threshold(\n",
    "            predicted_dbox, ground_truth_dboxes, intersection_threshold\n",
    "        )\n",
    "        \n",
    "        if is_pass_threshold and not is_obj_already_detected[max_iou_index]:\n",
    "            true_positives[box_num] = 1\n",
    "            is_obj_already_detected[max_iou_index] = True\n",
    "        else:\n",
    "            false_positives[box_num] = 1\n",
    "\n",
    "    return true_positives, false_positives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:Blue\">2. Average Precission Evaluator</font>\n",
    "\n",
    "Here, we have a class `AveragePrecisionEvaluator` to calculate the average precision metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AveragePrecisionEvaluator:\n",
    "    def __init__(self, intersection_threshold=0.5, points_number=11):\n",
    "        self.recall_levels = torch.linspace(0, 1, points_number)\n",
    "        self.intersection_threshold = intersection_threshold\n",
    "        self.recalls = None\n",
    "        self.precisions = None\n",
    "        self.average_precision = 0.\n",
    "        self.true_positives = None\n",
    "        self.false_positives = None\n",
    "        self.false_negatives = None\n",
    "\n",
    "    def run_ap_calculation(self, predicted_dboxes, predicted_dbox_score, ground_truth_dboxes):\n",
    "        \"\"\"\n",
    "            Initiate AP calculation process\n",
    "\n",
    "            Parameters:\n",
    "                predicted_dboxes: predicted detection boxes\n",
    "                predicted_dbox_score: confidence scores of detection boxes\n",
    "                ground_truth_dboxes: ground truth boxes\n",
    "        \"\"\"\n",
    "        sorted_ind = np.argsort(-predicted_dbox_score)\n",
    "        predicted_dboxes = predicted_dboxes[sorted_ind, :]\n",
    "\n",
    "        true_positives = torch.zeros((len(sorted_ind)), dtype=torch.float64)\n",
    "        false_positives = torch.zeros((len(sorted_ind)), dtype=torch.float64)\n",
    "\n",
    "        if len(ground_truth_dboxes) == 0:\n",
    "            false_positives = torch.ones((len(sorted_ind)), dtype=torch.float64)\n",
    "        else:\n",
    "            true_positives, false_positives = match(\n",
    "                sorted_ind, predicted_dboxes, ground_truth_dboxes, self.intersection_threshold\n",
    "            )\n",
    "\n",
    "        self.true_positives = torch.cumsum(true_positives, dim=0)\n",
    "        self.false_positives = torch.cumsum(false_positives, dim=0)\n",
    "        self.false_negatives = ground_truth_dboxes.shape[0] - self.true_positives\n",
    "\n",
    "        # with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        self.precisions = self.true_positives / (\n",
    "            self.false_positives + self.true_positives + torch.finfo(torch.float64).eps\n",
    "        )\n",
    "\n",
    "        # with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        self.recalls = self.true_positives / (\n",
    "            self.true_positives + self.false_negatives + torch.finfo(torch.float64).eps\n",
    "        )\n",
    "\n",
    "        return self.get_voc_ap()\n",
    "\n",
    "    def get_voc_ap(self):\n",
    "        \"\"\"\n",
    "            Evaluates VOC Mean Average Precision\n",
    "\n",
    "        \"\"\"\n",
    "        self.average_precision = torch.tensor(0.)\n",
    "        if self.precisions is None or self.recalls is None:\n",
    "            self.average_precision = torch.tensor(np.nan)\n",
    "\n",
    "        for recall_level in self.recall_levels:\n",
    "            recalls_check = self.recalls >= recall_level\n",
    "            if torch.sum(recalls_check) == 0:\n",
    "                val = torch.tensor(0.)\n",
    "            else:\n",
    "                val = torch.max(self.precisions[recalls_check])\n",
    "            self.average_precision = self.average_precision + val\n",
    "\n",
    "        self.average_precision = self.average_precision / float(self.recall_levels.shape[0])\n",
    "\n",
    "        return self.average_precision\n",
    "\n",
    "    def precision_recall_curve(self, experiment_name):\n",
    "        \"\"\"\n",
    "            Precision-recall curve visualization for specified class\n",
    "\n",
    "            Parameters:\n",
    "                experiment_name: title of the running experiment\n",
    "        \"\"\"\n",
    "        plt.plot(self.recalls, self.precisions)\n",
    "        plt.xlabel('recall')\n",
    "        plt.ylabel('precision')\n",
    "        fig_name = \"_rec_prec_curve\"\n",
    "        plt.savefig(experiment_name + fig_name, bbox_inches='tight')\n",
    "\n",
    "    def visualize_results(self, experiment_name, is_chart=False):\n",
    "        \"\"\"\n",
    "            Summarized visualization of obtained results\n",
    "\n",
    "            Parameters:\n",
    "                experiment_name: title of the running experiment\n",
    "                is_chart: flag for chart visualization\n",
    "        \"\"\"\n",
    "        if is_chart:\n",
    "            self.precision_recall_curve(experiment_name)\n",
    "        print(\n",
    "            \"=== {}:\\n recall: {}, \\n precision: {}, \\n VOC mAP: {}\".format(\n",
    "                experiment_name, self.recalls, self.precisions, self.average_precision\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:green\">Check Implementation</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_ap():\n",
    "    # pylint: disable=not-callable\n",
    "    # ground truth\n",
    "    ground_truth_dboxes = torch.tensor([[8, 12, 352, 498], [10, 15, 450, 500]], dtype=torch.float)\n",
    "\n",
    "    # predicted results\n",
    "    test_dboxes_predicted = torch.tensor([\n",
    "        [1.000000, 116.384613, 353.000000, 116.384613], [1.000000, 1.000000, 353.000000, 500.000000],\n",
    "        [9.000000, 14.000000, 452.500000, 500.500000], [1.000000, 154.846161, 353.000000, 154.846161],\n",
    "        [196.776474, 231.769226, 196.776474, 231.769226], [1.000000, 116.384613, 353.000000, 116.384613],\n",
    "        [1.000000, 1.000000, 353.000000, 500.000000], [1.000000, 231.769226, 353.000000, 231.769226],\n",
    "        [45.000000, 235.230774, 175.000000, 361.230774], [44.500000, 234.230774, 176.000000, 362.230774]\n",
    "    ],\n",
    "                                         dtype=torch.float)\n",
    "\n",
    "    predicted_scores = torch.tensor([\n",
    "        0.180000, 0.985000, 0.98000, 0.200000, 0.090000, 0.170000, 0.570000, 0.210000, 0.855000, 0.850000\n",
    "    ],\n",
    "                                    dtype=torch.float)\n",
    "\n",
    "    # run check\n",
    "    ap_evaluator = AveragePrecisionEvaluator()\n",
    "    ap_evaluator.run_ap_calculation(test_dboxes_predicted, predicted_scores, ground_truth_dboxes)\n",
    "    ap_evaluator.visualize_results(experiment_name=\"first_exp\", is_chart=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== first_exp:\n",
      " recall: tensor([0.5000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000], dtype=torch.float64), \n",
      " precision: tensor([1.0000, 1.0000, 0.6667, 0.5000, 0.4000, 0.3333, 0.2857, 0.2500, 0.2222,\n",
      "        0.2000], dtype=torch.float64), \n",
      " VOC mAP: 1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    check_ap()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_json": true,
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
