{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V2cvA6soQQFV"
   },
   "source": [
    "# MLP with Single Hidden Layer using PyTorch\n",
    "\n",
    "1. Define an MLP with variable number of inputs (num_inputs), outputs (num_outputs), and nodes in hidden layer (num_hidden_layer_nodes).  \n",
    "2. Use ReLU activation for each node \n",
    "3. Use MSE loss\n",
    "4. Use SGD optimizer\n",
    "\n",
    "\n",
    "<img src=\"https://www.learnopencv.com/wp-content/uploads/2020/01/mlp.png\" alt=\"mlp\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "PPyTXeGJQcuY",
    "outputId": "81d37dac-ed1a-4cd6-c51f-0be62d487340"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10581.46484375\n",
      "1 9755.71875\n",
      "2 9161.2998046875\n",
      "3 8637.4951171875\n",
      "4 8135.08154296875\n",
      "5 7634.916015625\n",
      "6 7127.3212890625\n",
      "7 6610.97021484375\n",
      "8 6087.3720703125\n",
      "9 5563.39599609375\n",
      "10 5048.9482421875\n",
      "11 4551.427734375\n",
      "12 4078.5126953125\n",
      "13 3634.7451171875\n",
      "14 3225.676025390625\n",
      "15 2852.951904296875\n",
      "16 2516.31201171875\n",
      "17 2214.470947265625\n",
      "18 1946.5286865234375\n",
      "19 1709.1199951171875\n",
      "20 1499.035400390625\n",
      "21 1313.6361083984375\n",
      "22 1151.3740234375\n",
      "23 1009.0540771484375\n",
      "24 883.8927612304688\n",
      "25 774.8145141601562\n",
      "26 679.2297973632812\n",
      "27 596.13232421875\n",
      "28 524.0948486328125\n",
      "29 462.2839660644531\n",
      "30 411.1573791503906\n",
      "31 372.46417236328125\n",
      "32 353.1471252441406\n",
      "33 370.2545471191406\n",
      "34 465.539794921875\n",
      "35 733.1923828125\n",
      "36 1382.0526123046875\n",
      "37 2739.25732421875\n",
      "38 5025.0751953125\n",
      "39 6997.12939453125\n",
      "40 6101.65625\n",
      "41 2680.3486328125\n",
      "42 893.8970336914062\n",
      "43 422.0373840332031\n",
      "44 282.54412841796875\n",
      "45 214.08810424804688\n",
      "46 170.41282653808594\n",
      "47 139.2351531982422\n",
      "48 115.83944702148438\n",
      "49 97.63912200927734\n",
      "50 83.17464447021484\n",
      "51 71.49728393554688\n",
      "52 61.89995193481445\n",
      "53 53.92588806152344\n",
      "54 47.21702194213867\n",
      "55 41.534393310546875\n",
      "56 36.68568420410156\n",
      "57 32.508480072021484\n",
      "58 28.900846481323242\n",
      "59 25.769697189331055\n",
      "60 23.046764373779297\n",
      "61 20.654979705810547\n",
      "62 18.54671859741211\n",
      "63 16.692882537841797\n",
      "64 15.051017761230469\n",
      "65 13.595919609069824\n",
      "66 12.301749229431152\n",
      "67 11.14464282989502\n",
      "68 10.114248275756836\n",
      "69 9.195928573608398\n",
      "70 8.369324684143066\n",
      "71 7.626868724822998\n",
      "72 6.962237358093262\n",
      "73 6.364145755767822\n",
      "74 5.822470664978027\n",
      "75 5.334048748016357\n",
      "76 4.891622543334961\n",
      "77 4.490553379058838\n",
      "78 4.126652717590332\n",
      "79 3.7967848777770996\n",
      "80 3.4966390132904053\n",
      "81 3.2248737812042236\n",
      "82 2.9762463569641113\n",
      "83 2.7496471405029297\n",
      "84 2.5429277420043945\n",
      "85 2.353240489959717\n",
      "86 2.179252862930298\n",
      "87 2.02005672454834\n",
      "88 1.8741883039474487\n",
      "89 1.740878939628601\n",
      "90 1.6177173852920532\n",
      "91 1.5047214031219482\n",
      "92 1.4005227088928223\n",
      "93 1.304586410522461\n",
      "94 1.215970754623413\n",
      "95 1.1345165967941284\n",
      "96 1.0593113899230957\n",
      "97 0.9895695447921753\n",
      "98 0.9253392815589905\n",
      "99 0.8653256297111511\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Get reproducible results\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Define the model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_inputs, num_hidden_layer_nodes, num_outputs):\n",
    "        # Initialize super class\n",
    "        super().__init__()\n",
    "\n",
    "        # Add hidden layer \n",
    "        self.linear1 = nn.Linear(num_inputs, num_hidden_layer_nodes)\n",
    "\n",
    "        # Add output layer\n",
    "        self.linear2 = nn.Linear(num_hidden_layer_nodes, num_outputs)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through hidden layer with \n",
    "        x = F.relu(self.linear1(x))\n",
    "        \n",
    "        # Foward pass to output layer\n",
    "        return self.linear2(x)\n",
    "\n",
    "# Num data points\n",
    "num_data = 1000\n",
    "\n",
    "# Network parameters\n",
    "num_inputs = 1000\n",
    "num_hidden_layer_nodes = 100\n",
    "num_outputs = 10\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 100 \n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(num_data, num_inputs)\n",
    "y = torch.randn(num_data, num_outputs)\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = MLP(num_inputs, num_hidden_layer_nodes, num_outputs)\n",
    "\n",
    "# Define loss function\n",
    "loss_function = nn.MSELoss(reduction='sum')\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "for t in range(num_epochs):\n",
    "\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = loss_function(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Calculate gradient using backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Update model parameters (weights)\n",
    "    optimizer.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WyjPulcDSJQo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "MLP-without-sequential",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
