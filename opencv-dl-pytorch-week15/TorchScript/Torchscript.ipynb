{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <font color='blue'>TorchScript</font>\n",
    "\n",
    "---\n",
    "<p align=\"center\">\n",
    "<img src=\"https://www.learnopencv.com/wp-content/uploads/2020/09/c3-w15-torchscript.png\" width=\"1000\">\n",
    "</p>\n",
    "<center> <a href=\"https://youtu.be/2awmrMRf0dA?t=397\"> Reference: TorchScript and PyTorch JIT | Deep Dive</a><center>\n",
    "   \n",
    "---\n",
    "    \n",
    "**What is TorchScript, and why do we need it?**\n",
    "\n",
    "TorchScript is a way to create serializable and optimizable models from PyTorch code. Any TorchScript program can be saved from a Python process and loaded in a process where there is no Python dependency. For example, a model trained using PyTorch can be serialized and saved using TorchScript, and that serialized models can be loaded and used in the `C++` code. \n",
    "\n",
    "It makes it possible to train a model in PyTorch in an eager model, then exports the model to the production environment, where the python program may be disadvantageous for performance and multi-threading reasons. \n",
    "\n",
    "**We need TorchScript for the portability and performance of a PyTorch model:**\n",
    "\n",
    "**Portability:** Models should be exportable to a wide verity of different environments, such as `C++`, mobile, embedded systems, etc. However, a tight couple with a  python environment makes it difficult. \n",
    "\n",
    "**Performance:** Common patterns in the neural network can be optimized further to improve inference latency and throughput. However, numerous other optimization techniques can not be applied due to the level of python's dynamism. \n",
    "\n",
    "\n",
    "**TorchScript does not just make the PyTorch model portable but also optimize it. How does TorchScript optimize the model?**\n",
    "\n",
    "We know that the compiler optimizes the codes by converting codes to machine-friendly language (mostly bytecode) that speed up the execution. However, python is an interpreter language that executes code directly that leads to a compromise in execution speed. The TorchScript usage a concept called [JIT](http://aboullaite.me/understanding-jit-compiler-just-in-time-compiler/#:~:text=The%20JIT%20compiler%20is%20enabled,directly%20instead%20of%20interpreting%20it.) (Just-In-Time) compiler that runs with the interpreter and optimizes specific regions of the code, which are called hot components (loops, function calls) and converts into machine code. While executing the code with the interpreter, the machine-code generated by JIT-compiler will be executed instead of the interpreter output that makes execution fast. \n",
    "\n",
    "---\n",
    "\n",
    "**Torchscript develops an intermediate representation (IR) in the form of a graph similar to [LLVM](https://llvm.org/docs/index.html) and uses the JIT compiler to optimize this intermediate representation.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an example of intermediate representation (IR) for adding two tensors.\n",
    "\n",
    "```\n",
    "graph(%a : Long(),\n",
    "      %b : Long()):\n",
    "  %2 : int = prim::Constant[value=1]()\n",
    "  %3 : Long() = aten::add(%a, %b, %2)\n",
    "  return (%3)\n",
    "```\n",
    "\n",
    "The intermediate representation (**IR**) used in the TorchScript has many components that you can find [here](https://github.com/pytorch/pytorch/blob/53af9df557aff745edf24193ece784fd008c6f19/torch/csrc/jit/OVERVIEW.md). In this notebook, instead of going into theoretical details (not required to use TorchScript), we will see how the TorchScript transforms the PyTorch model to an intermediate graph representation (IR) and save the IR. We will also see how to load the IR model in python. In the coming section, we will also see how we can load IR in `C++`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_gLQc7hnGJQB"
   },
   "source": [
    "## <font color='blue'> 1. Generating Intermediate Representation </font>\n",
    "\n",
    "**There are two ways to get IR from python code:**\n",
    "\n",
    "1. Tracing, and\n",
    "\n",
    "2. Scripting.\n",
    "\n",
    "Let's see both one-by-one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A0E0xBeXHUPT"
   },
   "source": [
    "###   <font color='green'>1.1. Tracing</font>\n",
    "\n",
    "The tracer runs the function with the input tensor and records the tensor operations performed and turns that into a torch script module. **It won't preserve the control flow and other language features like data-structures**.\n",
    "\n",
    "Torchscript provides a simple function `torch.jit.trace()` that takes the target function and the input tensor and returns the TorchScript formate (IR representation). The resultant TorchScript code is dependent on the input tensor shape. For example, if we apply a tensor of **batch size** `32`, then the TorchScript code only accepts tensor of **batch size** `32` to get the same answer obtained with PyTorch code. Thus we should be clear about the tensor shape before applying `torch.jit.trace()`.\n",
    "\n",
    "\n",
    "**Let's take an example of a simple neural network and transform it into IR using `torch.jit.trace()`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ws6kVZDVtkgn"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Converting a single layer neural network using torch.jit.trace\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# get device\n",
    "device = 'cpu' if not torch.cuda.is_available() else 'cuda'\n",
    "\n",
    "# a simple NN\n",
    "class NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NN, self).__init__()\n",
    "        self.layer1 = nn.Linear(in_features=4, \n",
    "                                out_features=2)\n",
    "    def forward(self,x):\n",
    "        x = self.layer1(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "# Transform the model to device\n",
    "net = NN().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lOMd5ASDOrve"
   },
   "source": [
    "**Note:** \n",
    "\n",
    "1. Various optimization is only supported on the CUDA device, so it is better to have it.\n",
    "\n",
    "2. If we are transferring the PyTorch model to IR using TorchScript only for inference purposes, we must freeze (`requires_grad=False`) the model. It will make inference fast because it will not store gradients. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "L8N7TIvbtoQK",
    "outputId": "22d0a80f-1914-462f-957f-6620dd02852f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%self.1 : __torch__.NN,\n",
      "      %input : Float(1:4, 4:1)):\n",
      "  %14 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"layer1\"](%self.1)\n",
      "  %16 : Tensor = prim::CallMethod[name=\"forward\"](%14, %input)\n",
      "  return (%16)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# freeze the model\n",
    "for parm in net.parameters():\n",
    "    parm.requires_grad=False\n",
    "\n",
    "\n",
    "# create a random input, a input is mandatory for IR.\n",
    "x = torch.randn(1,4, device=device)\n",
    "\n",
    "# Use torch script to get IR\n",
    "trace_nn = torch.jit.trace(net, x)\n",
    "\n",
    "print(trace_nn.graph) # To visualize the IR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's call `trace_nn`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3251,  0.0045]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace_nn(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j-0sx5LUP5Fb"
   },
   "source": [
    "**Let's take another example to show that `torch.jit.trace()` does not create a perfect IR when there is a branch (`control statements`, `for loop`, etc.).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "colab_type": "code",
    "id": "iaS62kWotxe3",
    "outputId": "3238ad69-9d46-4d43-a315-2b3d09690a94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%x : Float(*, *)):\n",
      "  %4 : int = prim::Constant[value=3]()\n",
      "  %1 : int = prim::Constant[value=1]() # <ipython-input-4-33c58bd45e7c>:3:0\n",
      "  %5 : Float(*, *) = aten::add(%x, %4, %1)\n",
      "  return (%5)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prakash/anaconda3/envs/pl/lib/python3.7/site-packages/ipykernel_launcher.py:2: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "def fun(x):\n",
    "    if x.size(0) == 1:\n",
    "        return x+3\n",
    "    else:\n",
    "        return x + 20\n",
    "\n",
    "\n",
    "# here, x.size(0) = 1, so this input satisfies if-condition\n",
    "x = torch.randn(1,4).to(device)\n",
    "\n",
    "trace_fun = torch.jit.trace(fun,x)\n",
    "\n",
    "print(trace_fun.graph_for(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the graph did not record the `if-else` condition; it just registered the tensor operations for the satisfied condition (for the given tensor `if` condition satisfied).\n",
    "\n",
    "**We can also see the warning!!!**\n",
    "\n",
    "**Now, let's choose another input such that it satisfies the `else`-condition.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%x : Float(*, *)):\n",
      "  %4 : int = prim::Constant[value=20]()\n",
      "  %1 : int = prim::Constant[value=1]() # <ipython-input-4-33c58bd45e7c>:5:0\n",
      "  %5 : Float(*, *) = aten::add(%x, %4, %1)\n",
      "  return (%5)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prakash/anaconda3/envs/pl/lib/python3.7/site-packages/ipykernel_launcher.py:2: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# here, x.size(0) = 2, so this input does not satisfy if-condition, so else-condition will be executed. \n",
    "x = torch.randn(2,4).to(device)\n",
    "\n",
    "trace_fun = torch.jit.trace(fun, x)\n",
    "\n",
    "print(trace_fun.graph_for(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As you see, the value for `%4` got changed per the condition being met.**\n",
    "\n",
    "Let's be more confident by calling the `trace_fun` with an input of dimension `(1, 4)` (`size(0) = 1`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[21., 21., 21., 21.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x.size(0) = 1\n",
    "x = torch.ones(1, 4).to(device)\n",
    "\n",
    "trace_fun(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can see that even the first dimension of the input is one, the output is the result of the else condition. So the trace is failing to register the control-flow.**\n",
    "\n",
    "\n",
    "**How can it be fixed? `Scripting` is the answer to it.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fna_AZFfHXO7"
   },
   "source": [
    "###  <font color='green'>1.2. Scripting</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J_8os7D9QcAW"
   },
   "source": [
    "Scripting is another way of converting code from eager mode to script mode. It preserves the control-flow by lexing, parsing the whole target python code.   Applying `torch.jit.script()` or `@torch.jit.script (`decorator`) on a deep learning model, by default, it scripts the forward function and recursively scripts the submodules and function which are called inside the forward function.\n",
    "\n",
    "**Let's see it by an example,**\n",
    "\n",
    "Let's take the above function `fun` and check how it is preserving the conditional branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "TKaQfXZht1wo",
    "outputId": "704c7674-80b7-4496-fb05-448b50b2d11c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%x.1 : Tensor):\n",
      "  %2 : int = prim::Constant[value=0]() # <ipython-input-4-33c58bd45e7c>:2:14\n",
      "  %4 : int = prim::Constant[value=1]() # <ipython-input-4-33c58bd45e7c>:2:20\n",
      "  %7 : int = prim::Constant[value=3]() # <ipython-input-4-33c58bd45e7c>:3:17\n",
      "  %11 : int = prim::Constant[value=20]() # <ipython-input-4-33c58bd45e7c>:5:19\n",
      "  %3 : int = aten::size(%x.1, %2) # <ipython-input-4-33c58bd45e7c>:2:7\n",
      "  %5 : bool = aten::eq(%3, %4) # <ipython-input-4-33c58bd45e7c>:2:7\n",
      "  %22 : Tensor = prim::If(%5) # <ipython-input-4-33c58bd45e7c>:2:4\n",
      "    block0():\n",
      "      %9 : Tensor = aten::add(%x.1, %7, %4) # <ipython-input-4-33c58bd45e7c>:3:15\n",
      "      -> (%9)\n",
      "    block1():\n",
      "      %13 : Tensor = aten::add(%x.1, %11, %4) # <ipython-input-4-33c58bd45e7c>:5:15\n",
      "      -> (%13)\n",
      "  return (%22)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# note that we do not need any input to use script\n",
    "\n",
    "scripted_fun = torch.jit.script(fun)\n",
    "\n",
    "print(scripted_fun.graph) # To visualize the intermediate representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `%3` records the value of `x ie %x.1` at dimension `0 ie %2` by calling `aten::size(%x.1, %2)`.  \n",
    "This `%3` is now compared with `%4`, and the boolean value is stored in `%5` by calling `aten::eq(%3, %4)`.\n",
    "    \n",
    "Notice the expression `%22 : Tensor = prim::If(%5)`.  \n",
    "This basically means that whatever is returned after the `if-else` block, store it in `%22`.  \n",
    "`block0()` evaluates the `if-condition` and `block1()` evaluates the `else-condtion`.\n",
    "\n",
    "**Note:** Do not need input to use the script. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The same flow can be recorded using the decorator `@torch.jit.script` too.**\n",
    "\n",
    "**The following is the same example as shown above.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "_frmOhpUt_uN",
    "outputId": "a7005e88-3901-4345-bcb8-8514cb465caa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%x.1 : Tensor):\n",
      "  %2 : int = prim::Constant[value=0]() # <ipython-input-8-f1a0376eb796>:3:14\n",
      "  %4 : int = prim::Constant[value=1]() # <ipython-input-8-f1a0376eb796>:3:20\n",
      "  %7 : int = prim::Constant[value=3]() # <ipython-input-8-f1a0376eb796>:4:17\n",
      "  %11 : int = prim::Constant[value=20]() # <ipython-input-8-f1a0376eb796>:6:17\n",
      "  %3 : int = aten::size(%x.1, %2) # <ipython-input-8-f1a0376eb796>:3:7\n",
      "  %5 : bool = aten::eq(%3, %4) # <ipython-input-8-f1a0376eb796>:3:7\n",
      "  %22 : Tensor = prim::If(%5) # <ipython-input-8-f1a0376eb796>:3:4\n",
      "    block0():\n",
      "      %9 : Tensor = aten::add(%x.1, %7, %4) # <ipython-input-8-f1a0376eb796>:4:15\n",
      "      -> (%9)\n",
      "    block1():\n",
      "      %13 : Tensor = aten::add(%x.1, %11, %4) # <ipython-input-8-f1a0376eb796>:6:15\n",
      "      -> (%13)\n",
      "  return (%22)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@torch.jit.script\n",
    "def fun(x):\n",
    "    if x.size(0) == 1:\n",
    "        return x+3\n",
    "    else:\n",
    "        return x+20\n",
    "print(fun.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above two blocks, we have seen how the IR graph gets defined for function.  However, we can also have a look at the underlying-function created when we use a `torch.jit.script()` over a function.\n",
    "\n",
    "\n",
    "The code to get the function is again a single line; we need to call `fun.code` instead of `fun.graph`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def fun(x: Tensor) -> Tensor:\n",
      "  if torch.eq(torch.size(x, 0), 1):\n",
      "    _0 = torch.add(x, 3, 1)\n",
      "  else:\n",
      "    _0 = torch.add(x, 20, 1)\n",
      "  return _0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A much better depiction of the function `fun` we defined previously.\n",
    "print(fun.code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bDWjIRtIRbvg"
   },
   "source": [
    "###  <font color='green'>1.3. Scripting a Neural-Network</font>\n",
    " \n",
    "Now let us apply the `torch.jit.script()` to the single-layer neural network (`net`), which we defined previously.\n",
    "\n",
    "\n",
    "Once we script it, we can run the TorchScript code with input to get the information of the Tensors and the Operations underuse, to optimize the performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tiny feed-forward layer and set its paramters to be non-trainable \n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.layer1 = nn.Linear(in_features=4, out_features=2)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.layer1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# transform model to device\n",
    "small_net = Network().to(device)\n",
    "\n",
    "# freeze the model\n",
    "for parm in small_net.parameters():\n",
    "    parm.requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "HWxelcFiuBsN",
    "outputId": "bec4c8c6-5075-4121-ae9d-cd391bb1aa94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%self : __torch__.Network,\n",
      "      %x.1 : Float(*, *)):\n",
      "  %4 : __torch__.torch.nn.modules.linear.___torch_mangle_7.Linear = prim::GetAttr[name=\"layer1\"](%self)\n",
      "  %5 : Float(*, *) = prim::GetAttr[name=\"weight\"](%4)\n",
      "  %6 : Float(*) = prim::GetAttr[name=\"bias\"](%4)\n",
      "  %10 : Float(*, *) = aten::t(%5) # /home/prakash/anaconda3/envs/pl/lib/python3.7/site-packages/torch/nn/functional.py:1674:39\n",
      "  %17 : int = prim::Constant[value=1]()\n",
      "  %18 : Float(*, *) = aten::mm(%x.1, %10) # <string>:3:24\n",
      "  %19 : Float(*, *) = aten::add(%6, %18, %17) # <string>:3:17\n",
      "  return (%19)\n",
      "\n",
      "Output is:\n",
      "  tensor([[-0.5723,  0.0863]])\n"
     ]
    }
   ],
   "source": [
    "scripted_nn = torch.jit.script(small_net)\n",
    "scripted_nn(x)\n",
    "print(scripted_nn.graph_for(x))\n",
    "\n",
    "print(\"Output is:\\n \", scripted_nn(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above cell, `%2` stores the attributes of `layer1`, which is basically the `nn.Linear` layer.    \n",
    "`%4` stores the `weights` and `%5` stores the `bias`.  \n",
    "`%16` stores the tranposed version of `%4 ie the weights` to calcualate `X * W.t` using the `aten::t()` function. \n",
    "\n",
    "`%29` stores the value of `X* W.t()` using the `aten::mm() ie the Matrix Multiplication` and `%30` returns the  addition of the bias to `%29`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='green'>Saving and Loading the Scripted Model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output is  tensor([[-0.5723,  0.0863]])\n"
     ]
    }
   ],
   "source": [
    "# We can also save and load this scripted-model.\n",
    "torch.jit.save(scripted_nn, 'tiny_model.pt')\n",
    "\n",
    "# Now we can load this model\n",
    "reloaded_tiny_model = torch.jit.load('tiny_model.pt')\n",
    "\n",
    "print(\"Output is \", reloaded_tiny_model(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now that we know how to create an IR for a Pytorch-based model, we shall see how we can load the IR in `Libtorch` in the next section.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>References:-</font>\n",
    "\n",
    "1. <a href=\"https://www.youtube.com/watch?v=2awmrMRf0dA&feature=youtu.be\" >Video on Torchscript</a>\n",
    "2. <a href=\"https://github.com/pytorch/pytorch/blob/53af9df557aff745edf24193ece784fd008c6f19/torch/csrc/jit/OVERVIEW.md\" >Pytorch docs on JIT</a> "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Torchscript.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
