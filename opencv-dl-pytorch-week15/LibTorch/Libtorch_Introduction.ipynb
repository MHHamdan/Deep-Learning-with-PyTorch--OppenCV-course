{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'> Introduction to Libtorch</font>\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"https://www.learnopencv.com/wp-content/uploads/2020/09/c3-w15-libtorch.png\" height=\"500\">\n",
    "\n",
    "---\n",
    "\n",
    "We all know that PyTorch is a deep learning framework is written in C++ backend and wrapped in Python frontend.  \n",
    "By this, what we mean is Pytorch functions like `torch.Tensor()`or modules like `torch.nn` implicitly call the underlying C++ code.\n",
    "\n",
    "However, we also have something like a C++ wrapper (frontend) over the underlying C++ code, and we call it LibTorch (a library version).\n",
    "\n",
    "At this point, we need to note that both the Libtorch and PyTorch are C++ and Python APIs, respectively, having a few commonly used functionalities like, \n",
    "\n",
    "- Tensor operations,\n",
    "\n",
    "- Representing data,\n",
    "\n",
    "- Creating neural networks,\n",
    "\n",
    "- Optimization APIs,\n",
    "\n",
    "- Data parallelization,\n",
    "\n",
    "- etc.\n",
    "\n",
    "\n",
    "The only difference is that we write C++ code in-case of the C++ frontend and write Python code in-case of the Python frontend.\n",
    "\n",
    "We also need to note that this C++ frontend largely follows the design of the Python API.  \n",
    "For example, a Pytorch code like `a = torch.randn(1,3,5,7)` would be `torch::Tensor a = torch::randn({1,3,5,7})` in C++.\n",
    "\n",
    "In the following sections, we will learn some of the basic stuff we require to build a C++ training pipeline using LibTorch. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```C++\n",
    "// Including the necessary libraries..!!\n",
    "\n",
    "#include <iostream>\n",
    "#include <torch/torch.h>\n",
    "using namespace torch::indexing;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <font color='green'> 1. Tensor Creation</font>\n",
    "\n",
    "\n",
    "Tensors represent data. And this representation could be a set of random numbers or a set of consecutive numbers from 100 to 200 or a set of constant values as well.  \n",
    "\n",
    "Generally, a Tensor representation follows some sort of a blueprint.  \n",
    "And this blueprint could be something like `torch::function-name(function-specific-options, size, other-options)`.\n",
    "\n",
    "There are some options for a Tensor such as its data-type, layout, and the device it lives in. Such options can be passed by using the namespace `torch::TensorOptions()`.\n",
    "\n",
    "\n",
    "The following are some examples of the general Tensor functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```C++\n",
    "//Create a Tensor of size [9,2,7,1]\n",
    "torch::Tensor a = torch::randn({9,2,7,1});\n",
    "\n",
    "//Create a Tensor with necessary datatype\n",
    "torch::Tensor b = torch::arange(13, 43,  torch::TensorOptions().dtype(torch::kInt32));\n",
    "\n",
    "//Reshape `b` to size-[2,5,3]\n",
    "torch::Tensor c = b.view({2,-1,3}); // or b.view({-1,5,3}) or b.view({2,5,-1})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, we need to operate over two Tensors. And, in this case, the Tensor operators or member functions available in Python frontend are also available in C++.\n",
    "\n",
    "\n",
    "They can be found [here](https://pytorch.org/cppdocs/api/classat_1_1_tensor.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <font color='green'> 2. Tensor Indexing</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color='green'> 2.1 Getting a Tensor</font>\n",
    "\n",
    "\n",
    "Indexing in the C++ works the same way as in Pytorch.\n",
    "However, there are some changes in the code. \n",
    "Following is the table which tells us the equivalent code in Libtorch.\n",
    "\n",
    "We don't have the `[]` operator in C++ frontend, so we need to use the Tensor's `.index()` member-function and then specify the indices.   \n",
    "Note that the indexing modules are under the `torch::indexing` namespace, so it is better to use the `using namespace torch::indexing;` in the beginning to avoid writing lengthy code.\n",
    "\n",
    "\n",
    "We shall consider an example in Pytorch and then convert it to the C++ code.\n",
    "\n",
    "\n",
    "Consider `torch::Tensor A = torch::randn({9,5,7,4})`  . `A` being a random Tensor of size `[9,5,7,4]`\n",
    "\n",
    "1. `A[:, 2:4, :, 1:3]` equivalenty in C++ is `A.index({Slice(), Slice(2,4), Slice(), Slice(1,3)})`.\n",
    "\n",
    "\n",
    "2. `A[[1,3,5], 3:, 2, 3]` equivalently in C++ is `A.index({torch::tensor({1,3,5}), Slice(3,None), 2, 3})`.\n",
    "\n",
    "\n",
    "3. `A[..., 3]` equivalently in C++ is `A.index({\"...\", 3})`\n",
    "\n",
    "\n",
    "It would be a good exercise in guessing the resulting Tensor's size for all the three cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```C++\n",
    "// let A be a random Tensor of size-[9,5,7,4]\n",
    "torch::Tensor A = torch::randn({9,5,7,4});\n",
    "\n",
    "// Then Libtorch equivalent of A[:,2:4,:,1:3] is shown below.\n",
    "torch::Tensor subA1 = A.index({Slice(), Slice(2,4), Slice(), Slice(1,3)}); \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code demonstrated that how we can slice through a Tensor or get a sub-Tensor out of the original one using the `Tensor.index()` method.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color='green'> 2.2 Setting a Tensor</font>\n",
    "\n",
    "**Setting a sub-tensor with a Scalar**\n",
    "\n",
    "This sub-section will learn about setting a sub-tensor with a scalar element or with another tensor.\n",
    "\n",
    "We must be familiar with a code something like this. `A[...,3] = 77`.   \n",
    "The above code sets all the sub-matrix elements resulting from `A[...,3]` to the value `77`.\n",
    "\n",
    "We can do the exact way in Libtorch by using the Tensor's `.index_put()` member-function.\n",
    "\n",
    "Concretely, the Libtorch way is `A.put_index_({\"...\",3}, 77)`.  \n",
    "The first argument would be the sub-matrix, and the next argument will be a scalar value that has to be updated to the sub-matrix. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```C++\n",
    "//Setting a subtensor to a scalar value\n",
    "A.index_put_({\"...\",3}, 77);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting a sub-tensor with another tensor or a sub-tensor**\n",
    "\n",
    "Sometimes, we also need to set a Tensor rather than just a Scalar. \n",
    "In Pytorch, if we had `X1 = torch.randn({4,5,6,7})` and `Y1 = torch.Tensor({3,2,2})`, and if we want to set particular submatrix of `X1` with `Y1`, we'd do something lke this.  \n",
    "`X1[1:, 2:4, 0, 2:4] = Y1`.\n",
    "\n",
    "The Libtorch way is shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```C++\n",
    "//setting X1's subtensor of size [3,2,2] with Y1's elements.\n",
    "torch::Tensor X1 = torch::randn({4,5,6,7});\n",
    "torch::Tensor Y1 = torch::randn({3,2,2});\n",
    "\n",
    "//Pytorch equivalent is X1[1:, 2:4, 0, 2:4] = Y1\n",
    "X1.index_put_({Slice(1,None), Slice(2,4), 0, Slice(2,4)}, Y1); \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```C++\n",
    "//setting X2's subtensor of size-[3,2] with Y2's subtensor.\n",
    "torch::Tensor X2 = torch::randn({4,5,6,7});\n",
    "torch::Tensor Y2 = torch::randn({3,2,2});\n",
    "\n",
    "//Pytorch equivalent is X2[1:, 2:4, 0, 2] = Y2[:,:,0]\n",
    "X2.index_put_({Slice(1,None), Slice(2,4), 0, 2}, Y2.index({Slice(),Slice(),0}));\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='green'>3. Creating a Neural Network</font>\n",
    "\n",
    "**In PyTorch, we have two ways to create a neural network:**\n",
    "1. The Modular way ie, creating a subclass inheriting from `torch.nn.Module`.   \n",
    "2. The Sequential way ie, creating a sequential set of layers inside the `torch.nn.Sequential`.  \n",
    "\n",
    "\n",
    "We have the same functionalities available in Libtorch as well, and in the following sub-sections, we'll see how we can create a Neural Network in Libtorch.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color='green'>3.1 The Modular Way</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we dive into the Libtorch code, let's take a step back and see the components required to create a Neural network in Pytorch and then connect it with the Libtorch code.\n",
    "\n",
    "We will consider a simple network with the following layers. `Convolution, Linear, MaxPool` and `BatchNorm`.  \n",
    "\n",
    "Following is the network: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "``` Python\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.mxpool = nn.MaxPool2d(2,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.mxpool(x, 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to note down a few things when we build a neural-network in Pytorch.\n",
    "1. The network needs to inherit the parent `nn.Module` class\n",
    "2. Create a constructor that consists of the layers we need.\n",
    "3. Create a forward function to guide the flow of tensors from input to the output.\n",
    "\n",
    "The Libtorch way is also the same, except for a few extra codes.\n",
    "Let's first write the same neural network in Libtorch and see what the extra code is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```C++\n",
    "struct DemoNetImpl : torch::nn::Module\n",
    "{   \n",
    "\n",
    "    torch::nn::Linear fc1 , fc2 ;\n",
    "    torch::nn::BatchNorm2d bn1 ;\n",
    "    torch::nn::Conv2d conv1, conv2 ;\n",
    "    torch::nn::MaxPool2d mxpool1 ;\n",
    "\n",
    "    DemoNetImpl(int64_t N):\n",
    "        conv1(torch::nn::Conv2dOptions(1, 32, 3).stride(1)),\n",
    "        conv2(torch::nn::Conv2dOptions(32,64,3).stride(1)),\n",
    "        fc1(torch::nn::Linear(9216, 128)),\n",
    "        fc2(torch::nn::Linear(128, N)),\n",
    "        bn1(torch::nn::BatchNorm2d(32)),\n",
    "        mxpool1(torch::nn::MaxPool2dOptions(2).stride(2))\n",
    "    {\n",
    "        register_module(\"ConvLayer1\", conv1);\n",
    "        register_module(\"ConvLayer2\", conv2);\n",
    "        register_module(\"DenseLayer1\", fc1);\n",
    "        register_module(\"DenseLayer2\", fc2);\n",
    "        register_module(\"MaxPoolLayer\", mxpool1);\n",
    "        register_module(\"BatchnormLayer\", bn1);\n",
    "\n",
    "    }\n",
    "\n",
    "    torch::Tensor forward(torch::Tensor x)\n",
    "    {\n",
    "        x = conv1(x);\n",
    "        x = bn1(x);\n",
    "        x = conv2(x);\n",
    "        x = mxpool1(x);\n",
    "        x = torch::flatten(x, 1);\n",
    "        x = fc1(x);\n",
    "        x = fc2(x);\n",
    "        return x;\n",
    "    }\n",
    "\n",
    "}; \n",
    "TORCH_MODULE(DemoNet);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In line 1, we create the `DemoNet` struct inheriting the `torch::nn::Module` class.  \n",
    "From line-4 to line-7, we declare the necessary layers. This is just like `self.conv1`; however, we declare it with the data-type it belongs to, such as `torch::nn::Conv2d conv1`.   \n",
    "\n",
    "At this point, `conv1` is just a variable of type `Conv2d`; it has no information `(or options)` about the kernel-size or input-output channels.   \n",
    "We assign such information in the constructor.    \n",
    "From line-9 to line-15, create a constructor just like `self.__init__()` and assign the necessary attributes to those members.   \n",
    "For example, `conv1(torch::nn::Conv2dOptions(1, 32, 3).stride(1))`.\n",
    "\n",
    "We also see some unfamiliar stuff from line-17 to line-22. Well, the syntax reveals that we are REGISTERING THE MODULES.!  \n",
    "What does it mean to register the modules? \n",
    "\n",
    "It just means that the weights for each module can be accessed and can be updated. That's it.  \n",
    "If we call `DemoNet->parameters()`, we will be able to access the weights `(or learnable parameters)` for those modules.  \n",
    "So, if we don't wrap a module under `register_modules()`, we simply won't access the respective modules' weights.  \n",
    "Simple.!  \n",
    "\n",
    "\n",
    "Once the constructor is defined, we have the `forward()` method, which handles Tensors' flow just like the Python version.\n",
    "\n",
    "\n",
    "But there is also this `Impl` appended when we create the class. It's just an alias for a Shared-Pointer. We need this because the base-module needs to access the `DemoNet` class and the `torch::nn::Module` class.\n",
    "\n",
    "Once we have created a class and appended the `Impl` to it, we need to pass this reference to `TORCH_MODULE`, shown in the last line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```C++\n",
    "// After the `DemoNet` class is created, we can create its object and use it for a forward pass.\n",
    "\n",
    "// We create the output number of logits \n",
    "int64_t N = 10;\n",
    "//instantite the model object\n",
    "DemoNet net(N); //equivalent to `net = DemoNet(N)` in Pytorch\n",
    "\n",
    "//create a random input and forward it.\n",
    "torch::Tensor x = torch::randn({1,1,28,28});\n",
    "torch::Tensor y = net->forward(x); // equivalent to `y = net(x)` in Pytorch\n",
    "\n",
    "std::cout<<\"Output shape from simple-CNN  is \"<<y.sizes()<<std::endl;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='green'>3.2 The Sequential Way</font>\n",
    "\n",
    "`nn.Sequential()` in PyTorch is meant for wrapping a series of operation; similarly, we can use `torch::nn::Sequential` in LibTorch.\n",
    "\n",
    "The following is a simple example.\n",
    "\n",
    "``` Python\n",
    "## The Pytorch way.!\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "mod = nn.Sequential(nn.Conv2d(1, 32, 3, 1), \n",
    "                   nn.BatchNorm2d(32),\n",
    "                   nn.Conv2d(32, 64, 3, 1),\n",
    "                   nn.MaxPool2d(2,2))\n",
    "\n",
    "logits = mod(torch.randn(3,1,28,28)) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```C++\n",
    "// The Libtorch way is \n",
    "torch::nn::Sequential simpleSequence(\n",
    "    torch::nn::Conv2d(torch::nn::Conv2dOptions(1, 32, 3).stride(1)),\n",
    "    torch::nn::BatchNorm2d(32),  \n",
    "    torch::nn::Conv2d(torch::nn::Conv2dOptions(32, 64, 3).stride(1)),\n",
    "    torch::nn::MaxPool2d(torch::nn::MaxPool2dOptions(2).stride(2))\n",
    "    );\n",
    "            \n",
    "// Calling the forward method on the Sequential object\n",
    "torch::Tensor logits = simpleSequence->forward(torch::randn({3,1,28,28}));\n",
    "std::cout<<\"Output from sequential model is of size- \"<<logits.sizes()<<std::endl;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All the code shown above has been written in a C++ file. Let's run a few bash commands to execute that code. The following code cells will build and run the code in the  CPP file.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/chetan/projects/piethon/pth_course/c3_w15_dl_pytorch/LibTorch/libtorch-intro\n"
     ]
    }
   ],
   "source": [
    "%cd libtorch-intro/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CMakeLists.txt\tLibtorch_Introduction.cpp  run_libtorch_basics.sh\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/chetan/projects/piethon/pth_course/c3_w15_dl_pytorch/LibTorch/libtorch-intro/build\n"
     ]
    }
   ],
   "source": [
    "%cd build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- The C compiler identification is GNU 7.5.0\n",
      "-- The CXX compiler identification is GNU 7.5.0\n",
      "-- Check for working C compiler: /usr/bin/cc\n",
      "-- Check for working C compiler: /usr/bin/cc -- works\n",
      "-- Detecting C compiler ABI info\n",
      "-- Detecting C compiler ABI info - done\n",
      "-- Detecting C compile features\n",
      "-- Detecting C compile features - done\n",
      "-- Check for working CXX compiler: /usr/bin/c++\n",
      "-- Check for working CXX compiler: /usr/bin/c++ -- works\n",
      "-- Detecting CXX compiler ABI info\n",
      "-- Detecting CXX compiler ABI info - done\n",
      "-- Detecting CXX compile features\n",
      "-- Detecting CXX compile features - done\n",
      "-- Looking for pthread.h\n",
      "-- Looking for pthread.h - found\n",
      "-- Looking for pthread_create\n",
      "-- Looking for pthread_create - not found\n",
      "-- Looking for pthread_create in pthreads\n",
      "-- Looking for pthread_create in pthreads - not found\n",
      "-- Looking for pthread_create in pthread\n",
      "-- Looking for pthread_create in pthread - found\n",
      "-- Found Threads: TRUE  \n",
      "-- Found Torch: /home/chetan/projects/piethon/pth_course/c3_w15_dl_pytorch/LibTorch/libtorch/lib/libtorch.so  \n",
      "-- Configuring done\n",
      "-- Generating done\n",
      "-- Build files have been written to: /home/chetan/projects/piethon/pth_course/c3_w15_dl_pytorch/LibTorch/libtorch-intro/build\n"
     ]
    }
   ],
   "source": [
    "! cmake  .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\u001b[1mScanning dependencies of target libtorch-basics\u001b[0m\n",
      "[ 50%] \u001b[32mBuilding CXX object CMakeFiles/libtorch-basics.dir/Libtorch_Introduction.cpp.o\u001b[0m\n",
      "\u001b[01m\u001b[K/home/chetan/projects/piethon/pth_course/c3_w15_dl_pytorch/LibTorch/libtorch-intro/Libtorch_Introduction.cpp:1:9:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K#pragma once in main file\n",
      " #pragma \u001b[01;35m\u001b[Konce\u001b[m\u001b[K\n",
      "         \u001b[01;35m\u001b[K^~~~\u001b[m\u001b[K\n",
      "[100%] \u001b[32m\u001b[1mLinking CXX executable libtorch-basics\u001b[0m\n",
      "[100%] Built target libtorch-basics\n"
     ]
    }
   ],
   "source": [
    "!cmake --build . --config Release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/chetan/projects/piethon/pth_course/c3_w15_dl_pytorch/LibTorch/libtorch-intro\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Tensor c is [2, 5, 3]\r\n",
      "Shape of Tensor subA1 is [9, 2, 7, 2]\r\n",
      "Residual sum between X1's subarray and Y1 after assignment is 0\r\n",
      "Residual sum between X2's subarray and Y2's subarray after assignment is 0\r\n",
      "Output shape from simple-CNN  is [1, 10]\r\n",
      "Output from sequential model is of size- [3, 64, 12, 12]\r\n"
     ]
    }
   ],
   "source": [
    "!./build/libtorch-basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='green'>References</font>\n",
    "\n",
    "1. https://pytorch.org/cppdocs/index.html\n",
    "2. https://github.com/pytorch/examples/blob/master/mnist/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
