{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Table of contents</font>\n",
    "\n",
    "- [LeNet5 Architecture](#lenet)\n",
    "- [Display the Network](#display)\n",
    "- [Get the Fashion-MNIST Data](#get-data)\n",
    "- [System Configuration](#sys-config)\n",
    "- [Training Configuration](#train-config)\n",
    "- [System Setup](#sys-setup)\n",
    "- [Training](#training)\n",
    "- [Validation](#validation)\n",
    "- [Main function](#main)\n",
    "- [Plot Loss](#plot-loss)\n",
    "- [Miscellaneous](#misc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">Convolutional Neural Network Using Batch Normalization</font>\n",
    "\n",
    "In this notebook, we  add batch norm layers to the LeNet network, and see how it affects network training and convergence.\n",
    "\n",
    "Instead of the MNIST dataset, which overfits easily, we will use the Fashion MNIST dataset.\n",
    "\n",
    "The figure below shows some samples from the Fashion MNIST dataset.\n",
    "\n",
    "<img src=\"https://www.learnopencv.com/wp-content/uploads/2021/01/c3-w3-fashion-mnist-sprite.jpg\" width=\"600\">\n",
    "\n",
    "There are 10 classes. Each training and testing example is assigned to one of the following labels:\n",
    "\n",
    "| Label | Description |\n",
    "| --- | --- |\n",
    "| 0 | T-shirt/top |\n",
    "| 1 | Trouser |\n",
    "| 2 | Pullover |\n",
    "| 3 | Dress |\n",
    "| 4 | Coat |\n",
    "| 5 | Sandal |\n",
    "| 6 | Shirt |\n",
    "| 7 | Sneaker |\n",
    "| 8 | Bag |\n",
    "| 9 | Ankle boot |\n",
    "\n",
    "\n",
    "\n",
    "We want to classify images in this dataset, using the LeNet network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  # one of the best graphics library for python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from typing import Iterable\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">1. LeNet Architecture with BatchNorm</font><a name=\"lenet\"></a>\n",
    "\n",
    "We have already explained the architecture for LeNet in the previous notebook.\n",
    "\n",
    "Here, we create another model called LeNetBN, adding Batch Normalization layers to the 2 convolution blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # convolution layers\n",
    "        self._body = nn.Sequential(\n",
    "            # First convolution Layer\n",
    "            # input size = (32, 32), output size = (28, 28)\n",
    "            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5),\n",
    "            # ReLU activation\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Max pool 2-d\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            \n",
    "            # Second convolution layer\n",
    "            # input size = (14, 14), output size = (10, 10)\n",
    "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            # output size = (5, 5)\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self._head = nn.Sequential(\n",
    "            # First fully connected layer\n",
    "            # in_features = total number of weights in last conv layer = 16 * 5 * 5\n",
    "            nn.Linear(in_features=16 * 5 * 5, out_features=120), \n",
    "            \n",
    "            # ReLU activation\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # second fully connected layer\n",
    "            # in_features = output of last linear layer = 120 \n",
    "            nn.Linear(in_features=120, out_features=84), \n",
    "            \n",
    "            # ReLU activation\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Third fully connected layer which is also output layer\n",
    "            # in_features = output of last linear layer = 84\n",
    "            # and out_features = number of classes = 10 (MNIST data 0-9)\n",
    "            nn.Linear(in_features=84, out_features=10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # apply feature extractor\n",
    "        x = self._body(x)\n",
    "        # flatten the output of conv layers\n",
    "        # dimension should be batch_size * number_of weight_in_last conv_layer\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        # apply classification head\n",
    "        x = self._head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNetBN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # convolution layers\n",
    "        self._body = nn.Sequential(\n",
    "            # First convolution Layer\n",
    "            # input size = (32, 32), output size = (28, 28)\n",
    "            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5),\n",
    "            nn.BatchNorm2d(6),\n",
    "            # ReLU activation\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Max pool 2-d\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            \n",
    "            # Second convolution layer\n",
    "            # input size = (14, 14), output size = (10, 10)\n",
    "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            # output size = (5, 5)\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self._head = nn.Sequential(\n",
    "            # First fully connected layer\n",
    "            # in_features = total number of weight in last conv layer = 16 * 5 * 5\n",
    "            nn.Linear(in_features=16 * 5 * 5, out_features=120), \n",
    "            \n",
    "            # ReLU activation\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # second fully connected layer\n",
    "            # in_features = output of last linear layer = 120 \n",
    "            nn.Linear(in_features=120, out_features=84), \n",
    "            \n",
    "            # ReLU activation\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Third fully connected layer. It is also output layer\n",
    "            # in_features = output of last linear layer = 84\n",
    "            # and out_features = number of classes = 10 (MNIST data 0-9)\n",
    "            nn.Linear(in_features=84, out_features=10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # apply feature extractor\n",
    "        x = self._body(x)\n",
    "        # flatten the output of conv layers\n",
    "        # dimension should be batch_size * number_of weights_in_last conv_layer\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        # apply classification head\n",
    "        x = self._head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">2. Display the Network</font><a name=\"display\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (_body): Sequential(\n",
      "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (_head): Sequential(\n",
      "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=120, out_features=84, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=84, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "LeNetBN(\n",
      "  (_body): Sequential(\n",
      "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (_head): Sequential(\n",
      "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=120, out_features=84, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=84, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "lenet_model = LeNet()\n",
    "print(lenet_model)\n",
    "lenetBN_model = LeNetBN()\n",
    "print(lenetBN_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## <font style=\"color:green\">3. Get Fashion-MNIST Data</font><a name=\"get-data\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(batch_size, data_root='data', num_workers=1):\n",
    "    \n",
    "    train_test_transforms = transforms.Compose([\n",
    "        # Resize to 32X32\n",
    "        transforms.Resize((32, 32)),\n",
    "        # this re-scales image tensor values between 0-1. image_tensor /= 255\n",
    "        transforms.ToTensor(),\n",
    "        # subtract mean (0.2860) and divide by variance (0.3530).\n",
    "        # This mean and variance is calculated on training data (verify for yourself)\n",
    "        transforms.Normalize((0.2860, ), (0.3530, ))\n",
    "    ])\n",
    "    \n",
    "    # train dataloader\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.FashionMNIST(root=data_root, train=True, download=True, transform=train_test_transforms),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    \n",
    "    # test dataloader\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.FashionMNIST(root=data_root, train=False, download=True, transform=train_test_transforms),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">4. System Configuration</font><a name=\"sys-config\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SystemConfiguration:\n",
    "    '''\n",
    "    Describes the common system setting needed for reproducible training\n",
    "    '''\n",
    "    seed: int = 42  # seed number to set the state of all random number generators\n",
    "    cudnn_benchmark_enabled: bool = True  # enable CuDNN benchmark for the sake of performance\n",
    "    cudnn_deterministic: bool = True  # make cudnn deterministic (reproducible training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">5. Training Configuration</font><a name=\"train-config\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingConfiguration:\n",
    "    '''\n",
    "    Describes configuration of the training process\n",
    "    '''\n",
    "    batch_size: int = 32  # amount of data to pass through the network at each forward-backward iteration\n",
    "    epochs_count: int = 20  # number of times the whole dataset will be passed through the network\n",
    "    learning_rate: float = 0.01  # determines the speed of network's weights update\n",
    "    log_interval: int = 100  # how many batches to wait between logging training status\n",
    "    test_interval: int = 1  # how many epochs to wait before another test. Set to 1 to get val loss at each epoch\n",
    "    data_root: str = \"data\"  # folder to save MNIST data (default: data)\n",
    "    num_workers: int = 10  # number of concurrent processes used to prepare data\n",
    "    device: str = 'cuda'  # device to use for training.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">6. System Setup</font><a name=\"sys-setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_system(system_config: SystemConfiguration) -> None:\n",
    "    torch.manual_seed(system_config.seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cudnn_benchmark_enabled = system_config.cudnn_benchmark_enabled\n",
    "        torch.backends.cudnn.deterministic = system_config.cudnn_deterministic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">7. Training</font><a name=\"training\"></a>\n",
    "We are familiar with the training pipeline used in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    train_config: TrainingConfiguration, model: nn.Module, optimizer: torch.optim.Optimizer,\n",
    "    train_loader: torch.utils.data.DataLoader, epoch_idx: int\n",
    ") -> None:\n",
    "    \n",
    "    # change model in training mode\n",
    "    model.train()\n",
    "    \n",
    "    # to get batch loss\n",
    "    batch_loss = np.array([])\n",
    "    \n",
    "    # to get batch accuracy\n",
    "    batch_acc = np.array([])\n",
    "        \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        # clone target\n",
    "        indx_target = target.clone()\n",
    "        # send data to device (its is medatory if GPU has to be used)\n",
    "        data = data.to(train_config.device)\n",
    "        # send target to device\n",
    "        target = target.to(train_config.device)\n",
    "\n",
    "        # reset parameters gradient to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass to the model\n",
    "        output = model(data)\n",
    "        \n",
    "        # cross entropy loss\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        \n",
    "        # find gradients w.r.t training parameters\n",
    "        loss.backward()\n",
    "        # Update parameters using gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_loss = np.append(batch_loss, [loss.item()])\n",
    "        \n",
    "        # get probability score using softmax\n",
    "        prob = F.softmax(output, dim=1)\n",
    "            \n",
    "        # get the index of the max probability\n",
    "        pred = prob.data.max(dim=1)[1]  \n",
    "                        \n",
    "        # correct prediction\n",
    "        correct = pred.cpu().eq(indx_target).sum()\n",
    "            \n",
    "        # accuracy\n",
    "        acc = float(correct) / float(len(data))\n",
    "        \n",
    "        batch_acc = np.append(batch_acc, [acc])\n",
    "\n",
    "        if batch_idx % train_config.log_interval == 0 and batch_idx > 0:              \n",
    "            print(\n",
    "                'Train Epoch: {} [{}/{}] Loss: {:.6f} Acc: {:.4f}'.format(\n",
    "                    epoch_idx, batch_idx * len(data), len(train_loader.dataset), loss.item(), acc\n",
    "                )\n",
    "            )\n",
    "            \n",
    "    epoch_loss = batch_loss.mean()\n",
    "    epoch_acc = batch_acc.mean()\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">8. Validation</font><a name=\"validation\"></a>\n",
    "\n",
    "After every few epochs **`validation`** is called, with the `trained model` and `test_loader` to get validation loss and accuracy.\n",
    "\n",
    "**Note:** We use `model.eval()` to enable evaluation mode of the model. This will stop calculating the running estimate of mean and variance of data. Using instead just the mean and variance computed while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(\n",
    "    train_config: TrainingConfiguration,\n",
    "    model: nn.Module,\n",
    "    test_loader: torch.utils.data.DataLoader,\n",
    ") -> float:\n",
    "    # \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    count_corect_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            indx_target = target.clone()\n",
    "            data = data.to(train_config.device)\n",
    "\n",
    "            target = target.to(train_config.device)\n",
    "\n",
    "            output = model(data)\n",
    "            # add loss for each mini batch\n",
    "            test_loss += F.cross_entropy(output, target).item()\n",
    "\n",
    "            # get probability score using softmax\n",
    "            prob = F.softmax(output, dim=1)\n",
    "\n",
    "            # get the index of the max probability\n",
    "            pred = prob.data.max(dim=1)[1] \n",
    "\n",
    "            # add correct prediction count\n",
    "            count_corect_predictions += pred.cpu().eq(indx_target).sum()\n",
    "\n",
    "        # average over number of mini-batches\n",
    "        test_loss = test_loss / len(test_loader)  \n",
    "\n",
    "        # average over number of dataset\n",
    "        accuracy = 100. * count_corect_predictions / len(test_loader.dataset)\n",
    "\n",
    "        print(\n",
    "            '\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "                test_loss, count_corect_predictions, len(test_loader.dataset), accuracy\n",
    "            )\n",
    "        )\n",
    "    return test_loss, accuracy/100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">9. Main</font><a name=\"main\"></a>\n",
    "\n",
    "\n",
    "Here, we use the configuration parameters defined above and start  training. \n",
    "\n",
    "1. Set up system parameters like CPU/GPU, number of threads etc.\n",
    "1. Load the data using dataloaders.\n",
    "1. Create an instance of the LeNet model.\n",
    "1. Specify optimizer to use.\n",
    "1. Set up variables to track loss and accuracy and start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model, system_configuration=SystemConfiguration(), training_configuration=TrainingConfiguration()):\n",
    "    \n",
    "    # system configuration\n",
    "    setup_system(system_configuration)\n",
    "\n",
    "    # batch size\n",
    "    batch_size_to_set = training_configuration.batch_size\n",
    "    # num_workers\n",
    "    num_workers_to_set = training_configuration.num_workers\n",
    "    # epochs\n",
    "    epoch_num_to_set = training_configuration.epochs_count\n",
    "\n",
    "    # if GPU is available use training config, \n",
    "    # else lower batch_size, num_workers and epochs count\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "        batch_size_to_set = 16\n",
    "        num_workers_to_set = 2\n",
    "        epoch_num_to_set = 10\n",
    "\n",
    "    # data loader\n",
    "    train_loader, test_loader = get_data(\n",
    "        batch_size=batch_size_to_set,\n",
    "        data_root=training_configuration.data_root,\n",
    "        num_workers=num_workers_to_set\n",
    "    )\n",
    "    \n",
    "    # Update training configuration\n",
    "    training_configuration = TrainingConfiguration(\n",
    "        device=device,\n",
    "        epochs_count=epoch_num_to_set,\n",
    "        batch_size=batch_size_to_set,\n",
    "        num_workers=num_workers_to_set\n",
    "    )\n",
    "        \n",
    "    # send model to device (GPU/CPU)\n",
    "    model.to(training_configuration.device)\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=training_configuration.learning_rate\n",
    "    )\n",
    "\n",
    "    best_loss = torch.tensor(np.inf)\n",
    "    \n",
    "    # epoch train/test loss\n",
    "    epoch_train_loss = np.array([])\n",
    "    epoch_test_loss = np.array([])\n",
    "    \n",
    "    # epch train/test accuracy\n",
    "    epoch_train_acc = np.array([])\n",
    "    epoch_test_acc = np.array([])\n",
    "    \n",
    "    # trainig time measurement\n",
    "    t_begin = time.time()\n",
    "    for epoch in range(training_configuration.epochs_count):\n",
    "        \n",
    "        train_loss, train_acc = train(training_configuration, model, optimizer, train_loader, epoch)\n",
    "        \n",
    "        epoch_train_loss = np.append(epoch_train_loss, [train_loss])\n",
    "        \n",
    "        epoch_train_acc = np.append(epoch_train_acc, [train_acc])\n",
    "\n",
    "        elapsed_time = time.time() - t_begin\n",
    "        speed_epoch = elapsed_time / (epoch + 1)\n",
    "        speed_batch = speed_epoch / len(train_loader)\n",
    "        eta = speed_epoch * training_configuration.epochs_count - elapsed_time\n",
    "        \n",
    "        print(\n",
    "            \"Elapsed {:.2f}s, {:.2f} s/epoch, {:.2f} s/batch, ets {:.2f}s\".format(\n",
    "                elapsed_time, speed_epoch, speed_batch, eta\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if epoch % training_configuration.test_interval == 0:\n",
    "            current_loss, current_accuracy = validate(training_configuration, model, test_loader)\n",
    "            \n",
    "            epoch_test_loss = np.append(epoch_test_loss, [current_loss])\n",
    "        \n",
    "            epoch_test_acc = np.append(epoch_test_acc, [current_accuracy])\n",
    "            \n",
    "            if current_loss < best_loss:\n",
    "                best_loss = current_loss\n",
    "                \n",
    "    print(\"Total time: {:.2f}, Best Loss: {:.3f}\".format(time.time() - t_begin, best_loss))\n",
    "    \n",
    "    return model, epoch_train_loss, epoch_train_acc, epoch_test_loss, epoch_test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26427392it [00:08, 3172115.21it/s]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32768it [00:00, 43757.35it/s]                           \n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4423680it [00:01, 2839659.97it/s]                             \n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8192it [00:00, 26669.86it/s]            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "Processing...\n",
      "Done!\n",
      "Train Epoch: 0 [3200/60000] Loss: 2.305011 Acc: 0.1250\n",
      "Train Epoch: 0 [6400/60000] Loss: 2.261773 Acc: 0.2500\n",
      "Train Epoch: 0 [9600/60000] Loss: 2.048268 Acc: 0.4062\n",
      "Train Epoch: 0 [12800/60000] Loss: 1.379581 Acc: 0.3750\n",
      "Train Epoch: 0 [16000/60000] Loss: 0.889184 Acc: 0.6562\n",
      "Train Epoch: 0 [19200/60000] Loss: 0.932409 Acc: 0.6250\n",
      "Train Epoch: 0 [22400/60000] Loss: 1.240133 Acc: 0.5312\n",
      "Train Epoch: 0 [25600/60000] Loss: 0.985355 Acc: 0.7500\n",
      "Train Epoch: 0 [28800/60000] Loss: 1.010952 Acc: 0.6875\n",
      "Train Epoch: 0 [32000/60000] Loss: 1.049646 Acc: 0.5312\n",
      "Train Epoch: 0 [35200/60000] Loss: 0.838904 Acc: 0.7188\n",
      "Train Epoch: 0 [38400/60000] Loss: 0.890075 Acc: 0.6875\n",
      "Train Epoch: 0 [41600/60000] Loss: 0.589139 Acc: 0.8438\n",
      "Train Epoch: 0 [44800/60000] Loss: 0.887304 Acc: 0.6562\n",
      "Train Epoch: 0 [48000/60000] Loss: 0.827041 Acc: 0.6875\n",
      "Train Epoch: 0 [51200/60000] Loss: 0.683016 Acc: 0.7500\n",
      "Train Epoch: 0 [54400/60000] Loss: 0.774134 Acc: 0.7188\n",
      "Train Epoch: 0 [57600/60000] Loss: 0.607422 Acc: 0.8125\n",
      "Elapsed 5.22s, 5.22 s/epoch, 0.00 s/batch, ets 99.20s\n",
      "\n",
      "Test set: Average loss: 0.6541, Accuracy: 7385/10000 (74%)\n",
      "\n",
      "Train Epoch: 1 [3200/60000] Loss: 0.507876 Acc: 0.8125\n",
      "Train Epoch: 1 [6400/60000] Loss: 0.860376 Acc: 0.7188\n",
      "Train Epoch: 1 [9600/60000] Loss: 0.726497 Acc: 0.6562\n",
      "Train Epoch: 1 [12800/60000] Loss: 0.364640 Acc: 0.8750\n",
      "Train Epoch: 1 [16000/60000] Loss: 0.415865 Acc: 0.8438\n",
      "Train Epoch: 1 [19200/60000] Loss: 0.310015 Acc: 0.9375\n",
      "Train Epoch: 1 [22400/60000] Loss: 0.647808 Acc: 0.7812\n",
      "Train Epoch: 1 [25600/60000] Loss: 0.585588 Acc: 0.8125\n",
      "Train Epoch: 1 [28800/60000] Loss: 0.539251 Acc: 0.8125\n",
      "Train Epoch: 1 [32000/60000] Loss: 1.057292 Acc: 0.5625\n",
      "Train Epoch: 1 [35200/60000] Loss: 0.895846 Acc: 0.5938\n",
      "Train Epoch: 1 [38400/60000] Loss: 0.600427 Acc: 0.7812\n",
      "Train Epoch: 1 [41600/60000] Loss: 0.318247 Acc: 0.8750\n",
      "Train Epoch: 1 [44800/60000] Loss: 0.711447 Acc: 0.6875\n",
      "Train Epoch: 1 [48000/60000] Loss: 0.472278 Acc: 0.8125\n",
      "Train Epoch: 1 [51200/60000] Loss: 0.335828 Acc: 0.8438\n",
      "Train Epoch: 1 [54400/60000] Loss: 0.305687 Acc: 0.9062\n",
      "Train Epoch: 1 [57600/60000] Loss: 0.433218 Acc: 0.8750\n",
      "Elapsed 11.10s, 5.55 s/epoch, 0.00 s/batch, ets 99.92s\n",
      "\n",
      "Test set: Average loss: 0.5096, Accuracy: 8135/10000 (81%)\n",
      "\n",
      "Train Epoch: 2 [3200/60000] Loss: 0.294234 Acc: 0.9062\n",
      "Train Epoch: 2 [6400/60000] Loss: 0.380558 Acc: 0.8438\n",
      "Train Epoch: 2 [9600/60000] Loss: 0.422810 Acc: 0.8438\n",
      "Train Epoch: 2 [12800/60000] Loss: 0.572978 Acc: 0.8125\n",
      "Train Epoch: 2 [16000/60000] Loss: 0.749128 Acc: 0.8125\n",
      "Train Epoch: 2 [19200/60000] Loss: 0.244775 Acc: 0.9062\n",
      "Train Epoch: 2 [22400/60000] Loss: 0.599652 Acc: 0.7500\n",
      "Train Epoch: 2 [25600/60000] Loss: 0.415467 Acc: 0.7812\n",
      "Train Epoch: 2 [28800/60000] Loss: 0.728330 Acc: 0.7188\n",
      "Train Epoch: 2 [32000/60000] Loss: 0.392753 Acc: 0.8438\n",
      "Train Epoch: 2 [35200/60000] Loss: 0.451952 Acc: 0.7812\n",
      "Train Epoch: 2 [38400/60000] Loss: 0.400922 Acc: 0.8125\n",
      "Train Epoch: 2 [41600/60000] Loss: 0.393893 Acc: 0.8750\n",
      "Train Epoch: 2 [44800/60000] Loss: 0.594798 Acc: 0.8125\n",
      "Train Epoch: 2 [48000/60000] Loss: 0.540503 Acc: 0.6875\n",
      "Train Epoch: 2 [51200/60000] Loss: 0.536200 Acc: 0.7500\n",
      "Train Epoch: 2 [54400/60000] Loss: 0.504963 Acc: 0.7812\n",
      "Train Epoch: 2 [57600/60000] Loss: 0.348504 Acc: 0.8438\n",
      "Elapsed 17.19s, 5.73 s/epoch, 0.00 s/batch, ets 97.42s\n",
      "\n",
      "Test set: Average loss: 0.4529, Accuracy: 8369/10000 (84%)\n",
      "\n",
      "Train Epoch: 3 [3200/60000] Loss: 0.440783 Acc: 0.7812\n",
      "Train Epoch: 3 [6400/60000] Loss: 0.344257 Acc: 0.9062\n",
      "Train Epoch: 3 [9600/60000] Loss: 0.546225 Acc: 0.7812\n",
      "Train Epoch: 3 [12800/60000] Loss: 0.340383 Acc: 0.8750\n",
      "Train Epoch: 3 [16000/60000] Loss: 0.219590 Acc: 0.9375\n",
      "Train Epoch: 3 [19200/60000] Loss: 0.223041 Acc: 0.9375\n",
      "Train Epoch: 3 [22400/60000] Loss: 0.418114 Acc: 0.8438\n",
      "Train Epoch: 3 [25600/60000] Loss: 0.427179 Acc: 0.8750\n",
      "Train Epoch: 3 [28800/60000] Loss: 0.525931 Acc: 0.7812\n",
      "Train Epoch: 3 [32000/60000] Loss: 0.451278 Acc: 0.8438\n",
      "Train Epoch: 3 [35200/60000] Loss: 0.583596 Acc: 0.7812\n",
      "Train Epoch: 3 [38400/60000] Loss: 0.643156 Acc: 0.7188\n",
      "Train Epoch: 3 [41600/60000] Loss: 0.480610 Acc: 0.8438\n",
      "Train Epoch: 3 [44800/60000] Loss: 0.390852 Acc: 0.8750\n",
      "Train Epoch: 3 [48000/60000] Loss: 0.535072 Acc: 0.8125\n",
      "Train Epoch: 3 [51200/60000] Loss: 0.184421 Acc: 0.9688\n",
      "Train Epoch: 3 [54400/60000] Loss: 0.549882 Acc: 0.8438\n",
      "Train Epoch: 3 [57600/60000] Loss: 0.419242 Acc: 0.8125\n",
      "Elapsed 23.52s, 5.88 s/epoch, 0.00 s/batch, ets 94.08s\n",
      "\n",
      "Test set: Average loss: 0.4088, Accuracy: 8533/10000 (85%)\n",
      "\n",
      "Train Epoch: 4 [3200/60000] Loss: 0.218270 Acc: 0.9062\n",
      "Train Epoch: 4 [6400/60000] Loss: 0.182843 Acc: 0.9062\n",
      "Train Epoch: 4 [9600/60000] Loss: 0.464214 Acc: 0.8125\n",
      "Train Epoch: 4 [12800/60000] Loss: 0.394803 Acc: 0.8438\n",
      "Train Epoch: 4 [16000/60000] Loss: 0.137618 Acc: 0.9688\n",
      "Train Epoch: 4 [19200/60000] Loss: 0.280210 Acc: 0.9062\n",
      "Train Epoch: 4 [22400/60000] Loss: 0.268325 Acc: 0.8750\n",
      "Train Epoch: 4 [25600/60000] Loss: 0.282860 Acc: 0.8750\n",
      "Train Epoch: 4 [28800/60000] Loss: 0.225083 Acc: 0.9062\n",
      "Train Epoch: 4 [32000/60000] Loss: 0.346966 Acc: 0.8750\n",
      "Train Epoch: 4 [35200/60000] Loss: 0.258856 Acc: 0.9375\n",
      "Train Epoch: 4 [38400/60000] Loss: 0.462814 Acc: 0.7812\n",
      "Train Epoch: 4 [41600/60000] Loss: 0.189067 Acc: 0.9375\n",
      "Train Epoch: 4 [44800/60000] Loss: 0.338372 Acc: 0.8750\n",
      "Train Epoch: 4 [48000/60000] Loss: 0.490463 Acc: 0.8438\n",
      "Train Epoch: 4 [51200/60000] Loss: 0.398166 Acc: 0.8438\n",
      "Train Epoch: 4 [54400/60000] Loss: 0.648591 Acc: 0.8125\n",
      "Train Epoch: 4 [57600/60000] Loss: 0.172724 Acc: 0.9688\n",
      "Elapsed 29.81s, 5.96 s/epoch, 0.00 s/batch, ets 89.44s\n",
      "\n",
      "Test set: Average loss: 0.3929, Accuracy: 8541/10000 (85%)\n",
      "\n",
      "Train Epoch: 5 [3200/60000] Loss: 0.201302 Acc: 0.9062\n",
      "Train Epoch: 5 [6400/60000] Loss: 0.412281 Acc: 0.8125\n",
      "Train Epoch: 5 [9600/60000] Loss: 0.235883 Acc: 0.9688\n",
      "Train Epoch: 5 [12800/60000] Loss: 0.578542 Acc: 0.8438\n",
      "Train Epoch: 5 [16000/60000] Loss: 0.184850 Acc: 0.9688\n",
      "Train Epoch: 5 [19200/60000] Loss: 0.290150 Acc: 0.8438\n",
      "Train Epoch: 5 [22400/60000] Loss: 0.380279 Acc: 0.8438\n",
      "Train Epoch: 5 [25600/60000] Loss: 0.326894 Acc: 0.8438\n",
      "Train Epoch: 5 [28800/60000] Loss: 0.273323 Acc: 0.9375\n",
      "Train Epoch: 5 [32000/60000] Loss: 0.258952 Acc: 0.8438\n",
      "Train Epoch: 5 [35200/60000] Loss: 0.350917 Acc: 0.8750\n",
      "Train Epoch: 5 [38400/60000] Loss: 0.333127 Acc: 0.9375\n",
      "Train Epoch: 5 [41600/60000] Loss: 0.393675 Acc: 0.8750\n",
      "Train Epoch: 5 [44800/60000] Loss: 0.404224 Acc: 0.8750\n",
      "Train Epoch: 5 [48000/60000] Loss: 0.469996 Acc: 0.8438\n",
      "Train Epoch: 5 [51200/60000] Loss: 0.391400 Acc: 0.8750\n",
      "Train Epoch: 5 [54400/60000] Loss: 0.287104 Acc: 0.9375\n",
      "Train Epoch: 5 [57600/60000] Loss: 0.364564 Acc: 0.7812\n",
      "Elapsed 35.91s, 5.99 s/epoch, 0.00 s/batch, ets 83.80s\n",
      "\n",
      "Test set: Average loss: 0.3845, Accuracy: 8624/10000 (86%)\n",
      "\n",
      "Train Epoch: 6 [3200/60000] Loss: 0.239899 Acc: 0.9062\n",
      "Train Epoch: 6 [6400/60000] Loss: 0.135834 Acc: 0.9688\n",
      "Train Epoch: 6 [9600/60000] Loss: 0.340143 Acc: 0.8438\n",
      "Train Epoch: 6 [12800/60000] Loss: 0.270801 Acc: 0.8438\n",
      "Train Epoch: 6 [16000/60000] Loss: 0.187625 Acc: 0.9375\n",
      "Train Epoch: 6 [19200/60000] Loss: 0.197935 Acc: 0.9375\n",
      "Train Epoch: 6 [22400/60000] Loss: 0.425608 Acc: 0.7812\n",
      "Train Epoch: 6 [25600/60000] Loss: 0.220734 Acc: 0.9375\n",
      "Train Epoch: 6 [28800/60000] Loss: 0.153883 Acc: 0.9375\n",
      "Train Epoch: 6 [32000/60000] Loss: 0.405654 Acc: 0.8125\n",
      "Train Epoch: 6 [35200/60000] Loss: 0.317202 Acc: 0.9375\n",
      "Train Epoch: 6 [38400/60000] Loss: 0.340015 Acc: 0.8750\n",
      "Train Epoch: 6 [41600/60000] Loss: 0.298867 Acc: 0.8438\n",
      "Train Epoch: 6 [44800/60000] Loss: 0.338637 Acc: 0.8750\n",
      "Train Epoch: 6 [48000/60000] Loss: 0.125916 Acc: 1.0000\n",
      "Train Epoch: 6 [51200/60000] Loss: 0.168078 Acc: 0.9375\n",
      "Train Epoch: 6 [54400/60000] Loss: 0.552747 Acc: 0.8125\n",
      "Train Epoch: 6 [57600/60000] Loss: 0.371152 Acc: 0.8125\n",
      "Elapsed 42.21s, 6.03 s/epoch, 0.00 s/batch, ets 78.38s\n",
      "\n",
      "Test set: Average loss: 0.3531, Accuracy: 8697/10000 (87%)\n",
      "\n",
      "Train Epoch: 7 [3200/60000] Loss: 0.150909 Acc: 0.9375\n",
      "Train Epoch: 7 [6400/60000] Loss: 0.376598 Acc: 0.9062\n",
      "Train Epoch: 7 [9600/60000] Loss: 0.292821 Acc: 0.9375\n",
      "Train Epoch: 7 [12800/60000] Loss: 0.178694 Acc: 0.9375\n",
      "Train Epoch: 7 [16000/60000] Loss: 0.321889 Acc: 0.8750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [19200/60000] Loss: 0.239725 Acc: 0.9062\n",
      "Train Epoch: 7 [22400/60000] Loss: 0.315298 Acc: 0.9062\n",
      "Train Epoch: 7 [25600/60000] Loss: 0.394747 Acc: 0.7812\n",
      "Train Epoch: 7 [28800/60000] Loss: 0.182193 Acc: 0.9688\n",
      "Train Epoch: 7 [32000/60000] Loss: 0.447015 Acc: 0.8125\n",
      "Train Epoch: 7 [35200/60000] Loss: 0.354310 Acc: 0.8750\n",
      "Train Epoch: 7 [38400/60000] Loss: 0.264993 Acc: 0.8750\n",
      "Train Epoch: 7 [41600/60000] Loss: 0.363598 Acc: 0.8750\n",
      "Train Epoch: 7 [44800/60000] Loss: 0.156804 Acc: 0.9375\n",
      "Train Epoch: 7 [48000/60000] Loss: 0.387383 Acc: 0.8438\n",
      "Train Epoch: 7 [51200/60000] Loss: 0.133595 Acc: 0.9375\n",
      "Train Epoch: 7 [54400/60000] Loss: 0.280993 Acc: 0.9062\n",
      "Train Epoch: 7 [57600/60000] Loss: 0.650849 Acc: 0.7500\n",
      "Elapsed 48.63s, 6.08 s/epoch, 0.00 s/batch, ets 72.95s\n",
      "\n",
      "Test set: Average loss: 0.3489, Accuracy: 8723/10000 (87%)\n",
      "\n",
      "Train Epoch: 8 [3200/60000] Loss: 0.299913 Acc: 0.9062\n",
      "Train Epoch: 8 [6400/60000] Loss: 0.171295 Acc: 0.9375\n",
      "Train Epoch: 8 [9600/60000] Loss: 0.376844 Acc: 0.8125\n",
      "Train Epoch: 8 [12800/60000] Loss: 0.222391 Acc: 0.9062\n",
      "Train Epoch: 8 [16000/60000] Loss: 0.283259 Acc: 0.9062\n",
      "Train Epoch: 8 [19200/60000] Loss: 0.249595 Acc: 0.9062\n",
      "Train Epoch: 8 [22400/60000] Loss: 0.209809 Acc: 0.9375\n",
      "Train Epoch: 8 [25600/60000] Loss: 0.446089 Acc: 0.8438\n",
      "Train Epoch: 8 [28800/60000] Loss: 0.376134 Acc: 0.8750\n",
      "Train Epoch: 8 [32000/60000] Loss: 0.206045 Acc: 0.9688\n",
      "Train Epoch: 8 [35200/60000] Loss: 0.136642 Acc: 0.9688\n",
      "Train Epoch: 8 [38400/60000] Loss: 0.177133 Acc: 0.9375\n",
      "Train Epoch: 8 [41600/60000] Loss: 0.138357 Acc: 0.9375\n",
      "Train Epoch: 8 [44800/60000] Loss: 0.305277 Acc: 0.8438\n",
      "Train Epoch: 8 [48000/60000] Loss: 0.266119 Acc: 0.9062\n",
      "Train Epoch: 8 [51200/60000] Loss: 0.167819 Acc: 0.9375\n",
      "Train Epoch: 8 [54400/60000] Loss: 0.344807 Acc: 0.8438\n",
      "Train Epoch: 8 [57600/60000] Loss: 0.269436 Acc: 0.9375\n",
      "Elapsed 54.87s, 6.10 s/epoch, 0.00 s/batch, ets 67.07s\n",
      "\n",
      "Test set: Average loss: 0.3451, Accuracy: 8738/10000 (87%)\n",
      "\n",
      "Train Epoch: 9 [3200/60000] Loss: 0.430611 Acc: 0.8125\n",
      "Train Epoch: 9 [6400/60000] Loss: 0.362199 Acc: 0.8438\n",
      "Train Epoch: 9 [9600/60000] Loss: 0.232000 Acc: 0.9062\n",
      "Train Epoch: 9 [12800/60000] Loss: 0.362943 Acc: 0.9375\n",
      "Train Epoch: 9 [16000/60000] Loss: 0.199817 Acc: 0.8750\n",
      "Train Epoch: 9 [19200/60000] Loss: 0.078908 Acc: 0.9688\n",
      "Train Epoch: 9 [22400/60000] Loss: 0.328962 Acc: 0.9062\n",
      "Train Epoch: 9 [25600/60000] Loss: 0.258471 Acc: 0.9062\n",
      "Train Epoch: 9 [28800/60000] Loss: 0.467460 Acc: 0.7812\n",
      "Train Epoch: 9 [32000/60000] Loss: 0.164341 Acc: 0.9375\n",
      "Train Epoch: 9 [35200/60000] Loss: 0.221952 Acc: 0.9688\n",
      "Train Epoch: 9 [38400/60000] Loss: 0.139497 Acc: 0.9062\n",
      "Train Epoch: 9 [41600/60000] Loss: 0.147520 Acc: 0.9062\n",
      "Train Epoch: 9 [44800/60000] Loss: 0.208690 Acc: 0.9062\n",
      "Train Epoch: 9 [48000/60000] Loss: 0.069604 Acc: 0.9688\n",
      "Train Epoch: 9 [51200/60000] Loss: 0.414484 Acc: 0.8750\n",
      "Train Epoch: 9 [54400/60000] Loss: 0.592086 Acc: 0.7812\n",
      "Train Epoch: 9 [57600/60000] Loss: 0.328195 Acc: 0.9375\n",
      "Elapsed 61.10s, 6.11 s/epoch, 0.00 s/batch, ets 61.10s\n",
      "\n",
      "Test set: Average loss: 0.3255, Accuracy: 8803/10000 (88%)\n",
      "\n",
      "Train Epoch: 10 [3200/60000] Loss: 0.370652 Acc: 0.9062\n",
      "Train Epoch: 10 [6400/60000] Loss: 0.115738 Acc: 0.9688\n",
      "Train Epoch: 10 [9600/60000] Loss: 0.259986 Acc: 0.9375\n",
      "Train Epoch: 10 [12800/60000] Loss: 0.472924 Acc: 0.8750\n",
      "Train Epoch: 10 [16000/60000] Loss: 0.440769 Acc: 0.8438\n",
      "Train Epoch: 10 [19200/60000] Loss: 0.492116 Acc: 0.8750\n",
      "Train Epoch: 10 [22400/60000] Loss: 0.273179 Acc: 0.9062\n",
      "Train Epoch: 10 [25600/60000] Loss: 0.266618 Acc: 0.9688\n",
      "Train Epoch: 10 [28800/60000] Loss: 0.341131 Acc: 0.8438\n",
      "Train Epoch: 10 [32000/60000] Loss: 0.251892 Acc: 0.9375\n",
      "Train Epoch: 10 [35200/60000] Loss: 0.126834 Acc: 0.9688\n",
      "Train Epoch: 10 [38400/60000] Loss: 0.236841 Acc: 0.9375\n",
      "Train Epoch: 10 [41600/60000] Loss: 0.230330 Acc: 0.9375\n",
      "Train Epoch: 10 [44800/60000] Loss: 0.212955 Acc: 0.9375\n",
      "Train Epoch: 10 [48000/60000] Loss: 0.415350 Acc: 0.7500\n",
      "Train Epoch: 10 [51200/60000] Loss: 0.292367 Acc: 0.8438\n",
      "Train Epoch: 10 [54400/60000] Loss: 0.368079 Acc: 0.8438\n",
      "Train Epoch: 10 [57600/60000] Loss: 0.422772 Acc: 0.8438\n",
      "Elapsed 67.21s, 6.11 s/epoch, 0.00 s/batch, ets 54.99s\n",
      "\n",
      "Test set: Average loss: 0.3244, Accuracy: 8817/10000 (88%)\n",
      "\n",
      "Train Epoch: 11 [3200/60000] Loss: 0.412141 Acc: 0.8438\n",
      "Train Epoch: 11 [6400/60000] Loss: 0.246967 Acc: 0.9062\n",
      "Train Epoch: 11 [9600/60000] Loss: 0.166644 Acc: 0.9375\n",
      "Train Epoch: 11 [12800/60000] Loss: 0.300772 Acc: 0.9062\n",
      "Train Epoch: 11 [16000/60000] Loss: 0.158267 Acc: 0.9062\n",
      "Train Epoch: 11 [19200/60000] Loss: 0.405222 Acc: 0.8438\n",
      "Train Epoch: 11 [22400/60000] Loss: 0.185632 Acc: 0.8750\n",
      "Train Epoch: 11 [25600/60000] Loss: 0.357625 Acc: 0.8750\n",
      "Train Epoch: 11 [28800/60000] Loss: 0.186999 Acc: 0.8750\n",
      "Train Epoch: 11 [32000/60000] Loss: 0.331724 Acc: 0.8750\n",
      "Train Epoch: 11 [35200/60000] Loss: 0.175267 Acc: 0.9688\n",
      "Train Epoch: 11 [38400/60000] Loss: 0.312835 Acc: 0.8750\n",
      "Train Epoch: 11 [41600/60000] Loss: 0.165003 Acc: 0.9375\n",
      "Train Epoch: 11 [44800/60000] Loss: 0.310624 Acc: 0.8750\n",
      "Train Epoch: 11 [48000/60000] Loss: 0.232811 Acc: 0.8438\n",
      "Train Epoch: 11 [51200/60000] Loss: 0.338405 Acc: 0.9375\n",
      "Train Epoch: 11 [54400/60000] Loss: 0.349569 Acc: 0.8125\n",
      "Train Epoch: 11 [57600/60000] Loss: 0.207934 Acc: 0.9062\n",
      "Elapsed 73.36s, 6.11 s/epoch, 0.00 s/batch, ets 48.91s\n",
      "\n",
      "Test set: Average loss: 0.3109, Accuracy: 8856/10000 (89%)\n",
      "\n",
      "Train Epoch: 12 [3200/60000] Loss: 0.208455 Acc: 0.9375\n",
      "Train Epoch: 12 [6400/60000] Loss: 0.125567 Acc: 0.9688\n",
      "Train Epoch: 12 [9600/60000] Loss: 0.263233 Acc: 0.9062\n",
      "Train Epoch: 12 [12800/60000] Loss: 0.057960 Acc: 1.0000\n",
      "Train Epoch: 12 [16000/60000] Loss: 0.309823 Acc: 0.8750\n",
      "Train Epoch: 12 [19200/60000] Loss: 0.230099 Acc: 0.9375\n",
      "Train Epoch: 12 [22400/60000] Loss: 0.221422 Acc: 0.9062\n",
      "Train Epoch: 12 [25600/60000] Loss: 0.216891 Acc: 0.9375\n",
      "Train Epoch: 12 [28800/60000] Loss: 0.328276 Acc: 0.9062\n",
      "Train Epoch: 12 [32000/60000] Loss: 0.269859 Acc: 0.9375\n",
      "Train Epoch: 12 [35200/60000] Loss: 0.186899 Acc: 0.9062\n",
      "Train Epoch: 12 [38400/60000] Loss: 0.362377 Acc: 0.8438\n",
      "Train Epoch: 12 [41600/60000] Loss: 0.181568 Acc: 0.9375\n",
      "Train Epoch: 12 [44800/60000] Loss: 0.239428 Acc: 0.9062\n",
      "Train Epoch: 12 [48000/60000] Loss: 0.099041 Acc: 1.0000\n",
      "Train Epoch: 12 [51200/60000] Loss: 0.329588 Acc: 0.9688\n",
      "Train Epoch: 12 [54400/60000] Loss: 0.306472 Acc: 0.8125\n",
      "Train Epoch: 12 [57600/60000] Loss: 0.505115 Acc: 0.7812\n",
      "Elapsed 79.64s, 6.13 s/epoch, 0.00 s/batch, ets 42.88s\n",
      "\n",
      "Test set: Average loss: 0.3065, Accuracy: 8881/10000 (89%)\n",
      "\n",
      "Train Epoch: 13 [3200/60000] Loss: 0.196209 Acc: 0.9375\n",
      "Train Epoch: 13 [6400/60000] Loss: 0.111297 Acc: 0.9688\n",
      "Train Epoch: 13 [9600/60000] Loss: 0.365175 Acc: 0.9062\n",
      "Train Epoch: 13 [12800/60000] Loss: 0.290610 Acc: 0.9062\n",
      "Train Epoch: 13 [16000/60000] Loss: 0.313617 Acc: 0.7812\n",
      "Train Epoch: 13 [19200/60000] Loss: 0.171189 Acc: 0.9688\n",
      "Train Epoch: 13 [22400/60000] Loss: 0.206733 Acc: 0.9375\n",
      "Train Epoch: 13 [25600/60000] Loss: 0.181055 Acc: 0.9062\n",
      "Train Epoch: 13 [28800/60000] Loss: 0.136631 Acc: 0.9375\n",
      "Train Epoch: 13 [32000/60000] Loss: 0.169478 Acc: 0.9062\n",
      "Train Epoch: 13 [35200/60000] Loss: 0.205105 Acc: 0.9375\n",
      "Train Epoch: 13 [38400/60000] Loss: 0.314738 Acc: 0.8438\n",
      "Train Epoch: 13 [41600/60000] Loss: 0.259192 Acc: 0.9062\n",
      "Train Epoch: 13 [44800/60000] Loss: 0.336936 Acc: 0.8438\n",
      "Train Epoch: 13 [48000/60000] Loss: 0.177421 Acc: 0.9688\n",
      "Train Epoch: 13 [51200/60000] Loss: 0.047329 Acc: 1.0000\n",
      "Train Epoch: 13 [54400/60000] Loss: 0.197876 Acc: 0.9062\n",
      "Train Epoch: 13 [57600/60000] Loss: 0.323160 Acc: 0.9375\n",
      "Elapsed 85.90s, 6.14 s/epoch, 0.00 s/batch, ets 36.81s\n",
      "\n",
      "Test set: Average loss: 0.2998, Accuracy: 8899/10000 (89%)\n",
      "\n",
      "Train Epoch: 14 [3200/60000] Loss: 0.214934 Acc: 0.9688\n",
      "Train Epoch: 14 [6400/60000] Loss: 0.255008 Acc: 0.9062\n",
      "Train Epoch: 14 [9600/60000] Loss: 0.196322 Acc: 0.8750\n",
      "Train Epoch: 14 [12800/60000] Loss: 0.350623 Acc: 0.9062\n",
      "Train Epoch: 14 [16000/60000] Loss: 0.143949 Acc: 0.9688\n",
      "Train Epoch: 14 [19200/60000] Loss: 0.151463 Acc: 0.9375\n",
      "Train Epoch: 14 [22400/60000] Loss: 0.193116 Acc: 1.0000\n",
      "Train Epoch: 14 [25600/60000] Loss: 0.546088 Acc: 0.7812\n",
      "Train Epoch: 14 [28800/60000] Loss: 0.283150 Acc: 0.9375\n",
      "Train Epoch: 14 [32000/60000] Loss: 0.297323 Acc: 0.8750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 14 [35200/60000] Loss: 0.242817 Acc: 0.9062\n",
      "Train Epoch: 14 [38400/60000] Loss: 0.300826 Acc: 0.8438\n",
      "Train Epoch: 14 [41600/60000] Loss: 0.155230 Acc: 0.9375\n",
      "Train Epoch: 14 [44800/60000] Loss: 0.154523 Acc: 0.9375\n",
      "Train Epoch: 14 [48000/60000] Loss: 0.119851 Acc: 0.9688\n",
      "Train Epoch: 14 [51200/60000] Loss: 0.466593 Acc: 0.8438\n",
      "Train Epoch: 14 [54400/60000] Loss: 0.208276 Acc: 0.9062\n",
      "Train Epoch: 14 [57600/60000] Loss: 0.106473 Acc: 1.0000\n",
      "Elapsed 92.12s, 6.14 s/epoch, 0.00 s/batch, ets 30.71s\n",
      "\n",
      "Test set: Average loss: 0.3120, Accuracy: 8851/10000 (89%)\n",
      "\n",
      "Train Epoch: 15 [3200/60000] Loss: 0.278953 Acc: 0.8750\n",
      "Train Epoch: 15 [6400/60000] Loss: 0.156669 Acc: 0.9375\n",
      "Train Epoch: 15 [9600/60000] Loss: 0.127001 Acc: 0.9375\n",
      "Train Epoch: 15 [12800/60000] Loss: 0.136738 Acc: 1.0000\n",
      "Train Epoch: 15 [16000/60000] Loss: 0.237523 Acc: 0.8750\n",
      "Train Epoch: 15 [19200/60000] Loss: 0.271087 Acc: 0.8438\n",
      "Train Epoch: 15 [22400/60000] Loss: 0.165440 Acc: 0.9062\n",
      "Train Epoch: 15 [25600/60000] Loss: 0.249023 Acc: 0.8438\n",
      "Train Epoch: 15 [28800/60000] Loss: 0.408027 Acc: 0.7188\n",
      "Train Epoch: 15 [32000/60000] Loss: 0.186742 Acc: 0.9062\n",
      "Train Epoch: 15 [35200/60000] Loss: 0.382967 Acc: 0.8750\n",
      "Train Epoch: 15 [38400/60000] Loss: 0.306791 Acc: 0.9062\n",
      "Train Epoch: 15 [41600/60000] Loss: 0.410660 Acc: 0.9062\n",
      "Train Epoch: 15 [44800/60000] Loss: 0.140944 Acc: 0.9688\n",
      "Train Epoch: 15 [48000/60000] Loss: 0.226126 Acc: 0.9375\n",
      "Train Epoch: 15 [51200/60000] Loss: 0.274341 Acc: 0.9688\n",
      "Train Epoch: 15 [54400/60000] Loss: 0.187435 Acc: 0.9062\n",
      "Train Epoch: 15 [57600/60000] Loss: 0.293416 Acc: 0.9062\n",
      "Elapsed 98.26s, 6.14 s/epoch, 0.00 s/batch, ets 24.57s\n",
      "\n",
      "Test set: Average loss: 0.2930, Accuracy: 8937/10000 (89%)\n",
      "\n",
      "Train Epoch: 16 [3200/60000] Loss: 0.064175 Acc: 1.0000\n",
      "Train Epoch: 16 [6400/60000] Loss: 0.086041 Acc: 0.9375\n",
      "Train Epoch: 16 [9600/60000] Loss: 0.121120 Acc: 0.9688\n",
      "Train Epoch: 16 [12800/60000] Loss: 0.302166 Acc: 0.9062\n",
      "Train Epoch: 16 [16000/60000] Loss: 0.163053 Acc: 0.9688\n",
      "Train Epoch: 16 [19200/60000] Loss: 0.234567 Acc: 0.8750\n",
      "Train Epoch: 16 [22400/60000] Loss: 0.354824 Acc: 0.8438\n",
      "Train Epoch: 16 [25600/60000] Loss: 0.194620 Acc: 0.9062\n",
      "Train Epoch: 16 [28800/60000] Loss: 0.110342 Acc: 0.9688\n",
      "Train Epoch: 16 [32000/60000] Loss: 0.225842 Acc: 0.9688\n",
      "Train Epoch: 16 [35200/60000] Loss: 0.162732 Acc: 0.9375\n",
      "Train Epoch: 16 [38400/60000] Loss: 0.113323 Acc: 1.0000\n",
      "Train Epoch: 16 [41600/60000] Loss: 0.145671 Acc: 0.9375\n",
      "Train Epoch: 16 [44800/60000] Loss: 0.268134 Acc: 0.8750\n",
      "Train Epoch: 16 [48000/60000] Loss: 0.151414 Acc: 0.9062\n",
      "Train Epoch: 16 [51200/60000] Loss: 0.126569 Acc: 0.9688\n",
      "Train Epoch: 16 [54400/60000] Loss: 0.141279 Acc: 0.9688\n",
      "Train Epoch: 16 [57600/60000] Loss: 0.231005 Acc: 0.8750\n",
      "Elapsed 104.44s, 6.14 s/epoch, 0.00 s/batch, ets 18.43s\n",
      "\n",
      "Test set: Average loss: 0.3006, Accuracy: 8895/10000 (89%)\n",
      "\n",
      "Train Epoch: 17 [3200/60000] Loss: 0.449140 Acc: 0.8750\n",
      "Train Epoch: 17 [6400/60000] Loss: 0.229727 Acc: 0.9375\n",
      "Train Epoch: 17 [9600/60000] Loss: 0.181282 Acc: 0.9688\n",
      "Train Epoch: 17 [12800/60000] Loss: 0.386352 Acc: 0.8750\n",
      "Train Epoch: 17 [16000/60000] Loss: 0.153220 Acc: 0.9688\n",
      "Train Epoch: 17 [19200/60000] Loss: 0.213026 Acc: 0.9062\n",
      "Train Epoch: 17 [22400/60000] Loss: 0.546978 Acc: 0.8438\n",
      "Train Epoch: 17 [25600/60000] Loss: 0.203140 Acc: 0.9688\n",
      "Train Epoch: 17 [28800/60000] Loss: 0.293355 Acc: 0.9062\n",
      "Train Epoch: 17 [32000/60000] Loss: 0.127861 Acc: 0.9375\n",
      "Train Epoch: 17 [35200/60000] Loss: 0.131085 Acc: 0.9375\n",
      "Train Epoch: 17 [38400/60000] Loss: 0.140873 Acc: 0.9688\n",
      "Train Epoch: 17 [41600/60000] Loss: 0.145352 Acc: 0.9375\n",
      "Train Epoch: 17 [44800/60000] Loss: 0.140877 Acc: 0.9688\n",
      "Train Epoch: 17 [48000/60000] Loss: 0.226598 Acc: 0.8750\n",
      "Train Epoch: 17 [51200/60000] Loss: 0.199978 Acc: 0.9375\n",
      "Train Epoch: 17 [54400/60000] Loss: 0.258557 Acc: 0.9062\n",
      "Train Epoch: 17 [57600/60000] Loss: 0.125101 Acc: 0.9062\n",
      "Elapsed 110.42s, 6.13 s/epoch, 0.00 s/batch, ets 12.27s\n",
      "\n",
      "Test set: Average loss: 0.3032, Accuracy: 8913/10000 (89%)\n",
      "\n",
      "Train Epoch: 18 [3200/60000] Loss: 0.227705 Acc: 0.9375\n",
      "Train Epoch: 18 [6400/60000] Loss: 0.141556 Acc: 0.9375\n",
      "Train Epoch: 18 [9600/60000] Loss: 0.220209 Acc: 0.8750\n",
      "Train Epoch: 18 [12800/60000] Loss: 0.185627 Acc: 0.9688\n",
      "Train Epoch: 18 [16000/60000] Loss: 0.263256 Acc: 0.9062\n",
      "Train Epoch: 18 [19200/60000] Loss: 0.123247 Acc: 0.9688\n",
      "Train Epoch: 18 [22400/60000] Loss: 0.221987 Acc: 0.8750\n",
      "Train Epoch: 18 [25600/60000] Loss: 0.233655 Acc: 0.9062\n",
      "Train Epoch: 18 [28800/60000] Loss: 0.278151 Acc: 0.8750\n",
      "Train Epoch: 18 [32000/60000] Loss: 0.315081 Acc: 0.8750\n",
      "Train Epoch: 18 [35200/60000] Loss: 0.198923 Acc: 0.9688\n",
      "Train Epoch: 18 [38400/60000] Loss: 0.312364 Acc: 0.9375\n",
      "Train Epoch: 18 [41600/60000] Loss: 0.171805 Acc: 0.9688\n",
      "Train Epoch: 18 [44800/60000] Loss: 0.183044 Acc: 0.8750\n",
      "Train Epoch: 18 [48000/60000] Loss: 0.404249 Acc: 0.8125\n",
      "Train Epoch: 18 [51200/60000] Loss: 0.080402 Acc: 0.9688\n",
      "Train Epoch: 18 [54400/60000] Loss: 0.152811 Acc: 0.9688\n",
      "Train Epoch: 18 [57600/60000] Loss: 0.201690 Acc: 0.9062\n",
      "Elapsed 116.73s, 6.14 s/epoch, 0.00 s/batch, ets 6.14s\n",
      "\n",
      "Test set: Average loss: 0.2937, Accuracy: 8958/10000 (90%)\n",
      "\n",
      "Train Epoch: 19 [3200/60000] Loss: 0.320440 Acc: 0.8438\n",
      "Train Epoch: 19 [6400/60000] Loss: 0.172557 Acc: 0.9062\n",
      "Train Epoch: 19 [9600/60000] Loss: 0.586339 Acc: 0.9062\n",
      "Train Epoch: 19 [12800/60000] Loss: 0.106348 Acc: 0.9688\n",
      "Train Epoch: 19 [16000/60000] Loss: 0.074152 Acc: 0.9688\n",
      "Train Epoch: 19 [19200/60000] Loss: 0.308980 Acc: 0.8750\n",
      "Train Epoch: 19 [22400/60000] Loss: 0.065371 Acc: 1.0000\n",
      "Train Epoch: 19 [25600/60000] Loss: 0.141370 Acc: 0.9375\n",
      "Train Epoch: 19 [28800/60000] Loss: 0.271981 Acc: 0.8438\n",
      "Train Epoch: 19 [32000/60000] Loss: 0.255091 Acc: 0.9062\n",
      "Train Epoch: 19 [35200/60000] Loss: 0.232604 Acc: 0.9062\n",
      "Train Epoch: 19 [38400/60000] Loss: 0.101710 Acc: 1.0000\n",
      "Train Epoch: 19 [41600/60000] Loss: 0.165721 Acc: 0.9688\n",
      "Train Epoch: 19 [44800/60000] Loss: 0.254255 Acc: 0.8750\n",
      "Train Epoch: 19 [48000/60000] Loss: 0.273236 Acc: 0.9062\n",
      "Train Epoch: 19 [51200/60000] Loss: 0.371949 Acc: 0.8438\n",
      "Train Epoch: 19 [54400/60000] Loss: 0.375186 Acc: 0.9062\n",
      "Train Epoch: 19 [57600/60000] Loss: 0.280283 Acc: 0.9375\n",
      "Elapsed 122.97s, 6.15 s/epoch, 0.00 s/batch, ets 0.00s\n",
      "\n",
      "Test set: Average loss: 0.3187, Accuracy: 8851/10000 (89%)\n",
      "\n",
      "Total time: 123.70, Best Loss: 0.293\n",
      "Train Epoch: 0 [3200/60000] Loss: 1.913762 Acc: 0.4688\n",
      "Train Epoch: 0 [6400/60000] Loss: 1.366199 Acc: 0.5625\n",
      "Train Epoch: 0 [9600/60000] Loss: 0.993837 Acc: 0.7188\n",
      "Train Epoch: 0 [12800/60000] Loss: 1.000988 Acc: 0.5625\n",
      "Train Epoch: 0 [16000/60000] Loss: 0.585033 Acc: 0.8438\n",
      "Train Epoch: 0 [19200/60000] Loss: 0.536666 Acc: 0.8438\n",
      "Train Epoch: 0 [22400/60000] Loss: 0.960056 Acc: 0.6875\n",
      "Train Epoch: 0 [25600/60000] Loss: 0.768767 Acc: 0.6875\n",
      "Train Epoch: 0 [28800/60000] Loss: 0.468666 Acc: 0.8125\n",
      "Train Epoch: 0 [32000/60000] Loss: 0.676432 Acc: 0.6875\n",
      "Train Epoch: 0 [35200/60000] Loss: 0.501992 Acc: 0.8438\n",
      "Train Epoch: 0 [38400/60000] Loss: 0.584635 Acc: 0.6875\n",
      "Train Epoch: 0 [41600/60000] Loss: 0.323162 Acc: 0.9062\n",
      "Train Epoch: 0 [44800/60000] Loss: 0.667134 Acc: 0.7812\n",
      "Train Epoch: 0 [48000/60000] Loss: 0.679572 Acc: 0.7812\n",
      "Train Epoch: 0 [51200/60000] Loss: 0.477021 Acc: 0.8125\n",
      "Train Epoch: 0 [54400/60000] Loss: 0.454536 Acc: 0.8438\n",
      "Train Epoch: 0 [57600/60000] Loss: 0.552928 Acc: 0.7500\n",
      "Elapsed 5.85s, 5.85 s/epoch, 0.00 s/batch, ets 111.19s\n",
      "\n",
      "Test set: Average loss: 0.4989, Accuracy: 8143/10000 (81%)\n",
      "\n",
      "Train Epoch: 1 [3200/60000] Loss: 0.444912 Acc: 0.7812\n",
      "Train Epoch: 1 [6400/60000] Loss: 0.799942 Acc: 0.7812\n",
      "Train Epoch: 1 [9600/60000] Loss: 0.457949 Acc: 0.7812\n",
      "Train Epoch: 1 [12800/60000] Loss: 0.350030 Acc: 0.8438\n",
      "Train Epoch: 1 [16000/60000] Loss: 0.211350 Acc: 0.9375\n",
      "Train Epoch: 1 [19200/60000] Loss: 0.214548 Acc: 0.9375\n",
      "Train Epoch: 1 [22400/60000] Loss: 0.442878 Acc: 0.8438\n",
      "Train Epoch: 1 [25600/60000] Loss: 0.380842 Acc: 0.9062\n",
      "Train Epoch: 1 [28800/60000] Loss: 0.415249 Acc: 0.8125\n",
      "Train Epoch: 1 [32000/60000] Loss: 0.685785 Acc: 0.7812\n",
      "Train Epoch: 1 [35200/60000] Loss: 0.990957 Acc: 0.6562\n",
      "Train Epoch: 1 [38400/60000] Loss: 0.375771 Acc: 0.8750\n",
      "Train Epoch: 1 [41600/60000] Loss: 0.229077 Acc: 0.9062\n",
      "Train Epoch: 1 [44800/60000] Loss: 0.420569 Acc: 0.7812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [48000/60000] Loss: 0.315432 Acc: 0.9062\n",
      "Train Epoch: 1 [51200/60000] Loss: 0.236998 Acc: 0.8750\n",
      "Train Epoch: 1 [54400/60000] Loss: 0.230768 Acc: 0.9062\n",
      "Train Epoch: 1 [57600/60000] Loss: 0.290227 Acc: 0.8750\n",
      "Elapsed 12.70s, 6.35 s/epoch, 0.00 s/batch, ets 114.29s\n",
      "\n",
      "Test set: Average loss: 0.3979, Accuracy: 8558/10000 (86%)\n",
      "\n",
      "Train Epoch: 2 [3200/60000] Loss: 0.436743 Acc: 0.7812\n",
      "Train Epoch: 2 [6400/60000] Loss: 0.306836 Acc: 0.9062\n",
      "Train Epoch: 2 [9600/60000] Loss: 0.265878 Acc: 0.8750\n",
      "Train Epoch: 2 [12800/60000] Loss: 0.375737 Acc: 0.8438\n",
      "Train Epoch: 2 [16000/60000] Loss: 0.562706 Acc: 0.7812\n",
      "Train Epoch: 2 [19200/60000] Loss: 0.219371 Acc: 0.9375\n",
      "Train Epoch: 2 [22400/60000] Loss: 0.332222 Acc: 0.9375\n",
      "Train Epoch: 2 [25600/60000] Loss: 0.237044 Acc: 0.9062\n",
      "Train Epoch: 2 [28800/60000] Loss: 0.389849 Acc: 0.8750\n",
      "Train Epoch: 2 [32000/60000] Loss: 0.365898 Acc: 0.7812\n",
      "Train Epoch: 2 [35200/60000] Loss: 0.212068 Acc: 0.9375\n",
      "Train Epoch: 2 [38400/60000] Loss: 0.467402 Acc: 0.9062\n",
      "Train Epoch: 2 [41600/60000] Loss: 0.337512 Acc: 0.9375\n",
      "Train Epoch: 2 [44800/60000] Loss: 0.523303 Acc: 0.7812\n",
      "Train Epoch: 2 [48000/60000] Loss: 0.387071 Acc: 0.7812\n",
      "Train Epoch: 2 [51200/60000] Loss: 0.366569 Acc: 0.8125\n",
      "Train Epoch: 2 [54400/60000] Loss: 0.425895 Acc: 0.8438\n",
      "Train Epoch: 2 [57600/60000] Loss: 0.248803 Acc: 0.9375\n",
      "Elapsed 19.46s, 6.49 s/epoch, 0.00 s/batch, ets 110.25s\n",
      "\n",
      "Test set: Average loss: 0.3635, Accuracy: 8711/10000 (87%)\n",
      "\n",
      "Train Epoch: 3 [3200/60000] Loss: 0.318891 Acc: 0.8125\n",
      "Train Epoch: 3 [6400/60000] Loss: 0.280946 Acc: 0.8750\n",
      "Train Epoch: 3 [9600/60000] Loss: 0.398927 Acc: 0.8750\n",
      "Train Epoch: 3 [12800/60000] Loss: 0.322511 Acc: 0.9375\n",
      "Train Epoch: 3 [16000/60000] Loss: 0.163940 Acc: 0.9375\n",
      "Train Epoch: 3 [19200/60000] Loss: 0.120980 Acc: 1.0000\n",
      "Train Epoch: 3 [22400/60000] Loss: 0.375481 Acc: 0.8438\n",
      "Train Epoch: 3 [25600/60000] Loss: 0.302495 Acc: 0.9062\n",
      "Train Epoch: 3 [28800/60000] Loss: 0.472078 Acc: 0.8438\n",
      "Train Epoch: 3 [32000/60000] Loss: 0.294011 Acc: 0.8750\n",
      "Train Epoch: 3 [35200/60000] Loss: 0.442789 Acc: 0.8125\n",
      "Train Epoch: 3 [38400/60000] Loss: 0.634242 Acc: 0.8125\n",
      "Train Epoch: 3 [41600/60000] Loss: 0.271653 Acc: 0.9062\n",
      "Train Epoch: 3 [44800/60000] Loss: 0.260425 Acc: 0.9062\n",
      "Train Epoch: 3 [48000/60000] Loss: 0.388122 Acc: 0.8750\n",
      "Train Epoch: 3 [51200/60000] Loss: 0.169958 Acc: 0.9375\n",
      "Train Epoch: 3 [54400/60000] Loss: 0.476631 Acc: 0.7500\n",
      "Train Epoch: 3 [57600/60000] Loss: 0.363796 Acc: 0.8750\n",
      "Elapsed 26.32s, 6.58 s/epoch, 0.00 s/batch, ets 105.29s\n",
      "\n",
      "Test set: Average loss: 0.3367, Accuracy: 8812/10000 (88%)\n",
      "\n",
      "Train Epoch: 4 [3200/60000] Loss: 0.238414 Acc: 0.9375\n",
      "Train Epoch: 4 [6400/60000] Loss: 0.150382 Acc: 0.9375\n",
      "Train Epoch: 4 [9600/60000] Loss: 0.515032 Acc: 0.8438\n",
      "Train Epoch: 4 [12800/60000] Loss: 0.285570 Acc: 0.8750\n",
      "Train Epoch: 4 [16000/60000] Loss: 0.188504 Acc: 0.9375\n",
      "Train Epoch: 4 [19200/60000] Loss: 0.155213 Acc: 0.9688\n",
      "Train Epoch: 4 [22400/60000] Loss: 0.392789 Acc: 0.8438\n",
      "Train Epoch: 4 [25600/60000] Loss: 0.249845 Acc: 0.9375\n",
      "Train Epoch: 4 [28800/60000] Loss: 0.281963 Acc: 0.8438\n",
      "Train Epoch: 4 [32000/60000] Loss: 0.191575 Acc: 0.9375\n",
      "Train Epoch: 4 [35200/60000] Loss: 0.149668 Acc: 1.0000\n",
      "Train Epoch: 4 [38400/60000] Loss: 0.308004 Acc: 0.8750\n",
      "Train Epoch: 4 [41600/60000] Loss: 0.214901 Acc: 0.8750\n",
      "Train Epoch: 4 [44800/60000] Loss: 0.277303 Acc: 0.8438\n",
      "Train Epoch: 4 [48000/60000] Loss: 0.407011 Acc: 0.8125\n",
      "Train Epoch: 4 [51200/60000] Loss: 0.387951 Acc: 0.8438\n",
      "Train Epoch: 4 [54400/60000] Loss: 0.338002 Acc: 0.8750\n",
      "Train Epoch: 4 [57600/60000] Loss: 0.220580 Acc: 0.8750\n",
      "Elapsed 32.83s, 6.57 s/epoch, 0.00 s/batch, ets 98.48s\n",
      "\n",
      "Test set: Average loss: 0.3388, Accuracy: 8771/10000 (88%)\n",
      "\n",
      "Train Epoch: 5 [3200/60000] Loss: 0.181129 Acc: 0.9375\n",
      "Train Epoch: 5 [6400/60000] Loss: 0.274682 Acc: 0.8750\n",
      "Train Epoch: 5 [9600/60000] Loss: 0.214960 Acc: 0.9375\n",
      "Train Epoch: 5 [12800/60000] Loss: 0.371052 Acc: 0.9062\n",
      "Train Epoch: 5 [16000/60000] Loss: 0.164170 Acc: 0.9375\n",
      "Train Epoch: 5 [19200/60000] Loss: 0.169027 Acc: 0.9688\n",
      "Train Epoch: 5 [22400/60000] Loss: 0.248558 Acc: 0.9062\n",
      "Train Epoch: 5 [25600/60000] Loss: 0.331176 Acc: 0.8750\n",
      "Train Epoch: 5 [28800/60000] Loss: 0.206622 Acc: 0.9062\n",
      "Train Epoch: 5 [32000/60000] Loss: 0.240593 Acc: 0.8750\n",
      "Train Epoch: 5 [35200/60000] Loss: 0.364583 Acc: 0.9062\n",
      "Train Epoch: 5 [38400/60000] Loss: 0.231615 Acc: 0.9375\n",
      "Train Epoch: 5 [41600/60000] Loss: 0.317340 Acc: 0.8750\n",
      "Train Epoch: 5 [44800/60000] Loss: 0.290205 Acc: 0.9062\n",
      "Train Epoch: 5 [48000/60000] Loss: 0.355866 Acc: 0.7812\n",
      "Train Epoch: 5 [51200/60000] Loss: 0.339312 Acc: 0.8750\n",
      "Train Epoch: 5 [54400/60000] Loss: 0.168126 Acc: 0.9688\n",
      "Train Epoch: 5 [57600/60000] Loss: 0.340069 Acc: 0.9062\n",
      "Elapsed 39.42s, 6.57 s/epoch, 0.00 s/batch, ets 91.98s\n",
      "\n",
      "Test set: Average loss: 0.3357, Accuracy: 8811/10000 (88%)\n",
      "\n",
      "Train Epoch: 6 [3200/60000] Loss: 0.102145 Acc: 0.9688\n",
      "Train Epoch: 6 [6400/60000] Loss: 0.093840 Acc: 0.9688\n",
      "Train Epoch: 6 [9600/60000] Loss: 0.246671 Acc: 0.8750\n",
      "Train Epoch: 6 [12800/60000] Loss: 0.225185 Acc: 0.9062\n",
      "Train Epoch: 6 [16000/60000] Loss: 0.282549 Acc: 0.8438\n",
      "Train Epoch: 6 [19200/60000] Loss: 0.114617 Acc: 0.9688\n",
      "Train Epoch: 6 [22400/60000] Loss: 0.423194 Acc: 0.8750\n",
      "Train Epoch: 6 [25600/60000] Loss: 0.233614 Acc: 0.9375\n",
      "Train Epoch: 6 [28800/60000] Loss: 0.111850 Acc: 0.9688\n",
      "Train Epoch: 6 [32000/60000] Loss: 0.344990 Acc: 0.8125\n",
      "Train Epoch: 6 [35200/60000] Loss: 0.199788 Acc: 0.9688\n",
      "Train Epoch: 6 [38400/60000] Loss: 0.313531 Acc: 0.9062\n",
      "Train Epoch: 6 [41600/60000] Loss: 0.286497 Acc: 0.9375\n",
      "Train Epoch: 6 [44800/60000] Loss: 0.300240 Acc: 0.8438\n",
      "Train Epoch: 6 [48000/60000] Loss: 0.110741 Acc: 1.0000\n",
      "Train Epoch: 6 [51200/60000] Loss: 0.279260 Acc: 0.8438\n",
      "Train Epoch: 6 [54400/60000] Loss: 0.311294 Acc: 0.9375\n",
      "Train Epoch: 6 [57600/60000] Loss: 0.323959 Acc: 0.8750\n",
      "Elapsed 46.04s, 6.58 s/epoch, 0.00 s/batch, ets 85.50s\n",
      "\n",
      "Test set: Average loss: 0.3149, Accuracy: 8891/10000 (89%)\n",
      "\n",
      "Train Epoch: 7 [3200/60000] Loss: 0.231192 Acc: 0.8750\n",
      "Train Epoch: 7 [6400/60000] Loss: 0.429808 Acc: 0.8438\n",
      "Train Epoch: 7 [9600/60000] Loss: 0.263131 Acc: 0.9062\n",
      "Train Epoch: 7 [12800/60000] Loss: 0.099467 Acc: 0.9688\n",
      "Train Epoch: 7 [16000/60000] Loss: 0.283472 Acc: 0.8438\n",
      "Train Epoch: 7 [19200/60000] Loss: 0.209067 Acc: 0.9375\n",
      "Train Epoch: 7 [22400/60000] Loss: 0.211048 Acc: 0.8750\n",
      "Train Epoch: 7 [25600/60000] Loss: 0.451800 Acc: 0.8125\n",
      "Train Epoch: 7 [28800/60000] Loss: 0.116837 Acc: 1.0000\n",
      "Train Epoch: 7 [32000/60000] Loss: 0.466892 Acc: 0.8438\n",
      "Train Epoch: 7 [35200/60000] Loss: 0.438771 Acc: 0.8438\n",
      "Train Epoch: 7 [38400/60000] Loss: 0.242079 Acc: 0.9375\n",
      "Train Epoch: 7 [41600/60000] Loss: 0.315426 Acc: 0.9062\n",
      "Train Epoch: 7 [44800/60000] Loss: 0.142164 Acc: 0.9375\n",
      "Train Epoch: 7 [48000/60000] Loss: 0.322213 Acc: 0.8438\n",
      "Train Epoch: 7 [51200/60000] Loss: 0.132368 Acc: 0.9062\n",
      "Train Epoch: 7 [54400/60000] Loss: 0.193045 Acc: 0.9062\n",
      "Train Epoch: 7 [57600/60000] Loss: 0.432870 Acc: 0.8438\n",
      "Elapsed 52.60s, 6.57 s/epoch, 0.00 s/batch, ets 78.90s\n",
      "\n",
      "Test set: Average loss: 0.3033, Accuracy: 8898/10000 (89%)\n",
      "\n",
      "Train Epoch: 8 [3200/60000] Loss: 0.384248 Acc: 0.9062\n",
      "Train Epoch: 8 [6400/60000] Loss: 0.087726 Acc: 0.9688\n",
      "Train Epoch: 8 [9600/60000] Loss: 0.265532 Acc: 0.8750\n",
      "Train Epoch: 8 [12800/60000] Loss: 0.088088 Acc: 0.9688\n",
      "Train Epoch: 8 [16000/60000] Loss: 0.254724 Acc: 0.9062\n",
      "Train Epoch: 8 [19200/60000] Loss: 0.211043 Acc: 0.9062\n",
      "Train Epoch: 8 [22400/60000] Loss: 0.205172 Acc: 0.9375\n",
      "Train Epoch: 8 [25600/60000] Loss: 0.347026 Acc: 0.9375\n",
      "Train Epoch: 8 [28800/60000] Loss: 0.400810 Acc: 0.8750\n",
      "Train Epoch: 8 [32000/60000] Loss: 0.204347 Acc: 0.9062\n",
      "Train Epoch: 8 [35200/60000] Loss: 0.075556 Acc: 1.0000\n",
      "Train Epoch: 8 [38400/60000] Loss: 0.131341 Acc: 0.9375\n",
      "Train Epoch: 8 [41600/60000] Loss: 0.060804 Acc: 0.9688\n",
      "Train Epoch: 8 [44800/60000] Loss: 0.235493 Acc: 0.8438\n",
      "Train Epoch: 8 [48000/60000] Loss: 0.265837 Acc: 0.8750\n",
      "Train Epoch: 8 [51200/60000] Loss: 0.141011 Acc: 0.9375\n",
      "Train Epoch: 8 [54400/60000] Loss: 0.260084 Acc: 0.9062\n",
      "Train Epoch: 8 [57600/60000] Loss: 0.249365 Acc: 0.9062\n",
      "Elapsed 59.16s, 6.57 s/epoch, 0.00 s/batch, ets 72.31s\n",
      "\n",
      "Test set: Average loss: 0.3414, Accuracy: 8766/10000 (88%)\n",
      "\n",
      "Train Epoch: 9 [3200/60000] Loss: 0.252476 Acc: 0.9062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [6400/60000] Loss: 0.425166 Acc: 0.8438\n",
      "Train Epoch: 9 [9600/60000] Loss: 0.122013 Acc: 0.9688\n",
      "Train Epoch: 9 [12800/60000] Loss: 0.418962 Acc: 0.9375\n",
      "Train Epoch: 9 [16000/60000] Loss: 0.122426 Acc: 0.9375\n",
      "Train Epoch: 9 [19200/60000] Loss: 0.151508 Acc: 0.9062\n",
      "Train Epoch: 9 [22400/60000] Loss: 0.311149 Acc: 0.8438\n",
      "Train Epoch: 9 [25600/60000] Loss: 0.170491 Acc: 0.9062\n",
      "Train Epoch: 9 [28800/60000] Loss: 0.512506 Acc: 0.8125\n",
      "Train Epoch: 9 [32000/60000] Loss: 0.182610 Acc: 0.9688\n",
      "Train Epoch: 9 [35200/60000] Loss: 0.144600 Acc: 1.0000\n",
      "Train Epoch: 9 [38400/60000] Loss: 0.105158 Acc: 1.0000\n",
      "Train Epoch: 9 [41600/60000] Loss: 0.112190 Acc: 1.0000\n",
      "Train Epoch: 9 [44800/60000] Loss: 0.143535 Acc: 0.9688\n",
      "Train Epoch: 9 [48000/60000] Loss: 0.059460 Acc: 1.0000\n",
      "Train Epoch: 9 [51200/60000] Loss: 0.376045 Acc: 0.8750\n",
      "Train Epoch: 9 [54400/60000] Loss: 0.534732 Acc: 0.8125\n",
      "Train Epoch: 9 [57600/60000] Loss: 0.239681 Acc: 0.9375\n",
      "Elapsed 66.02s, 6.60 s/epoch, 0.00 s/batch, ets 66.02s\n",
      "\n",
      "Test set: Average loss: 0.3000, Accuracy: 8919/10000 (89%)\n",
      "\n",
      "Train Epoch: 10 [3200/60000] Loss: 0.280259 Acc: 0.8750\n",
      "Train Epoch: 10 [6400/60000] Loss: 0.109035 Acc: 0.9688\n",
      "Train Epoch: 10 [9600/60000] Loss: 0.251394 Acc: 0.9062\n",
      "Train Epoch: 10 [12800/60000] Loss: 0.393977 Acc: 0.8750\n",
      "Train Epoch: 10 [16000/60000] Loss: 0.309339 Acc: 0.8438\n",
      "Train Epoch: 10 [19200/60000] Loss: 0.355785 Acc: 0.9062\n",
      "Train Epoch: 10 [22400/60000] Loss: 0.217772 Acc: 0.9062\n",
      "Train Epoch: 10 [25600/60000] Loss: 0.163385 Acc: 0.9688\n",
      "Train Epoch: 10 [28800/60000] Loss: 0.255248 Acc: 0.9375\n",
      "Train Epoch: 10 [32000/60000] Loss: 0.260465 Acc: 0.9375\n",
      "Train Epoch: 10 [35200/60000] Loss: 0.094442 Acc: 0.9688\n",
      "Train Epoch: 10 [38400/60000] Loss: 0.206552 Acc: 0.9375\n",
      "Train Epoch: 10 [41600/60000] Loss: 0.354853 Acc: 0.8438\n",
      "Train Epoch: 10 [44800/60000] Loss: 0.155540 Acc: 0.9375\n",
      "Train Epoch: 10 [48000/60000] Loss: 0.291811 Acc: 0.8750\n",
      "Train Epoch: 10 [51200/60000] Loss: 0.322007 Acc: 0.8438\n",
      "Train Epoch: 10 [54400/60000] Loss: 0.233570 Acc: 0.9062\n",
      "Train Epoch: 10 [57600/60000] Loss: 0.495338 Acc: 0.8125\n",
      "Elapsed 72.75s, 6.61 s/epoch, 0.00 s/batch, ets 59.52s\n",
      "\n",
      "Test set: Average loss: 0.2935, Accuracy: 8938/10000 (89%)\n",
      "\n",
      "Train Epoch: 11 [3200/60000] Loss: 0.323821 Acc: 0.9062\n",
      "Train Epoch: 11 [6400/60000] Loss: 0.193061 Acc: 0.9375\n",
      "Train Epoch: 11 [9600/60000] Loss: 0.137606 Acc: 0.9688\n",
      "Train Epoch: 11 [12800/60000] Loss: 0.189039 Acc: 0.9375\n",
      "Train Epoch: 11 [16000/60000] Loss: 0.143793 Acc: 0.8750\n",
      "Train Epoch: 11 [19200/60000] Loss: 0.248259 Acc: 0.9375\n",
      "Train Epoch: 11 [22400/60000] Loss: 0.151908 Acc: 0.9062\n",
      "Train Epoch: 11 [25600/60000] Loss: 0.260076 Acc: 0.9375\n",
      "Train Epoch: 11 [28800/60000] Loss: 0.102952 Acc: 0.9688\n",
      "Train Epoch: 11 [32000/60000] Loss: 0.321571 Acc: 0.9062\n",
      "Train Epoch: 11 [35200/60000] Loss: 0.065904 Acc: 1.0000\n",
      "Train Epoch: 11 [38400/60000] Loss: 0.310662 Acc: 0.8750\n",
      "Train Epoch: 11 [41600/60000] Loss: 0.143900 Acc: 0.9688\n",
      "Train Epoch: 11 [44800/60000] Loss: 0.222389 Acc: 0.9062\n",
      "Train Epoch: 11 [48000/60000] Loss: 0.122047 Acc: 0.9688\n",
      "Train Epoch: 11 [51200/60000] Loss: 0.264619 Acc: 0.9375\n",
      "Train Epoch: 11 [54400/60000] Loss: 0.446887 Acc: 0.8750\n",
      "Train Epoch: 11 [57600/60000] Loss: 0.138684 Acc: 0.9375\n",
      "Elapsed 79.76s, 6.65 s/epoch, 0.00 s/batch, ets 53.17s\n",
      "\n",
      "Test set: Average loss: 0.2894, Accuracy: 8960/10000 (90%)\n",
      "\n",
      "Train Epoch: 12 [3200/60000] Loss: 0.407344 Acc: 0.9375\n",
      "Train Epoch: 12 [6400/60000] Loss: 0.132422 Acc: 0.9688\n",
      "Train Epoch: 12 [9600/60000] Loss: 0.270598 Acc: 0.8438\n",
      "Train Epoch: 12 [12800/60000] Loss: 0.085074 Acc: 0.9688\n",
      "Train Epoch: 12 [16000/60000] Loss: 0.338674 Acc: 0.8438\n",
      "Train Epoch: 12 [19200/60000] Loss: 0.284592 Acc: 0.8438\n",
      "Train Epoch: 12 [22400/60000] Loss: 0.218124 Acc: 0.8750\n",
      "Train Epoch: 12 [25600/60000] Loss: 0.136713 Acc: 0.9375\n",
      "Train Epoch: 12 [28800/60000] Loss: 0.309454 Acc: 0.9062\n",
      "Train Epoch: 12 [32000/60000] Loss: 0.321859 Acc: 0.8750\n",
      "Train Epoch: 12 [35200/60000] Loss: 0.155531 Acc: 0.9375\n",
      "Train Epoch: 12 [38400/60000] Loss: 0.381749 Acc: 0.8438\n",
      "Train Epoch: 12 [41600/60000] Loss: 0.200498 Acc: 0.9375\n",
      "Train Epoch: 12 [44800/60000] Loss: 0.089929 Acc: 0.9688\n",
      "Train Epoch: 12 [48000/60000] Loss: 0.169475 Acc: 0.9375\n",
      "Train Epoch: 12 [51200/60000] Loss: 0.384965 Acc: 0.8750\n",
      "Train Epoch: 12 [54400/60000] Loss: 0.221142 Acc: 0.9375\n",
      "Train Epoch: 12 [57600/60000] Loss: 0.340476 Acc: 0.8438\n",
      "Elapsed 86.67s, 6.67 s/epoch, 0.00 s/batch, ets 46.67s\n",
      "\n",
      "Test set: Average loss: 0.2975, Accuracy: 8924/10000 (89%)\n",
      "\n",
      "Train Epoch: 13 [3200/60000] Loss: 0.293343 Acc: 0.8438\n",
      "Train Epoch: 13 [6400/60000] Loss: 0.152418 Acc: 0.9375\n",
      "Train Epoch: 13 [9600/60000] Loss: 0.436257 Acc: 0.8750\n",
      "Train Epoch: 13 [12800/60000] Loss: 0.239025 Acc: 0.9062\n",
      "Train Epoch: 13 [16000/60000] Loss: 0.238002 Acc: 0.9375\n",
      "Train Epoch: 13 [19200/60000] Loss: 0.278704 Acc: 0.8750\n",
      "Train Epoch: 13 [22400/60000] Loss: 0.169894 Acc: 0.9688\n",
      "Train Epoch: 13 [25600/60000] Loss: 0.153386 Acc: 0.9375\n",
      "Train Epoch: 13 [28800/60000] Loss: 0.109542 Acc: 0.9688\n",
      "Train Epoch: 13 [32000/60000] Loss: 0.088915 Acc: 0.9688\n",
      "Train Epoch: 13 [35200/60000] Loss: 0.189008 Acc: 0.9062\n",
      "Train Epoch: 13 [38400/60000] Loss: 0.293869 Acc: 0.9062\n",
      "Train Epoch: 13 [41600/60000] Loss: 0.194299 Acc: 0.9688\n",
      "Train Epoch: 13 [44800/60000] Loss: 0.305924 Acc: 0.8750\n",
      "Train Epoch: 13 [48000/60000] Loss: 0.117977 Acc: 0.9688\n",
      "Train Epoch: 13 [51200/60000] Loss: 0.035519 Acc: 1.0000\n",
      "Train Epoch: 13 [54400/60000] Loss: 0.247959 Acc: 0.9062\n",
      "Train Epoch: 13 [57600/60000] Loss: 0.301004 Acc: 0.9062\n",
      "Elapsed 93.65s, 6.69 s/epoch, 0.00 s/batch, ets 40.13s\n",
      "\n",
      "Test set: Average loss: 0.2949, Accuracy: 8946/10000 (89%)\n",
      "\n",
      "Train Epoch: 14 [3200/60000] Loss: 0.229767 Acc: 0.9062\n",
      "Train Epoch: 14 [6400/60000] Loss: 0.271069 Acc: 0.9062\n",
      "Train Epoch: 14 [9600/60000] Loss: 0.162814 Acc: 0.9062\n",
      "Train Epoch: 14 [12800/60000] Loss: 0.370729 Acc: 0.8750\n",
      "Train Epoch: 14 [16000/60000] Loss: 0.225270 Acc: 0.9062\n",
      "Train Epoch: 14 [19200/60000] Loss: 0.126029 Acc: 0.9688\n",
      "Train Epoch: 14 [22400/60000] Loss: 0.117928 Acc: 0.9688\n",
      "Train Epoch: 14 [25600/60000] Loss: 0.434438 Acc: 0.8438\n",
      "Train Epoch: 14 [28800/60000] Loss: 0.251593 Acc: 0.9375\n",
      "Train Epoch: 14 [32000/60000] Loss: 0.234331 Acc: 0.8750\n",
      "Train Epoch: 14 [35200/60000] Loss: 0.162455 Acc: 0.8750\n",
      "Train Epoch: 14 [38400/60000] Loss: 0.358767 Acc: 0.8750\n",
      "Train Epoch: 14 [41600/60000] Loss: 0.098375 Acc: 1.0000\n",
      "Train Epoch: 14 [44800/60000] Loss: 0.252813 Acc: 0.8438\n",
      "Train Epoch: 14 [48000/60000] Loss: 0.105699 Acc: 0.9688\n",
      "Train Epoch: 14 [51200/60000] Loss: 0.391207 Acc: 0.8750\n",
      "Train Epoch: 14 [54400/60000] Loss: 0.187191 Acc: 0.9062\n",
      "Train Epoch: 14 [57600/60000] Loss: 0.105252 Acc: 0.9062\n",
      "Elapsed 100.37s, 6.69 s/epoch, 0.00 s/batch, ets 33.46s\n",
      "\n",
      "Test set: Average loss: 0.2937, Accuracy: 8937/10000 (89%)\n",
      "\n",
      "Train Epoch: 15 [3200/60000] Loss: 0.152860 Acc: 0.9688\n",
      "Train Epoch: 15 [6400/60000] Loss: 0.196764 Acc: 0.9062\n",
      "Train Epoch: 15 [9600/60000] Loss: 0.065858 Acc: 1.0000\n",
      "Train Epoch: 15 [12800/60000] Loss: 0.143458 Acc: 0.9375\n",
      "Train Epoch: 15 [16000/60000] Loss: 0.220000 Acc: 0.9375\n",
      "Train Epoch: 15 [19200/60000] Loss: 0.200191 Acc: 0.8750\n",
      "Train Epoch: 15 [22400/60000] Loss: 0.173793 Acc: 0.9062\n",
      "Train Epoch: 15 [25600/60000] Loss: 0.177986 Acc: 0.8750\n",
      "Train Epoch: 15 [28800/60000] Loss: 0.503234 Acc: 0.8125\n",
      "Train Epoch: 15 [32000/60000] Loss: 0.171619 Acc: 0.9375\n",
      "Train Epoch: 15 [35200/60000] Loss: 0.284721 Acc: 0.9062\n",
      "Train Epoch: 15 [38400/60000] Loss: 0.315225 Acc: 0.8750\n",
      "Train Epoch: 15 [41600/60000] Loss: 0.415655 Acc: 0.8750\n",
      "Train Epoch: 15 [44800/60000] Loss: 0.068128 Acc: 1.0000\n",
      "Train Epoch: 15 [48000/60000] Loss: 0.138095 Acc: 0.9688\n",
      "Train Epoch: 15 [51200/60000] Loss: 0.210818 Acc: 0.9062\n",
      "Train Epoch: 15 [54400/60000] Loss: 0.160816 Acc: 0.9375\n",
      "Train Epoch: 15 [57600/60000] Loss: 0.196604 Acc: 0.9375\n",
      "Elapsed 107.14s, 6.70 s/epoch, 0.00 s/batch, ets 26.78s\n",
      "\n",
      "Test set: Average loss: 0.2828, Accuracy: 8990/10000 (90%)\n",
      "\n",
      "Train Epoch: 16 [3200/60000] Loss: 0.057620 Acc: 0.9688\n",
      "Train Epoch: 16 [6400/60000] Loss: 0.121888 Acc: 0.9375\n",
      "Train Epoch: 16 [9600/60000] Loss: 0.094509 Acc: 0.9375\n",
      "Train Epoch: 16 [12800/60000] Loss: 0.278067 Acc: 0.9062\n",
      "Train Epoch: 16 [16000/60000] Loss: 0.070983 Acc: 1.0000\n",
      "Train Epoch: 16 [19200/60000] Loss: 0.201291 Acc: 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 16 [22400/60000] Loss: 0.347742 Acc: 0.8438\n",
      "Train Epoch: 16 [25600/60000] Loss: 0.180350 Acc: 0.9375\n",
      "Train Epoch: 16 [28800/60000] Loss: 0.058046 Acc: 1.0000\n",
      "Train Epoch: 16 [32000/60000] Loss: 0.266591 Acc: 0.9375\n",
      "Train Epoch: 16 [35200/60000] Loss: 0.218201 Acc: 0.8750\n",
      "Train Epoch: 16 [38400/60000] Loss: 0.070371 Acc: 1.0000\n",
      "Train Epoch: 16 [41600/60000] Loss: 0.115711 Acc: 0.9375\n",
      "Train Epoch: 16 [44800/60000] Loss: 0.256617 Acc: 0.8750\n",
      "Train Epoch: 16 [48000/60000] Loss: 0.150983 Acc: 0.9375\n",
      "Train Epoch: 16 [51200/60000] Loss: 0.107541 Acc: 0.9375\n",
      "Train Epoch: 16 [54400/60000] Loss: 0.057391 Acc: 0.9688\n",
      "Train Epoch: 16 [57600/60000] Loss: 0.258996 Acc: 0.8750\n",
      "Elapsed 113.72s, 6.69 s/epoch, 0.00 s/batch, ets 20.07s\n",
      "\n",
      "Test set: Average loss: 0.2854, Accuracy: 8972/10000 (90%)\n",
      "\n",
      "Train Epoch: 17 [3200/60000] Loss: 0.350819 Acc: 0.8750\n",
      "Train Epoch: 17 [6400/60000] Loss: 0.221051 Acc: 0.9375\n",
      "Train Epoch: 17 [9600/60000] Loss: 0.160791 Acc: 0.9688\n",
      "Train Epoch: 17 [12800/60000] Loss: 0.308498 Acc: 0.8125\n",
      "Train Epoch: 17 [16000/60000] Loss: 0.183566 Acc: 0.9062\n",
      "Train Epoch: 17 [19200/60000] Loss: 0.249773 Acc: 0.9062\n",
      "Train Epoch: 17 [22400/60000] Loss: 0.453275 Acc: 0.8125\n",
      "Train Epoch: 17 [25600/60000] Loss: 0.329399 Acc: 0.7812\n",
      "Train Epoch: 17 [28800/60000] Loss: 0.248101 Acc: 0.9062\n",
      "Train Epoch: 17 [32000/60000] Loss: 0.081818 Acc: 0.9375\n",
      "Train Epoch: 17 [35200/60000] Loss: 0.168356 Acc: 0.9375\n",
      "Train Epoch: 17 [38400/60000] Loss: 0.125672 Acc: 0.9688\n",
      "Train Epoch: 17 [41600/60000] Loss: 0.097566 Acc: 0.9688\n",
      "Train Epoch: 17 [44800/60000] Loss: 0.151840 Acc: 0.9688\n",
      "Train Epoch: 17 [48000/60000] Loss: 0.190647 Acc: 0.9375\n",
      "Train Epoch: 17 [51200/60000] Loss: 0.142548 Acc: 0.9375\n",
      "Train Epoch: 17 [54400/60000] Loss: 0.216620 Acc: 0.9062\n",
      "Train Epoch: 17 [57600/60000] Loss: 0.087549 Acc: 0.9688\n",
      "Elapsed 120.42s, 6.69 s/epoch, 0.00 s/batch, ets 13.38s\n",
      "\n",
      "Test set: Average loss: 0.3014, Accuracy: 8940/10000 (89%)\n",
      "\n",
      "Train Epoch: 18 [3200/60000] Loss: 0.116883 Acc: 0.9688\n",
      "Train Epoch: 18 [6400/60000] Loss: 0.037588 Acc: 0.9688\n",
      "Train Epoch: 18 [9600/60000] Loss: 0.160617 Acc: 0.9375\n",
      "Train Epoch: 18 [12800/60000] Loss: 0.290501 Acc: 0.8750\n",
      "Train Epoch: 18 [16000/60000] Loss: 0.151753 Acc: 0.9062\n",
      "Train Epoch: 18 [19200/60000] Loss: 0.165154 Acc: 0.9375\n",
      "Train Epoch: 18 [22400/60000] Loss: 0.233402 Acc: 0.8750\n",
      "Train Epoch: 18 [25600/60000] Loss: 0.168258 Acc: 0.9375\n",
      "Train Epoch: 18 [28800/60000] Loss: 0.268832 Acc: 0.8438\n",
      "Train Epoch: 18 [32000/60000] Loss: 0.451161 Acc: 0.8438\n",
      "Train Epoch: 18 [35200/60000] Loss: 0.091762 Acc: 0.9688\n",
      "Train Epoch: 18 [38400/60000] Loss: 0.258829 Acc: 0.9375\n",
      "Train Epoch: 18 [41600/60000] Loss: 0.302335 Acc: 0.9375\n",
      "Train Epoch: 18 [44800/60000] Loss: 0.136723 Acc: 0.9062\n",
      "Train Epoch: 18 [48000/60000] Loss: 0.335525 Acc: 0.8125\n",
      "Train Epoch: 18 [51200/60000] Loss: 0.108735 Acc: 0.9375\n",
      "Train Epoch: 18 [54400/60000] Loss: 0.110170 Acc: 1.0000\n",
      "Train Epoch: 18 [57600/60000] Loss: 0.208705 Acc: 0.9062\n",
      "Elapsed 127.13s, 6.69 s/epoch, 0.00 s/batch, ets 6.69s\n",
      "\n",
      "Test set: Average loss: 0.2760, Accuracy: 9029/10000 (90%)\n",
      "\n",
      "Train Epoch: 19 [3200/60000] Loss: 0.338423 Acc: 0.8438\n",
      "Train Epoch: 19 [6400/60000] Loss: 0.087157 Acc: 0.9688\n",
      "Train Epoch: 19 [9600/60000] Loss: 0.525601 Acc: 0.9062\n",
      "Train Epoch: 19 [12800/60000] Loss: 0.094365 Acc: 0.9688\n",
      "Train Epoch: 19 [16000/60000] Loss: 0.068628 Acc: 1.0000\n",
      "Train Epoch: 19 [19200/60000] Loss: 0.440677 Acc: 0.8125\n",
      "Train Epoch: 19 [22400/60000] Loss: 0.060394 Acc: 0.9688\n",
      "Train Epoch: 19 [25600/60000] Loss: 0.118227 Acc: 0.9688\n",
      "Train Epoch: 19 [28800/60000] Loss: 0.249056 Acc: 0.8750\n",
      "Train Epoch: 19 [32000/60000] Loss: 0.133308 Acc: 1.0000\n",
      "Train Epoch: 19 [35200/60000] Loss: 0.227074 Acc: 0.8750\n",
      "Train Epoch: 19 [38400/60000] Loss: 0.110251 Acc: 0.9688\n",
      "Train Epoch: 19 [41600/60000] Loss: 0.110069 Acc: 1.0000\n",
      "Train Epoch: 19 [44800/60000] Loss: 0.161664 Acc: 0.9062\n",
      "Train Epoch: 19 [48000/60000] Loss: 0.242238 Acc: 0.9062\n",
      "Train Epoch: 19 [51200/60000] Loss: 0.425480 Acc: 0.8438\n",
      "Train Epoch: 19 [54400/60000] Loss: 0.349881 Acc: 0.8750\n",
      "Train Epoch: 19 [57600/60000] Loss: 0.193975 Acc: 0.9375\n",
      "Elapsed 133.77s, 6.69 s/epoch, 0.00 s/batch, ets 0.00s\n",
      "\n",
      "Test set: Average loss: 0.2908, Accuracy: 9008/10000 (90%)\n",
      "\n",
      "Total time: 134.47, Best Loss: 0.276\n"
     ]
    }
   ],
   "source": [
    "model = LeNet()\n",
    "modelBN = LeNetBN() \n",
    "\n",
    "model, epoch_train_loss, epoch_train_acc, epoch_test_loss, epoch_test_acc = main(model)\n",
    "\n",
    "modelBN, epoch_train_loss_bn, epoch_train_acc_bn, epoch_test_loss_bn, epoch_test_acc_bn = main(modelBN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">10. Plot Loss</font> <a name=\"plot-loss\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot loss\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "x = range(len(epoch_train_loss))\n",
    "\n",
    "\n",
    "plt.figure\n",
    "plt.plot(x, epoch_train_loss, 'r', label=\"train loss\")\n",
    "plt.plot(x, epoch_test_loss, 'b', label=\"validation loss\")\n",
    "\n",
    "plt.plot(x, epoch_train_loss_bn, 'r--', label=\"train loss with BN\")\n",
    "plt.plot(x, epoch_test_loss_bn, 'b--',label=\"validation loss with BN\")\n",
    "\n",
    "plt.xlabel('epoch no.')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above curves show that when we use Batch Normalization, the training converges faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:blue\">Miscellaneous: Calculate Mean and Standard Deviation of Fashion MNIST </font> <a name=\"misc\"></a>\n",
    "\n",
    "Ideally, we should not use the same mean and standard deviation for Fashion MNIST and MNIST. Refrain, even when you find many continuing to do this, simply because it does not have a profound effect on the results.\n",
    "\n",
    "Let us find  the mean and standard deviation for Fashion MNIST and use it instead of MNIST.\n",
    "\n",
    "We need to simply find the mean and standard deviation of the whole dataset. So, we load the dataset and then use the functions given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2860)\n",
      "tensor(0.3530)\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "train_transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_set = torchvision.datasets.FashionMNIST(root=\"./data\", train=True, download=True, transform=train_transform)\n",
    "\n",
    "print(train_set.data.float().mean()/255)\n",
    "print(train_set.data.float().std()/255)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
