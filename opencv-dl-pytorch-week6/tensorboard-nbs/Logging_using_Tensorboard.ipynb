{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">How to add TensorBoard logs</font>\n",
    "\n",
    "In this unit we will learn how to add logs to Tensorboard.\n",
    "\n",
    "**We will add the following logs to the TensorBoard**\n",
    "\n",
    "1. Scalars: We can add loss, accuracy etc as scalars.\n",
    "\n",
    "2. Images: We can add plots or figures.\n",
    "\n",
    "3. Graphs: We can add a network graph and its input-output. \n",
    "\n",
    "4. Histograms: We can add n-d array to get its histogram. For example, in each epoch we can add convolution weights.\n",
    "\n",
    "5. PR Curves: We can add prediction probability and labels to get precision vs recall curves.\n",
    "\n",
    "6. Projector: We can add sampled training data to get its embeddings. \n",
    "\n",
    "\n",
    "**We will use two experiments of the regularization notebook**\n",
    "\n",
    "1. Medium-sized model, training without regularization.\n",
    "\n",
    "2. Medium-sized model, training with regularization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  # one of the best graphics library for python\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from typing import Iterable\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:Blue\">1. TensorBoard Dashboard</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "# %reload_ext tensorboard\n",
    "\n",
    "%tensorboard --logdir=logs_fashion_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# <font style=\"color:blue\">2. Training Utils</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">2.1. Get Fashion MNIST data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fashion mnist class name\n",
    "fashion_mnist_classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "                         'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_inputs_labels(inputs, targets, n=100):\n",
    "    \"\"\"\n",
    "    get random inputs and labels\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(inputs) == len(targets)\n",
    "\n",
    "    rand_indices = torch.randperm(len(targets))\n",
    "    \n",
    "    data = inputs[rand_indices][:n]\n",
    "    \n",
    "    labels = targets[rand_indices][:n]\n",
    "    \n",
    "    class_labels = [fashion_mnist_classes[lab] for lab in labels]\n",
    "    \n",
    "    return data, class_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:magenta\">i. Projector in TensorBoard</font>\n",
    "\n",
    "We can add sampled training data to get its embeddings (e.g. PCA, T-SNE, etc.).\n",
    "\n",
    "```\n",
    "SummaryWriter.add_embedding(mat, metadata=None, label_img=None, global_step=None, tag='default', metadata_header=None)\n",
    "```\n",
    "\n",
    "**Parameters:**\n",
    "\n",
    "- **mat** (`torch.Tensor` or `numpy.array`) – A matrix in which each row is the feature vector of the data point. For example, if we have n images of `3` channel (colored) with width `W`and height `H`, then the data shape will be `n x 3 x H x W`. So the mat input should be `n x 3 * H * W`.\n",
    "\n",
    "- **metadata** (`list`) – A list of labels of which each element will be converted to string.\n",
    "\n",
    "- **label_img** (`torch.Tensor`) – Images correspond to each data point.\n",
    "\n",
    "- **global_step** (`python:int`) – Global step value to record.\n",
    "\n",
    "- **tag** (`string`) – Name for the embedding.\n",
    "\n",
    "Get details [here](https://pytorch.org/docs/stable/tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_embedding)\n",
    "\n",
    "\n",
    "Function **`add_data_embedings`** in below cell samples add n-datapoints from the dataset and data to TensorBoard.\n",
    "\n",
    "\n",
    "**PROJECTOR** tab will look similar to the following after changing colored as labels:\n",
    "\n",
    "\n",
    "<img src=\"https://www.learnopencv.com/wp-content/uploads/2020/02/w3-w4-projrctor-tensorboard.png\" width=900>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_data_embedings(dataset, tb_writer, n=100):\n",
    "    \"\"\"\n",
    "    Add a few inputs and labels to tensorboard. \n",
    "    \"\"\"\n",
    "    \n",
    "    images, labels = get_random_inputs_labels(inputs=dataset.data, targets=dataset.targets, n=n)\n",
    "    \n",
    "    tb_writer.add_embedding(mat = images.view(-1, 28 * 28), \n",
    "                            metadata=labels, \n",
    "                            label_img=images.unsqueeze(1))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(batch_size, data_root, tb_writer, num_workers=1, data_augmentation=False):\n",
    "    \n",
    "    # common transforms\n",
    "    common_transforms = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.2860, ), (0.3530, ))\n",
    "    ])\n",
    "    \n",
    "    # if data_augmentation is true \n",
    "    # data augmentation implementation\n",
    "    if data_augmentation:\n",
    "        train_transforms = transforms.Compose([\n",
    "            transforms.RandomChoice([\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomVerticalFlip(),\n",
    "                transforms.RandomRotation(90, fill=(0,)),\n",
    "                transforms.RandomCrop(28, padding=4, fill=(0,))\n",
    "            ]),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.2860, ), (0.3530, ))\n",
    "        ])\n",
    "    # else do common transforms\n",
    "    else:\n",
    "        train_transforms = common_transforms\n",
    "        \n",
    "        \n",
    "    \n",
    "    # train dataloader\n",
    "    traindata = datasets.FashionMNIST(root=data_root, train=True, download=True, transform=train_transforms)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        traindata,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    \n",
    "    # test dataloader\n",
    "    testdata = datasets.FashionMNIST(root=data_root, train=False, download=True, transform=common_transforms)\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        testdata,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    \n",
    "    # add embedding / projector\n",
    "    \n",
    "    add_data_embedings(testdata, tb_writer, n=100)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">2.2. System Configuration</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SystemConfiguration:\n",
    "    '''\n",
    "    Describes the common system setting needed for reproducible training\n",
    "    '''\n",
    "    seed: int = 21  # seed number to set the state of all random number generators\n",
    "    cudnn_benchmark_enabled: bool = True  # enable CuDNN benchmark for the sake of performance\n",
    "    cudnn_deterministic: bool = True  # make cudnn deterministic (reproducible training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">2.3. Training Configuration</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingConfiguration:\n",
    "    '''\n",
    "    Describes configuration of the training process\n",
    "    '''\n",
    "    batch_size: int = 32  \n",
    "    epochs_count: int = 50  \n",
    "    init_learning_rate: float = 0.02  # initial learning rate for lr scheduler\n",
    "    decay_rate: float = 0.1  \n",
    "    log_interval: int = 500  \n",
    "    test_interval: int = 1  \n",
    "    data_root: str = \"../resource/lib/publicdata/images\" \n",
    "    num_workers: int = 10  \n",
    "    device: str = 'cuda'  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">2.4. System Setup</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_system(system_config: SystemConfiguration) -> None:\n",
    "    torch.manual_seed(system_config.seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cudnn_benchmark_enabled = system_config.cudnn_benchmark_enabled\n",
    "        torch.backends.cudnn.deterministic = system_config.cudnn_deterministic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">2.5. Predictions</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(model, device, batch_input, max_prob=True):\n",
    "    \"\"\"\n",
    "    get prediction for batch inputs\n",
    "    \"\"\"\n",
    "    \n",
    "    # send model to cpu/cuda according to your system configuration\n",
    "    model.to(device)\n",
    "    \n",
    "    # it is important to do model.eval() before prediction\n",
    "    model.eval()\n",
    "\n",
    "    data = batch_input.to(device)\n",
    "\n",
    "    output = model(data)\n",
    "\n",
    "    # get probability score using softmax\n",
    "    prob = F.softmax(output, dim=1)\n",
    "    \n",
    "    if max_prob:\n",
    "        # get the max probability\n",
    "        pred_prob = prob.data.max(dim=1)[0]\n",
    "    else:\n",
    "        pred_prob = prob.data\n",
    "    \n",
    "    # get the index of the max probability\n",
    "    pred_index = prob.data.max(dim=1)[1]\n",
    "    \n",
    "    return pred_index.cpu().numpy(), pred_prob.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_and_prob(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    get targets and prediction probabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    pred_prob = []\n",
    "    targets = []\n",
    "    \n",
    "    for _, (data, target) in enumerate(dataloader):\n",
    "        \n",
    "        _, prob = prediction(model, device, data, max_prob=False)\n",
    "        \n",
    "        pred_prob.append(prob)\n",
    "        \n",
    "        target = target.numpy()\n",
    "        targets.append(target)\n",
    "        \n",
    "    targets = np.concatenate(targets)\n",
    "    targets = targets.astype(int)\n",
    "    pred_prob = np.concatenate(pred_prob, axis=0)\n",
    "    \n",
    "    return targets, pred_prob\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:magenta\">ii. PR Curves in TensorBoard</font>\n",
    "\n",
    "Precision-recall curve tells us about the model’s performance under different threshold settings. With this function we have to provide the ground truth labeling (T/F) and prediction confidence (usually the output of the model) for each target. The TensorBoard UI will let you choose the threshold interactively.\n",
    "```\n",
    "SummaryWriter.add_pr_curve(tag, labels, predictions, global_step=None, num_thresholds=127, weights=None, walltime=None)\n",
    "```\n",
    "\n",
    "\n",
    "**Parameters:**\n",
    "\n",
    "- **tag** (`string`) – Data identifier.\n",
    "\n",
    "- **labels** (`torch.Tensor`, `numpy.array`, or `string/blobname`) – Ground truth data. Binary label for each element.\n",
    "\n",
    "- **predictions** (`torch.Tensor`, `numpy.array`, or `string/blobname`) – The probability that an element be classified as true. Value should in [0, 1].\n",
    "\n",
    "- **global_step** (`python:int`) – Global step value to record.\n",
    "\n",
    "- **num_thresholds** (`python:int`) – Number of thresholds used to draw the curve.\n",
    "\n",
    "- **walltime** (`python:float`) – Optional override default walltime (time.time()) in seconds.\n",
    "\n",
    "Get more details [here](https://pytorch.org/docs/stable/tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_pr_curve).\n",
    "\n",
    "Function **`add_pr_curves_to_tensorboard`**, gets target and prediction probabilities from the function `get_target_and_prob` and pass it to `SummaryWriter.add_pr_curve` to get precision-recall curve in TensorBoard.\n",
    "\n",
    "**PR CURVES** tab will look similar to the following:\n",
    "\n",
    "<img src=\"https://www.learnopencv.com/wp-content/uploads/2020/02/c3-w4-prcurves-tensorboard.png\" width=900>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pr_curves_to_tensorboard(model, dataloader, device, tb_writer, epoch, num_classes=10):\n",
    "    \"\"\"\n",
    "    Add precession and recall curve to tensorboard.\n",
    "    \"\"\"\n",
    "    \n",
    "    targets, pred_prob = get_target_and_prob(model, dataloader, device)\n",
    "    \n",
    "    for cls_idx in range(num_classes):\n",
    "        binary_target = targets == cls_idx\n",
    "        true_prediction_prob = pred_prob[:, cls_idx]\n",
    "        \n",
    "        tb_writer.add_pr_curve(fashion_mnist_classes[cls_idx], \n",
    "                               binary_target, \n",
    "                               true_prediction_prob, \n",
    "                               global_step=epoch)\n",
    "        \n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:magenta\">iii. Images in TensorBoard</font>\n",
    "\n",
    "**Add image data to the summary.**\n",
    "\n",
    "```\n",
    "SummaryWriter.add_image(tag, img_tensor, global_step=None, walltime=None, dataformats='CHW')\n",
    "```\n",
    "\n",
    "**Parameters:**\n",
    "\n",
    "- **tag** (`string`) – Data identifier.\n",
    "\n",
    "- **img_tensor** (`torch.Tensor`, `numpy.array`, or `string/blobname`) – Image data. e.g Matplotlib figures, images, etc.\n",
    "\n",
    "- **global_step** (`python:int`) – Global step value to record.\n",
    "\n",
    "- **walltime** (`python:float`) – Optional override default walltime (`time.time()`) in seconds.\n",
    "\n",
    "Find details [here](https://pytorch.org/docs/stable/tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_image).\n",
    "\n",
    "In the function **`add_wrong_prediction_to_tensorboard`**, we will find wrong predictions, plot as a figure and then add this figure to TensorBoard.\n",
    "\n",
    "The following is a sample image, which is added to TensorBoard.\n",
    "\n",
    "<img src=\"https://www.learnopencv.com/wp-content/uploads/2020/02/c3-w4-images-tensorboard.png\" width=900>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_wrong_prediction_to_tensorboard(model, dataloader, device, tb_writer, \n",
    "                                        epoch, tag='Wrong_Predections', max_images='all'):\n",
    "    \"\"\"\n",
    "    Add wrong predicted images to tensorboard.\n",
    "    \"\"\"\n",
    "    #number of images in one row\n",
    "    num_images_per_row = 8\n",
    "    im_scale = 3\n",
    "    \n",
    "    plot_images = []\n",
    "    wrong_labels = []\n",
    "    pred_prob = []\n",
    "    right_label = []\n",
    "    \n",
    "    for _, (data, target) in enumerate(dataloader):\n",
    "        \n",
    "        \n",
    "        images = data.numpy()\n",
    "        pred, prob = prediction(model, device, data)\n",
    "        target = target.numpy()\n",
    "        indices = pred.astype(int) != target.astype(int)\n",
    "        \n",
    "        plot_images.append(images[indices])\n",
    "        wrong_labels.append(pred[indices])\n",
    "        pred_prob.append(prob[indices])\n",
    "        right_label.append(target[indices])\n",
    "        \n",
    "    plot_images = np.concatenate(plot_images, axis=0).squeeze()\n",
    "    wrong_labels = np.concatenate(wrong_labels)\n",
    "    wrong_labels = wrong_labels.astype(int)\n",
    "    right_label = np.concatenate(right_label)\n",
    "    right_label = right_label.astype(int)\n",
    "    pred_prob = np.concatenate(pred_prob)\n",
    "    \n",
    "    \n",
    "    if max_images == 'all':\n",
    "        num_images = len(images)\n",
    "    else:\n",
    "        num_images = min(len(plot_images), max_images)\n",
    "        \n",
    "    fig_width = num_images_per_row * im_scale\n",
    "    \n",
    "    if num_images % num_images_per_row == 0:\n",
    "        num_row = num_images/num_images_per_row\n",
    "    else:\n",
    "        num_row = int(num_images/num_images_per_row) + 1\n",
    "        \n",
    "    fig_height = num_row * im_scale\n",
    "        \n",
    "    plt.style.use('default')\n",
    "    plt.rcParams[\"figure.figsize\"] = (fig_width, fig_height)\n",
    "    fig = plt.figure()\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        plt.subplot(num_row, num_images_per_row, i+1, xticks=[], yticks=[])\n",
    "        plt.imshow(plot_images[i], cmap='gray')\n",
    "        plt.gca().set_title('{0}({1:.2}), {2}'.format(fashion_mnist_classes[wrong_labels[i]], \n",
    "                                                          pred_prob[i], \n",
    "                                                          fashion_mnist_classes[right_label[i]]))\n",
    "        \n",
    "    tb_writer.add_figure(tag, fig, global_step=epoch)\n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">2.6. Training Function</font>\n",
    "\n",
    "In this section we will train the model. We are already familiar with the training pipeline used in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    train_config: TrainingConfiguration, model: nn.Module, optimizer: torch.optim.Optimizer,\n",
    "    train_loader: torch.utils.data.DataLoader, epoch_idx: int, tb_writer: SummaryWriter\n",
    ") -> None:\n",
    "    \n",
    "    # change model in training mood\n",
    "    model.train()\n",
    "    \n",
    "    # to get batch loss\n",
    "    batch_loss = np.array([])\n",
    "    \n",
    "    # to get batch accuracy\n",
    "    batch_acc = np.array([])\n",
    "        \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        # clone target\n",
    "        indx_target = target.clone()\n",
    "        # send data to device (its is mandatory if GPU has to be used)\n",
    "        data = data.to(train_config.device)\n",
    "        # send target to device\n",
    "        target = target.to(train_config.device)\n",
    "\n",
    "        # reset parameters gradient to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass to the model\n",
    "        output = model(data)\n",
    "        \n",
    "        # cross entropy loss\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        \n",
    "        # find gradients w.r.t training parameters\n",
    "        loss.backward()\n",
    "        # Update parameters using gardients\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_loss = np.append(batch_loss, [loss.item()])\n",
    "        \n",
    "        # Score to probability using softmax\n",
    "        prob = F.softmax(output, dim=1)\n",
    "            \n",
    "        # get the index of the max probability\n",
    "        pred = prob.data.max(dim=1)[1]  \n",
    "                        \n",
    "        # correct prediction\n",
    "        correct = pred.cpu().eq(indx_target).sum()\n",
    "            \n",
    "        # accuracy\n",
    "        acc = float(correct) / float(len(data))\n",
    "        \n",
    "        batch_acc = np.append(batch_acc, [acc])\n",
    "\n",
    "        if batch_idx % train_config.log_interval == 0 and batch_idx > 0:\n",
    "            \n",
    "            total_batch = epoch_idx * len(train_loader.dataset)/train_config.batch_size + batch_idx\n",
    "            tb_writer.add_scalar('Loss/train-batch', loss.item(), total_batch)\n",
    "            tb_writer.add_scalar('Accuracy/train-batch', acc, total_batch)\n",
    "            \n",
    "    epoch_loss = batch_loss.mean()\n",
    "    epoch_acc = batch_acc.mean()\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">2.7. Validation Function</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(\n",
    "    train_config: TrainingConfiguration,\n",
    "    model: nn.Module,\n",
    "    test_loader: torch.utils.data.DataLoader\n",
    ") -> float:\n",
    "    # \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    count_corect_predictions = 0\n",
    "    for data, target in test_loader:\n",
    "        indx_target = target.clone()\n",
    "        data = data.to(train_config.device)\n",
    "        \n",
    "        target = target.to(train_config.device)\n",
    "        \n",
    "        output = model(data)\n",
    "        # add loss for each mini batch\n",
    "        test_loss += F.cross_entropy(output, target).item()\n",
    "        \n",
    "        # get probability score using softmax\n",
    "        prob = F.softmax(output, dim=1)\n",
    "        \n",
    "        # get the index of the max probability\n",
    "        pred = prob.data.max(dim=1)[1] \n",
    "        \n",
    "        # add correct prediction count\n",
    "        count_corect_predictions += pred.cpu().eq(indx_target).sum()\n",
    "\n",
    "    # average over number of mini-batches\n",
    "    test_loss = test_loss / len(test_loader)  \n",
    "    \n",
    "    # average over number of dataset\n",
    "    accuracy = 100. * count_corect_predictions / len(test_loader.dataset)\n",
    "    \n",
    "    return test_loss, accuracy/100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:magenta\">iv. Histogram in TensorBoard</font>\n",
    "\n",
    "**Add histogram to summary.**\n",
    "\n",
    "```\n",
    "SummaryWriter.add_histogram(tag, values, global_step=None, bins='tensorflow', walltime=None, max_bins=None)\n",
    "```\n",
    "\n",
    "**Parameters:**\n",
    "\n",
    "- **tag** (`string`) – Data identifier.\n",
    "\n",
    "- **values** (`torch.Tensor`, `numpy.array`, or `string/blobname`) – Values to build histogram.\n",
    "\n",
    "- **global_step** (`python:int`) – Global step value to record.\n",
    "\n",
    "- **bins** (`string`) – One of {‘tensorflow’,’auto’, ‘fd’, …}. This determines how the bins are made. You can find other options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html.\n",
    "\n",
    "- **walltime** (`python:float`) – Optional override default walltime (`time.time()`) seconds after epoch of event.\n",
    "\n",
    "Find details [here](https://pytorch.org/docs/stable/tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_histogram).\n",
    "\n",
    "In the function **`add_model_weights_as_histogram`**, we are iterating through named parameters and plotting as a histogram. \n",
    "\n",
    "**The followings are the histogram plots for first layer CNN filters for no-regularization and regularization for all epochs:**\n",
    "\n",
    "---\n",
    "\n",
    "<table><tr>\n",
    "<td><img src=\"https://www.learnopencv.com/wp-content/uploads/2020/02/c3-w4-no-reg-tensorboard.png\" alt=\"Drawing\" style=\"width: 500px;\"></td>\n",
    "<td> <img src=\"https://www.learnopencv.com/wp-content/uploads/2020/02/c3-w4-reg-tensorboard.png\" alt=\"Drawing\" style=\"width: 500px;\"></td>\n",
    "</tr></table>\n",
    "\n",
    "---\n",
    "    \n",
    "From the histogram, we can observe that the number of weights for no-regularization is lower than regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_model_weights_as_histogram(model, tb_writer, epoch):\n",
    "    \"\"\"\n",
    "    Get named parameters and plot as histogram\n",
    "    \"\"\"\n",
    "    for name, param in model.named_parameters():\n",
    "        tb_writer.add_histogram(name.replace('.', '/'), param.data.cpu().abs(), epoch)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:magenta\">v. Graph in TensorBoard</font>\n",
    "\n",
    "**Add network graph and input shape to the summary.**\n",
    "\n",
    "```\n",
    "SummaryWriter.add_graph(model, input_to_model=None, verbose=False)\n",
    "```\n",
    "\n",
    "**Parameters:**\n",
    "\n",
    "- **model** (`nn.modules`) – PyTorch model.\n",
    "\n",
    "- **input_to_model** (`tensor`) – Input tensor.\n",
    "\n",
    "In the function **`add_network_graph_tensorboard`**, we add a neural network graph and it's inputs.\n",
    "\n",
    "**The followings are images of the graph and it's inputs**\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "<table><tr>\n",
    "<td><img src=\"https://www.learnopencv.com/wp-content/uploads/2020/02/c3-w4-graph-tensorboard.png\" alt=\"Drawing\" style=\"width: 500px;\"></td>\n",
    "<td> <img src=\"https://www.learnopencv.com/wp-content/uploads/2020/02/c3-w4-inputs-tensorboard.png\" alt=\"Drawing\" style=\"width: 5\n",
    "    00px;\"></td>\n",
    "</tr></table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_network_graph_tensorboard(model, inputs, tb_writer):\n",
    "    tb_writer.add_graph(model, inputs)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">2.8. Main Function for Training and Validation</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:magenta\">vi. Scalar In TensorBoard</font>\n",
    "\n",
    "**Add scalar data to the summary. e.g. loss, accuracy etc.**\n",
    "\n",
    "```\n",
    "SummaryWriter.add_scalar(tag, scalar_value, global_step=None, walltime=None)\n",
    "```\n",
    "\n",
    "**Parameters:**\n",
    "\n",
    "- **tag** (`string`) – Data identifier.\n",
    "\n",
    "- **scalar_value** (`python:float` or `string/blobname`) – Value to save.\n",
    "\n",
    "- **global_step** (`python:int`) – Global step value to record.\n",
    "\n",
    "- **walltime** (`python:float`) – Optional override default walltime (`time.time()`) in seconds.\n",
    "\n",
    "Get details [here](https://pytorch.org/docs/stable/tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_scalar).\n",
    "\n",
    "In the **`main`** function, we add loss, accuracy etc. as a scalar. \n",
    "\n",
    "The following is a sample plot of scalar (validation accuracy): \n",
    "\n",
    "<img src=\"https://www.learnopencv.com/wp-content/uploads/2020/02/c3-w4-scalar-tensorboard.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:magenta\">vii. Scalars in TensorBoard</font>\n",
    "\n",
    "**Add many scalar data to the summary. e.g. validation and train loss together.**\n",
    "\n",
    "<font style=\"color:red\">Note that this function also keeps logged scalars in memory. In extreme case it explodes your RAM.</font>\n",
    "\n",
    "```\n",
    "SummaryWriter.add_scalars(main_tag, tag_scalar_dict, global_step=None, walltime=None)\n",
    "```\n",
    "\n",
    "**Parameters:**\n",
    "\n",
    "- **main_tag** (`string`) – The parent name for the tags.\n",
    "\n",
    "- **tag_scalar_dict** (`dict`) – Key-value pair storing the tag and the corresponding values, e.g `dict`.\n",
    "\n",
    "- **global_step** (`python:int`) – Global step value to record.\n",
    "\n",
    "- **walltime** (`python:float`) – Optional override default walltime (`time.time()`) in seconds.\n",
    "\n",
    "Get details [here](https://pytorch.org/docs/stable/tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_scalars).\n",
    "\n",
    "In the **`main`** function, We will add loss and accuracy of training and validation simaltanously as scalars. \n",
    "\n",
    "The following is a sample plot of scalar (training and validation loss): \n",
    "\n",
    "<img src=\"https://www.learnopencv.com/wp-content/uploads/2020/02/c3-w4-scalars-tensorboard.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model, optimizer, tb_writer, scheduler=None, system_configuration=SystemConfiguration(), \n",
    "         training_configuration=TrainingConfiguration(), data_augmentation=False):\n",
    "    \n",
    "    # system configuration\n",
    "    setup_system(system_configuration)\n",
    "\n",
    "    # batch size\n",
    "    batch_size_to_set = training_configuration.batch_size\n",
    "    # num_workers\n",
    "    num_workers_to_set = training_configuration.num_workers\n",
    "    # epochs\n",
    "    epoch_num_to_set = training_configuration.epochs_count\n",
    "\n",
    "    # if GPU is available use training config, \n",
    "    # else lower batch_size, num_workers and epochs count\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "        batch_size_to_set = 16\n",
    "        num_workers_to_set = 2\n",
    "\n",
    "    # data loader\n",
    "    train_loader, test_loader = get_data(\n",
    "        batch_size=batch_size_to_set,\n",
    "        data_root=training_configuration.data_root,\n",
    "        tb_writer=tb_writer,\n",
    "        num_workers=num_workers_to_set,\n",
    "        data_augmentation=data_augmentation\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Update training configuration\n",
    "    training_configuration = TrainingConfiguration(\n",
    "        device=device,\n",
    "        batch_size=batch_size_to_set,\n",
    "        num_workers=num_workers_to_set\n",
    "    )\n",
    "        \n",
    "    # send model to device (GPU/CPU)\n",
    "    model.to(training_configuration.device)\n",
    "    \n",
    "    \n",
    "    # add network graph with inputs info\n",
    "    images, labels = next(iter(test_loader))\n",
    "    images = images.to(training_configuration.device)\n",
    "    add_network_graph_tensorboard(model, images, tb_writer)\n",
    "\n",
    "    best_loss = torch.tensor(np.inf)\n",
    "    \n",
    "    # epoch train/test loss\n",
    "    epoch_train_loss = np.array([])\n",
    "    epoch_test_loss = np.array([])\n",
    "    \n",
    "    # epoch train/test accuracy\n",
    "    epoch_train_acc = np.array([])\n",
    "    epoch_test_acc = np.array([])\n",
    "    \n",
    "    \n",
    "    # training time measurement\n",
    "    t_begin = time.time()\n",
    "    for epoch in range(training_configuration.epochs_count):\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_acc = train(training_configuration, model, optimizer, train_loader, epoch, tb_writer)\n",
    "        \n",
    "        epoch_train_loss = np.append(epoch_train_loss, [train_loss])\n",
    "        \n",
    "        epoch_train_acc = np.append(epoch_train_acc, [train_acc])\n",
    "        \n",
    "        # add scalar (loss/accuracy) to tensorboard\n",
    "        tb_writer.add_scalar('Loss/Train',train_loss, epoch)\n",
    "        tb_writer.add_scalar('Accuracy/Train', train_acc, epoch)\n",
    "\n",
    "        elapsed_time = time.time() - t_begin\n",
    "        speed_epoch = elapsed_time / (epoch + 1)\n",
    "        speed_batch = speed_epoch / len(train_loader)\n",
    "        eta = speed_epoch * training_configuration.epochs_count - elapsed_time\n",
    "        \n",
    "        # add time metadata to tensorboard\n",
    "        tb_writer.add_scalar('Time/elapsed_time', elapsed_time, epoch)\n",
    "        tb_writer.add_scalar('Time/speed_epoch', speed_epoch, epoch)\n",
    "        tb_writer.add_scalar('Time/speed_batch', speed_batch, epoch)\n",
    "        tb_writer.add_scalar('Time/eta', eta, epoch)\n",
    "\n",
    "        # Validate\n",
    "        if epoch % training_configuration.test_interval == 0:\n",
    "            current_loss, current_accuracy = validate(training_configuration, model, test_loader)\n",
    "            \n",
    "            epoch_test_loss = np.append(epoch_test_loss, [current_loss])\n",
    "        \n",
    "            epoch_test_acc = np.append(epoch_test_acc, [current_accuracy])\n",
    "            \n",
    "            # add scalar (loss/accuracy) to tensorboard\n",
    "            tb_writer.add_scalar('Loss/Validation', current_loss, epoch)\n",
    "            tb_writer.add_scalar('Accuracy/Validation', current_accuracy, epoch)\n",
    "            \n",
    "            # add scalars (loss/accuracy) to tensorboard\n",
    "            tb_writer.add_scalars('Loss/train-val', {'train': train_loss, \n",
    "                                           'validation': current_loss}, epoch)\n",
    "            tb_writer.add_scalars('Accuracy/train-val', {'train': train_acc, \n",
    "                                               'validation': current_accuracy}, epoch)\n",
    "            \n",
    "            if current_loss < best_loss:\n",
    "                best_loss = current_loss\n",
    "                \n",
    "            # add wrong predicted image to tensorboard\n",
    "            add_wrong_prediction_to_tensorboard(model, test_loader, \n",
    "                                                training_configuration.device, \n",
    "                                                tb_writer, epoch, max_images=300)\n",
    "        \n",
    "        # scheduler step/ update learning rate\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "            \n",
    "        # adding model weights to tensorboard as histogram\n",
    "        add_model_weights_as_histogram(model, tb_writer, epoch)\n",
    "        \n",
    "        # add pr curves to tensor board\n",
    "        add_pr_curves_to_tensorboard(model, test_loader, \n",
    "                                     training_configuration.device, \n",
    "                                     tb_writer, epoch, num_classes=10)\n",
    "        \n",
    "                \n",
    "    print(\"Total time: {:.2f}, Best Loss: {:.3f}\".format(time.time() - t_begin, best_loss))\n",
    "    \n",
    "    \n",
    "    \n",
    "    return model, epoch_train_loss, epoch_train_acc, epoch_test_loss, epoch_test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">2.9. Optimizer and Scheduler</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer_and_scheduler(model):\n",
    "    train_config = TrainingConfiguration()\n",
    "\n",
    "    init_learning_rate = train_config.init_learning_rate\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr = init_learning_rate,\n",
    "        momentum = 0.9\n",
    "    )\n",
    "\n",
    "    decay_rate = train_config.decay_rate\n",
    "\n",
    "    lmbda = lambda epoch: 1/(1 + decay_rate * epoch)\n",
    "\n",
    "    # Scheduler\n",
    "    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lmbda)\n",
    "    \n",
    "    return optimizer, scheduler\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">3. Model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MediumModel(nn.Module):\n",
    "    def __init__(self, dropout=0.0, batch_norm=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # convolution layers\n",
    "        if batch_norm:\n",
    "            self._body = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5),\n",
    "                nn.BatchNorm2d(16),\n",
    "                nn.ReLU(inplace=True),\n",
    "\n",
    "                nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(kernel_size=2),\n",
    "\n",
    "                nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(kernel_size=2),\n",
    "                nn.Dropout(dropout)\n",
    "            )\n",
    "        else:\n",
    "             self._body = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5),\n",
    "                nn.ReLU(inplace=True),\n",
    "\n",
    "                nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(kernel_size=2),\n",
    "\n",
    "                nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(kernel_size=2),\n",
    "                nn.Dropout(dropout)\n",
    "            )\n",
    "            \n",
    "        \n",
    "        # Fully connected layers\n",
    "        self._head = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(in_features=64 * 4 * 4, out_features=512), \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Linear(in_features=512, out_features=128), \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Linear(in_features=128, out_features=10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self._body(x)\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = self._head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">3.1. Experiment 1: No Regularization</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MediumModel()\n",
    "\n",
    "# get optimizer and scheduler\n",
    "optimizer, scheduler = get_optimizer_and_scheduler(model)\n",
    "\n",
    "# Tensorboard summary writer\n",
    "no_regularization_sw = SummaryWriter('logs_fashion_mnist/no_regularization')   \n",
    "\n",
    "# train and validate\n",
    "model, train_loss_exp2, train_acc_exp2, val_loss_exp2, val_acc_exp2 = main(model, \n",
    "                                                                           optimizer,\n",
    "                                                                           no_regularization_sw,\n",
    "                                                                           scheduler)\n",
    "no_regularization_sw.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">3.2. Experiment 2: Regularization</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: Embedding dir exists, did you set global_step for add_embedding()?\n"
     ]
    }
   ],
   "source": [
    "model = MediumModel(0.25, batch_norm=True)\n",
    "\n",
    "optimizer, scheduler = get_optimizer_and_scheduler(model)\n",
    "\n",
    "# Tensorboard summary writer\n",
    "regularization_sw = SummaryWriter('logs_fashion_mnist/regularization')  \n",
    "\n",
    "model, train_loss_exp9, train_acc_exp9, val_loss_exp9, val_acc_exp9 = main(model, \n",
    "                                                                           optimizer, \n",
    "                                                                           regularization_sw,\n",
    "                                                                           scheduler,\n",
    "                                                                           data_augmentation=True)\n",
    "\n",
    "regularization_sw.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:Blue\">4. Exercise</font>\n",
    "\n",
    "1. Add CNN layers as images to TensorBoard.\n",
    "\n",
    "2. Add CNN layers output after activation and see what it has learned. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:Blue\">References</font>\n",
    "\n",
    "- https://pytorch.org/docs/stable/tensorboard.html\n",
    "\n",
    "- https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
