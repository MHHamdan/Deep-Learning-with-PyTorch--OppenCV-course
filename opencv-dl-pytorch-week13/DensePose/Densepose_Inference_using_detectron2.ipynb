{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">Densepose Inference using detectron2</font>\n",
    "Detectron2 provides 2 tools to visualize dataset and run inference on test images.\n",
    "- **Apply Net**\n",
    "    - A tool to print or visualize DensePose results on a set of images. It has two modes: dump to save DensePose model results to a pickle file and show to visualize them on images\n",
    "\n",
    "- **Query Db**\n",
    "    -  A tool to print or visualize DensePose data from a dataset. It has two modes: print and show to output dataset entries to standard output or to visualize them on images.\n",
    "\n",
    "We will use apply net in this notebook. Query db is to visualize any dataset which will be of use while training in the next notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">1. Setup Code</font>\n",
    "\n",
    "To use the above tools, we have to download the densepose project from detectron2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'detectron2'...\n",
      "remote: Enumerating objects: 8, done.\u001b[K\n",
      "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
      "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
      "remote: Total 5058 (delta 0), reused 1 (delta 0), pack-reused 5050\u001b[K\n",
      "Receiving objects: 100% (5058/5058), 2.46 MiB | 1.28 MiB/s, done.\n",
      "Resolving deltas: 100% (3620/3620), done.\n",
      "Checking connectivity... done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/facebookresearch/detectron2.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data-ssd-4tb-b/prakash/densepose/Densepose/detectron2/projects/DensePose\n"
     ]
    }
   ],
   "source": [
    "%cd detectron2/projects/DensePose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">2. Import Config and Model files</font>\n",
    "\n",
    "Densepose config file can be found at `detectron2/projects/DensePose/configs/densepose_rcnn_R_50_FPN_s1x.yaml`\n",
    "\n",
    "Model weights files can be found <a href=\"https://github.com/facebookresearch/detectron2/blob/master/projects/DensePose/doc/MODEL_ZOO.md\" target=\"_blank\">here</a>. From the link, we have used improved baselines with original fully convolutional head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "def download(url, filepath):\n",
    "    response = urllib.request.urlretrieve(url, filepath)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('model_final_162be9.pkl', <http.client.HTTPMessage at 0x7f9438276050>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download(\"https://dl.fbaipublicfiles.com/densepose/densepose_rcnn_R_50_FPN_s1x/165712039/model_final_162be9.pkl\", \n",
    "         \"model_final_162be9.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the major functions of apply net, now lets see how we can run inference on Video using detectron2's densepose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">3. Inference on Video</font>\n",
    "\n",
    "### 3.1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from typing import ClassVar, Dict\n",
    "\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.structures.instances import Instances\n",
    "from detectron2.engine.defaults import DefaultPredictor\n",
    "\n",
    "from densepose import add_densepose_config\n",
    "from densepose.vis.base import CompoundVisualizer\n",
    "from densepose.vis.bounding_box import ScoredBoundingBoxVisualizer\n",
    "from densepose.vis.extractor import CompoundExtractor, create_extractor\n",
    "\n",
    "from densepose.vis.densepose_results import (\n",
    "    DensePoseResultsContourVisualizer,\n",
    "    DensePoseResultsFineSegmentationVisualizer,\n",
    "    DensePoseResultsUVisualizer,\n",
    "    DensePoseResultsVVisualizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Visualizers\n",
    "- Below mentioned object contains the different visualization methods like contour, segmentation, U coordinates, V coordinates and bounding box.\n",
    "\n",
    "**Sample visualizer method**:\n",
    "\n",
    "```\n",
    "class DensePoseResultsFineSegmentationVisualizer(DensePoseMaskedColormapResultsVisualizer):\n",
    "    def __init__(self, inplace=True, cmap=cv2.COLORMAP_PARULA, alpha=0.7):\n",
    "        super(DensePoseResultsFineSegmentationVisualizer, self).__init__(\n",
    "            _extract_i_from_iuvarr,\n",
    "            _extract_i_from_iuvarr,\n",
    "            inplace,\n",
    "            cmap,\n",
    "            alpha,\n",
    "            val_scale=255.0 / DensePoseDataRelative.N_PART_LABELS,\n",
    "        )\n",
    "```\n",
    "\n",
    "From above we can see how the segmentation visualizer **`DensePoseResultsFineSegmentationVisualizer`** works by calling other classes like **`DensePoseMaskedColormapResultsVisualizer`** which again calls **`DensePoseResultsVisualizer`** and few other functions like **`_extract_i_from_iuvarr`**.\n",
    "\n",
    "```\n",
    "class DensePoseMaskedColormapResultsVisualizer(DensePoseResultsVisualizer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_extractor,\n",
    "        segm_extractor,\n",
    "        inplace=True,\n",
    "        cmap=cv2.COLORMAP_PARULA,\n",
    "        alpha=0.7,\n",
    "        val_scale=1.0,\n",
    "    ):\n",
    "        self.mask_visualizer = MatrixVisualizer(\n",
    "            inplace=inplace, cmap=cmap, val_scale=val_scale, alpha=alpha\n",
    "        )\n",
    "        self.data_extractor = data_extractor\n",
    "        self.segm_extractor = segm_extractor\n",
    "\n",
    "    def create_visualization_context(self, image_bgr: Image):\n",
    "        return image_bgr\n",
    "\n",
    "    def context_to_image_bgr(self, context):\n",
    "        return context\n",
    "\n",
    "    def get_image_bgr_from_context(self, context):\n",
    "        return context\n",
    "\n",
    "    def visualize_iuv_arr(self, context, iuv_arr, bbox_xywh):\n",
    "        image_bgr = self.get_image_bgr_from_context(context)\n",
    "        matrix = self.data_extractor(iuv_arr)\n",
    "        segm = self.segm_extractor(iuv_arr)\n",
    "        mask = np.zeros(matrix.shape, dtype=np.uint8)\n",
    "        mask[segm > 0] = 1\n",
    "        image_bgr = self.mask_visualizer.visualize(image_bgr, mask, matrix, bbox_xywh)\n",
    "        return image_bgr\n",
    "\n",
    "\n",
    "def _extract_i_from_iuvarr(iuv_arr):\n",
    "    return iuv_arr[0, :, :]\n",
    "\n",
    "\n",
    "def _extract_u_from_iuvarr(iuv_arr):\n",
    "    return iuv_arr[1, :, :]\n",
    "\n",
    "\n",
    "def _extract_v_from_iuvarr(iuv_arr):\n",
    "    return iuv_arr[2, :, :]\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "class DensePoseResultsVisualizer(object):\n",
    "    def visualize(self, image_bgr: Image, densepose_result: Optional[DensePoseResult]) -> Image:\n",
    "        if densepose_result is None:\n",
    "            return image_bgr\n",
    "        context = self.create_visualization_context(image_bgr)\n",
    "        for i, result_encoded_w_shape in enumerate(densepose_result.results):\n",
    "            iuv_arr = DensePoseResult.decode_png_data(*result_encoded_w_shape)\n",
    "            bbox_xywh = densepose_result.boxes_xywh[i]\n",
    "            self.visualize_iuv_arr(context, iuv_arr, bbox_xywh)\n",
    "        image_bgr = self.context_to_image_bgr(context)\n",
    "        return image_bgr\n",
    "```\n",
    "\n",
    "- Visualize function of `DensePoseResultsVisualizer` decoded densepose result data to get iuv_arr and corresponding bounding boxes.\n",
    "- `visualize_iuv_arr` extracts matrix and segm from iuv_arr, since the selected visulization format is segm and I is also partwise segmentation, both the matrix and segm are same. In case of other visualizations, we may use `_extract_u_from_iuvarr` or `_extract_v_from_iuvarr`\n",
    "- Mask of segmentation is generated.\n",
    "- mask_visualizer uses `MatrixVisualizer` defined in `densepose/vis/base.py`\n",
    "    - resizes the matrix, mask according to the bbox width, height.\n",
    "    - multiples the matrix with val_scale, clips the matrix values to (0,255) and converts to the image format.\n",
    "    - Then it applies color coding to the matrix image and the original image is colored accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualizer methods\n",
    "VISUALIZERS: ClassVar[Dict[str, object]] = {\n",
    "    \"dp_contour\": DensePoseResultsContourVisualizer,\n",
    "    \"dp_segm\": DensePoseResultsFineSegmentationVisualizer,\n",
    "    \"dp_u\": DensePoseResultsUVisualizer,\n",
    "    \"dp_v\": DensePoseResultsVVisualizer,\n",
    "    \"bbox\": ScoredBoundingBoxVisualizer,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Setup Config\n",
    "- It imports the default config and gets the densepose specific config `add_densepose_config` which can be viewed at `detectron2/projects/DensePose/densepose/config.py`.\n",
    "- It also imports the config file and model weights file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setConfig():\n",
    "    cfg = get_cfg()\n",
    "    add_densepose_config(cfg)\n",
    "\n",
    "    cfg.merge_from_file(\"configs/densepose_rcnn_R_50_FPN_s1x.yaml\")\n",
    "    cfg.MODEL.DEVICE = \"cuda\"\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5 \n",
    "\n",
    "    cfg.MODEL.WEIGHTS = \"model_final_162be9.pkl\"\n",
    "    \n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Visualizer and Extractor\n",
    "- Initializes the visualizer and extractor method for the different types of visualizations given in the arguments.\n",
    "- Simlutaneously multiple visulization formats can be selected which are handled by CompoundVisualizer and CompoundExtractor.\n",
    "- These methods extract contour, segmentation or points information from IUV mapping output given by the densepose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVisAndExtract(vis_specs):\n",
    "    visualizers = []\n",
    "    extractors = []\n",
    "    for vis_spec in vis_specs:\n",
    "        vis = VISUALIZERS[vis_spec]()\n",
    "        visualizers.append(vis)\n",
    "        extractor = create_extractor(vis)\n",
    "        extractors.append(extractor)\n",
    "    visualizer = CompoundVisualizer(visualizers)\n",
    "    extractor = CompoundExtractor(extractors)\n",
    "    \n",
    "    return extractor, visualizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Create context\n",
    "```\n",
    "context = {\n",
    "            \"extractor\": extractor,\n",
    "            \"visualizer\": visualizer,\n",
    "            \"out_fname\": args.output,\n",
    "            \"entry_idx\": 0,\n",
    "        }\n",
    "```\n",
    "- Creates context object with visualizer, extractor, output filename and entry idx. Here, we use only visualizer and extractor keys for our purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createContext(extractor, visualizer):\n",
    "    context = {\n",
    "        \"extractor\": extractor,\n",
    "        \"visualizer\": visualizer\n",
    "    }\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Predict Image\n",
    "- Extractor finds the IUV mapping of the detected humans in the image in the DensePoseOutput format.\n",
    "- This output is processed in the visualizer to the viewable format like contours, points or segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(img, predictor, context):\n",
    "    outputs = predictor(img)['instances']\n",
    "    image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    image = np.tile(image[:, :, np.newaxis], [1, 1, 3])\n",
    "    data = context[\"extractor\"](outputs)\n",
    "    image_vis = context[\"visualizer\"].visualize(image, data)\n",
    "    return image_vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the inference video function, we are performing detection for every 10th frame, which can be changed accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inferenceOnVideo(videoPath, predictor, context):\n",
    "    cap = cv2.VideoCapture(videoPath)\n",
    "    cnt = 0\n",
    "    n_frame = 10\n",
    "\n",
    "    output_frames = []\n",
    "    \n",
    "    import time\n",
    "\n",
    "    while True:\n",
    "        ret, im = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if cnt%n_frame == 0:\n",
    "            output = predict(im, predictor, context)\n",
    "            time.sleep(1)\n",
    "            output_frames.append(output)\n",
    "\n",
    "        cnt = cnt + 1\n",
    "\n",
    "\n",
    "    height, width, _ = output_frames[0].shape\n",
    "    size = (width,height)\n",
    "    out = cv2.VideoWriter(\"out.mp4\",cv2.VideoWriter_fourcc(*'mp4v'), 10, size)\n",
    "\n",
    "    for i in range(len(output_frames)):\n",
    "        out.write(output_frames[i])\n",
    "\n",
    "    out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6. Main Execution\n",
    "- Define visulization formats to be used in vis_specs. {'bbox', 'dp_segm', 'dp_contour', 'dp_u', 'dp_v'}\n",
    "- Initialize config\n",
    "- Initialize detectron2's default predictor method.\n",
    "- Define visualizer and extractor methods based on vis_specs\n",
    "- Context created with required functions to use in the prediction\n",
    "- All frames predicted by densepose are compiled to output video out.mp4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download <a href=\"https://www.dropbox.com/s/kk4zjqcfm5yf1cp/test_cut.mp4?dl=1\" target=\"_blank\">test_cut.mp4</a>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('test_cut.mp4', <http.client.HTTPMessage at 0x7f93c6bab4d0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download('https://www.dropbox.com/s/kk4zjqcfm5yf1cp/test_cut.mp4?dl=1', 'test_cut.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config 'configs/densepose_rcnn_R_50_FPN_s1x.yaml' has no VERSION. Assuming it to be compatible with latest v2.\n"
     ]
    }
   ],
   "source": [
    "vis_specs = ['dp_segm', 'bbox']\n",
    "\n",
    "cfg = setConfig()\n",
    "\n",
    "##Initialize predictor\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "extractor, visualizer = getVisAndExtract(vis_specs)\n",
    "\n",
    "context = createContext(extractor, visualizer)\n",
    "\n",
    "inferenceOnVideo(\"test_cut.mp4\", predictor, context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">References</font>\n",
    "\n",
    "- <a href=\"https://github.com/facebookresearch/detectron2\" target=\"_blank\">https://github.com/facebookresearch/detectron2</a>\n",
    "- <a href=\"http://densepose.org/\" target=\"_blank\">http://densepose.org/</a>\n",
    "- <a href=\"https://research.fb.com/downloads/densepose/\" target=\"_blank\">https://research.fb.com/downloads/densepose/</a>\n",
    "- <a href=\"https://arxiv.org/pdf/1802.00434.pdf\" target=\"_blank\">https://arxiv.org/pdf/1802.00434.pdf</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
